CHAPTER 11
Network and Web Programming

This chapter is about various topics related to using Python in networked and dis‐
tributed applications. Topics are split between using Python as a client to access existing
services and using Python to implement networked services as a server. Common tech‐
niques for writing code involving cooperating or communicating with interpreters are
also given.
11.1. Interacting with HTTP Services As a Client
Problem
You need to access various services via HTTP as a client. For example, downloading
data or interacting with a REST-based API.

Solution
For simple things, it’s usually easy enough to use the  urllib.request module. For
example, to send a simple HTTP GET request to a remote service, do something like this:

from urllib import request, parse

# Base URL being accessed
url = 'http://httpbin.org/get'

# Dictionary of query parameters (if any)
parms = {
   'name1' : 'value1',
   'name2' : 'value2'
}

# Encode the query string
querystring = parse.urlencode(parms)

# Make a GET request and read the response
u = request.urlopen(url+'?' + querystring)
resp = u.read()

If you need to send the query parameters in the request body using a POST method,
encode them and supply them as an optional argument to urlopen() like this:

from urllib import request, parse

# Base URL being accessed
url = 'http://httpbin.org/post'

# Dictionary of query parameters (if any)
parms = {
   'name1' : 'value1',
   'name2' : 'value2'
}

# Encode the query string
querystring = parse.urlencode(parms)

# Make a POST request and read the response
u = request.urlopen(url, querystring.encode('ascii'))
resp = u.read()

If you need to supply some custom HTTP headers in the outgoing request such as a
change to the user-agent field, make a dictionary containing their value and create a
Request instance and pass it to urlopen() like this:

from urllib import request, parse
...

# Extra headers
headers = {
    'User-agent' : 'none/ofyourbusiness',
    'Spam' : 'Eggs'
}

req = request.Request(url, querystring.encode('ascii'), headers=headers)

# Make a request and read the response
u = request.urlopen(req)
resp = u.read()

If your interaction with a service is more complicated than this, you should probably
look at the requests library. For example, here is equivalent requests code for the
preceding operations:

import requests

# Base URL being accessed
url = 'http://httpbin.org/post'

# Dictionary of query parameters (if any)
parms = {
   'name1' : 'value1',
   'name2' : 'value2'
}

# Extra headers
headers = {
    'User-agent' : 'none/ofyourbusiness',
    'Spam' : 'Eggs'
}

resp = requests.post(url, data=parms, headers=headers)

# Decoded text returned by the request
text = resp.text

A notable feature of requests is how it returns the resulting response content from a
request. As shown, the resp.text attribute gives you the Unicode decoded text of a
request. However, if you access resp.content, you get the raw binary content instead.
On the other hand, if you access resp.json, then you get the response content inter‐
preted as JSON.
Here is an example of using requests to make a HEAD request and extract a few fields
of header data from the response:

import requests

resp = requests.head('http://www.python.org/index.html')

status = resp.status_code
last_modified = resp.headers['last-modified']
content_type = resp.headers['content-type']
content_length = resp.headers['content-length']

Here is a requests example that executes a login into the Python Package index using
basic authentication:
import requests

resp = requests.get('http://pypi.python.org/pypi?:action=login',
                    auth=('user','password'))

Here is an example of using requests to pass HTTP cookies from one request to the
next:

import requests

# First request
resp1 = requests.get(url)
...

11.1. Interacting with HTTP Services As a Client 

# Second requests with cookies received on first requests
resp2 = requests.get(url, cookies=resp1.cookies)

Last, but not least, here is an example of using requests to upload content:

import requests
url = 'http://httpbin.org/post'
files = { 'file': ('data.csv', open('data.csv', 'rb')) }

r = requests.post(url, files=files)

Discussion
For really simple HTTP client code, using the built-in urllib module is usually fine.
However, if you have to do anything other than simple GET or POST requests, you really
can’t rely on its functionality. This is where a third-party module, such as requests,
comes in handy.
For example, if you decided to stick entirely with the standard library instead of a library
like requests, you might have to implement your code using the low-level http.cli
ent module instead. For example, this code shows how to execute a HEAD request:

from http.client import HTTPConnection
from urllib import parse

c = HTTPConnection('www.python.org', 80)
c.request('HEAD', '/index.html')
resp = c.getresponse()

print('Status', resp.status)
for name, value in resp.getheaders():
    print(name, value)

Similarly, if you have to write code involving proxies, authentication, cookies, and other
details, using urllib is awkward and verbose. For example, here is a sample of code that
authenticates to the Python package index:

import urllib.request

auth = urllib.request.HTTPBasicAuthHandler()
auth.add_password('pypi','http://pypi.python.org','username','password')
opener = urllib.request.build_opener(auth)

r = urllib.request.Request('http://pypi.python.org/pypi?:action=login')
u = opener.open(r)
resp = u.read()

# From here. You can access more pages using opener
...

Frankly, all of this is much easier in requests.
Testing HTTP client code during development can often be frustrating because of all
the tricky details you need to worry about (e.g., cookies, authentication, headers, en‐
codings, etc.). To do this, consider using the httpbin service. This site receives requests
and then echoes information back to you in the form a JSON response. Here is an
interactive example:

>>> import requests
>>> r = requests.get('http://httpbin.org/get?name=Dave&n=37',
...     headers = { 'User-agent': 'goaway/1.0' })
>>> resp = r.json
>>> resp['headers']
{'User-Agent': 'goaway/1.0', 'Content-Length': '', 'Content-Type': '',
'Accept-Encoding': 'gzip, deflate, compress', 'Connection':
'keep-alive', 'Host': 'httpbin.org', 'Accept': '*/*'}
>>> resp['args']
{'name': 'Dave', 'n': '37'}
>>>

Working with a site such as httpbin.org is often preferable to experimenting with a real
site—especially if there’s a risk it might shut down your account after three failed login
attempts (i.e., don’t try to learn how to write an HTTP authentication client by logging
into your bank).
Although it’s not discussed here, requests provides support for many more advanced
HTTP-client protocols, such as OAuth. The requests documentation is excellent (and
frankly better than anything that could be provided in this short space). Go there for
more information.
11.2. Creating a TCP Server
Problem
You want to implement a server that communicates with clients using the TCP Internet
protocol.

Solution
An easy way to create a TCP server is to use the socketserver library. For example,
here is a simple echo server:

from socketserver import BaseRequestHandler, TCPServer

class EchoHandler(BaseRequestHandler):
    def handle(self):
        print('Got connection from', self.client_address)
        while True:

11.2. Creating a TCP Server 

            msg = self.request.recv(8192)
            if not msg:
                break
            self.request.send(msg)

if __name__ == '__main__':
    serv = TCPServer(('', 20000), EchoHandler)
    serv.serve_forever()

In this code, you define a special handler class that implements a handle() method for
servicing client connections. The request attribute is the underlying client socket and
client_address has client address.
To test the server, run it and then open a separate Python process that connects to it:

>>> from socket import socket, AF_INET, SOCK_STREAM
>>> s = socket(AF_INET, SOCK_STREAM)
>>> s.connect(('localhost', 20000))
>>> s.send(b'Hello')
5
>>> s.recv(8192)
b'Hello'
>>>

In many cases, it may be easier to define a slightly different kind of handler. Here is an
example that uses the StreamRequestHandler base class to put a file-like interface on
the underlying socket:

from socketserver import StreamRequestHandler, TCPServer

class EchoHandler(StreamRequestHandler):
    def handle(self):
        print('Got connection from', self.client_address)
        # self.rfile is a file-like object for reading
        for line in self.rfile:
            # self.wfile is a file-like object for writing
            self.wfile.write(line)

if __name__ == '__main__':
    serv = TCPServer(('', 20000), EchoHandler)
    serv.serve_forever()

Discussion
socketserver  makes  it  relatively  easy  to  create  simple  TCP  servers.  However,  you
should be aware that, by default, the servers are single threaded and can only serve one
client at a time. If you want to handle multiple clients, either instantiate a ForkingTCP
Server or ThreadingTCPServer object instead. For example:

from socketserver import ThreadingTCPServer
...

if __name__ == '__main__':
    serv = ThreadingTCPServer(('', 20000), EchoHandler)
    serv.serve_forever()

One issue with forking and threaded servers is that they spawn a new process or thread
on each client connection. There is no upper bound on the number of allowed clients,
so a malicious hacker could potentially launch a large number of simultaneous con‐
nections in an effort to make your server explode.
If this is a concern, you can create a pre-allocated pool of worker threads or processes.
To do this, you create an instance of a normal nonthreaded server, but then launch the
serve_forever() method in a pool of multiple threads. For example:

...
if __name__ == '__main__':
    from threading import Thread
    NWORKERS = 16
    serv = TCPServer(('', 20000), EchoHandler)
    for n in range(NWORKERS):
        t = Thread(target=serv.serve_forever)
        t.daemon = True
        t.start()
    serv.serve_forever()

Normally, a TCPServer binds and activates the underlying socket upon instantiation.
However, sometimes you might want to adjust the underlying socket by setting options.
To do this, supply the bind_and_activate=False argument, like this:

if __name__ == '__main__':
    serv = TCPServer(('', 20000), EchoHandler, bind_and_activate=False)
    # Set up various socket options
    serv.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)
    # Bind and activate
    serv.server_bind()
    serv.server_activate()
    serv.serve_forever()

The socket option shown is actually a very common setting that allows the server to
rebind to a previously used port number. It’s actually so common that it’s a class variable
that can be set on TCPServer. Set it before instantiating the server, as shown in this
example:

...
if __name__ == '__main__':
    TCPServer.allow_reuse_address = True
    serv = TCPServer(('', 20000), EchoHandler)
    serv.serve_forever()

In the solution, two different handler base classes were shown (BaseRequestHandler
and StreamRequestHandler). The StreamRequestHandler class is actually a bit more

11.2. Creating a TCP Server 

flexible, and supports some features that can be enabled through the specification of
additional class variables. For example:

import socket

class EchoHandler(StreamRequestHandler):
    # Optional settings (defaults shown)
    timeout = 5                      # Timeout on all socket operations
    rbufsize = -1                    # Read buffer size
    wbufsize = 0                     # Write buffer size
    disable_nagle_algorithm = False  # Sets TCP_NODELAY socket option
    def handle(self):
        print('Got connection from', self.client_address)
        try:
            for line in self.rfile:
                # self.wfile is a file-like object for writing
                self.wfile.write(line)
        except socket.timeout:
            print('Timed out!')

Finally, it should be noted that most of Python’s higher-level networking modules (e.g.,
HTTP, XML-RPC, etc.) are built on top of the socketserver functionality. That said,
it is also not difficult to implement servers directly using the socket library as well. Here
is a simple example of directly programming a server with Sockets:

from socket import socket, AF_INET, SOCK_STREAM

def echo_handler(address, client_sock):
    print('Got connection from {}'.format(address))
    while True:
        msg = client_sock.recv(8192)
        if not msg:
            break
        client_sock.sendall(msg)
    client_sock.close()

def echo_server(address, backlog=5):
    sock = socket(AF_INET, SOCK_STREAM)
    sock.bind(address)
    sock.listen(backlog)
    while True:
        client_sock, client_addr = sock.accept()
        echo_handler(client_addr, client_sock)

if __name__ == '__main__':
    echo_server(('', 20000))

11.3. Creating a UDP Server
Problem
You want to implement a server that communicates with clients using the UDP Internet
protocol.

Solution
As with TCP, UDP servers are also easy to create using the socketserver library. For
example, here is a simple time server:

from socketserver import BaseRequestHandler, UDPServer
import time

class TimeHandler(BaseRequestHandler):
    def handle(self):
        print('Got connection from', self.client_address)
        # Get message and client socket
        msg, sock = self.request
        resp = time.ctime()
        sock.sendto(resp.encode('ascii'), self.client_address)

if __name__ == '__main__':
    serv = UDPServer(('', 20000), TimeHandler)
    serv.serve_forever()

As before, you define a special handler class that implements a handle() method for
servicing client connections. The request attribute is a tuple that contains the incoming
datagram and underlying socket object for the server. The client_address contains
the client address.
To test the server, run it and then open a separate Python process that sends messages
to it:

>>> from socket import socket, AF_INET, SOCK_DGRAM
>>> s = socket(AF_INET, SOCK_DGRAM)
>>> s.sendto(b'', ('localhost', 20000))
0
>>> s.recvfrom(8192)
(b'Wed Aug 15 20:35:08 2012', ('127.0.0.1', 20000))
>>>

Discussion
A typical UDP server receives an incoming datagram (message) along with a client
address. If the server is to respond, it sends a datagram back to the client. For trans‐
mission of datagrams, you should use the  sendto() and  recvfrom() methods of a

11.3. Creating a UDP Server 

socket. Although the traditional send() and recv() methods also might work, the for‐
mer two methods are more commonly used with UDP communication.
Given that there is no underlying connection, UDP servers are often much easier to
write than a TCP server. However, UDP is also inherently unreliable (e.g., no “connec‐
tion” is established and messages might be lost). Thus, it would be up to you to figure
out how to deal with lost messages. That’s a topic beyond the scope of this book, but
typically you might need to introduce sequence numbers, retries, timeouts, and other
mechanisms to ensure reliability if it matters for your application. UDP is often used in
cases where the requirement of reliable delivery can be relaxed. For instance, in real-
time applications such as multimedia streaming and games where there is simply no
option to go back in time and recover a lost packet (the program simply skips it and
keeps moving forward).
The UDPServer class is single threaded, which means that only one request can be serv‐
iced at a time. In practice, this is less of an issue with UDP than with TCP connections.
However, should you want concurrent operation, instantiate a ForkingUDPServer or
ThreadingUDPServer object instead:

from socketserver import ThreadingUDPServer
...
if __name__ == '__main__':
    serv = ThreadingUDPServer(('',20000), TimeHandler)
    serv.serve_forever()

Implementing  a  UDP  server  directly  using  sockets  is  also  not  difficult.  Here  is  an
example:

from socket import socket, AF_INET, SOCK_DGRAM
import time

def time_server(address):
    sock = socket(AF_INET, SOCK_DGRAM)
    sock.bind(address)
    while True:
        msg, addr = sock.recvfrom(8192)
        print('Got message from', addr)
        resp = time.ctime()
        sock.sendto(resp.encode('ascii'), addr)

if __name__ == '__main__':
    time_server(('', 20000))

11.4. Generating a Range of IP Addresses from a CIDR
Address
Problem
You have a CIDR network address such as “123.45.67.89/27,” and you want to generate
a range of all the IP addresses that it represents (e.g., “123.45.67.64,” “123.45.67.65,” …,
“123.45.67.95”).

Solution
The ipaddress module can be easily used to perform such calculations. For example:

>>> import ipaddress
>>> net = ipaddress.ip_network('123.45.67.64/27')
>>> net
IPv4Network('123.45.67.64/27')
>>> for a in net:
...     print(a)
...
123.45.67.64
123.45.67.65
123.45.67.66
123.45.67.67
123.45.67.68
...
123.45.67.95
>>>

>>> net6 = ipaddress.ip_network('12:3456:78:90ab:cd:ef01:23:30/125')
>>> net6
IPv6Network('12:3456:78:90ab:cd:ef01:23:30/125')
>>> for a in net6:
...     print(a)
...
12:3456:78:90ab:cd:ef01:23:30
12:3456:78:90ab:cd:ef01:23:31
12:3456:78:90ab:cd:ef01:23:32
12:3456:78:90ab:cd:ef01:23:33
12:3456:78:90ab:cd:ef01:23:34
12:3456:78:90ab:cd:ef01:23:35
12:3456:78:90ab:cd:ef01:23:36
12:3456:78:90ab:cd:ef01:23:37
>>>

Network objects also allow indexing like arrays. For example:

>>> net.num_addresses
32
>>> net[0]

11.4. Generating a Range of IP Addresses from a CIDR Address 

IPv4Address('123.45.67.64')
>>> net[1]
IPv4Address('123.45.67.65')
>>> net[-1]
IPv4Address('123.45.67.95')
>>> net[-2]
IPv4Address('123.45.67.94')
>>>

In addition, you can perform operations such as a check for network membership:

>>> a = ipaddress.ip_address('123.45.67.69')
>>> a in net
True
>>> b = ipaddress.ip_address('123.45.67.123')
>>> b in net
False
>>>

An IP address and network address can be specified together as an IP interface. For
example:

>>> inet = ipaddress.ip_interface('123.45.67.73/27')
>>> inet.network
IPv4Network('123.45.67.64/27')
>>> inet.ip
IPv4Address('123.45.67.73')
>>>

Discussion
The ipaddress module has classes for representing IP addresses, networks, and inter‐
faces. This can be especially useful if you want to write code that needs to manipulate
network addresses in some way (e.g., parsing, printing, validating, etc.).
Be aware that there is only limited interaction between the ipaddress module and other
network-related modules, such as the  socket library. In particular, it is usually not
possible to use an instance of IPv4Address as a substitute for address string. Instead,
you have to explicitly convert it using str() first. For example:

>>> a = ipaddress.ip_address('127.0.0.1')
>>> from socket import socket, AF_INET, SOCK_STREAM
>>> s = socket(AF_INET, SOCK_STREAM)
>>> s.connect((a, 8080))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: Can't convert 'IPv4Address' object to str implicitly
>>> s.connect((str(a), 8080))
>>>

See “An Introduction to the ipaddress Module” for more information and advanced
usage.

11.5. Creating a Simple REST-Based Interface
Problem
You want to be able to control or interact with your program remotely over the network
using a simple REST-based interface. However, you don’t want to do it by installing a
full-fledged web programming framework.

Solution
One of the easiest ways to build REST-based interfaces is to create a tiny library based
on the WSGI standard, as described in PEP 3333. Here is an example:

# resty.py

import cgi

def notfound_404(environ, start_response):
    start_response('404 Not Found', [ ('Content-type', 'text/plain') ])
    return [b'Not Found']

class PathDispatcher:
    def __init__(self):
        self.pathmap = { }

    def __call__(self, environ, start_response):
        path = environ['PATH_INFO']
        params = cgi.FieldStorage(environ['wsgi.input'],
                                  environ=environ)
        method = environ['REQUEST_METHOD'].lower()
        environ['params'] = { key: params.getvalue(key) for key in params }
        handler = self.pathmap.get((method,path), notfound_404)
        return handler(environ, start_response)

    def register(self, method, path, function):
        self.pathmap[method.lower(), path] = function
        return function

To use this dispatcher, you simply write different handlers, such as the following:

import time

_hello_resp = '''\
<html>
  <head>
     <title>Hello {name}</title>
   </head>
   <body>
     <h1>Hello {name}!</h1>
   </body>
</html>'''

11.5. Creating a Simple REST-Based Interface 

def hello_world(environ, start_response):
    start_response('200 OK', [ ('Content-type','text/html')])
    params = environ['params']
    resp = _hello_resp.format(name=params.get('name'))
    yield resp.encode('utf-8')

_localtime_resp = '''\
<?xml version="1.0"?>
<time>
  <year>{t.tm_year}</year>
  <month>{t.tm_mon}</month>
  <day>{t.tm_mday}</day>
  <hour>{t.tm_hour}</hour>
  <minute>{t.tm_min}</minute>
  <second>{t.tm_sec}</second>
</time>'''

def localtime(environ, start_response):
    start_response('200 OK', [ ('Content-type', 'application/xml') ])
    resp = _localtime_resp.format(t=time.localtime())
    yield resp.encode('utf-8')

if __name__ == '__main__':
    from resty import PathDispatcher
    from wsgiref.simple_server import make_server

    # Create the dispatcher and register functions
    dispatcher = PathDispatcher()
    dispatcher.register('GET', '/hello', hello_world)
    dispatcher.register('GET', '/localtime', localtime)

    # Launch a basic server
    httpd = make_server('', 8080, dispatcher)
    print('Serving on port 8080...')
    httpd.serve_forever()

To test your server, you can interact with it using a browser or urllib. For example:

>>> u = urlopen('http://localhost:8080/hello?name=Guido')
>>> print(u.read().decode('utf-8'))
<html>
  <head>
     <title>Hello Guido</title>
   </head>
   <body>
     <h1>Hello Guido!</h1>
   </body>
</html>
>>> u = urlopen('http://localhost:8080/localtime')
>>> print(u.read().decode('utf-8'))
<?xml version="1.0"?>
<time>

  <year>2012</year>
  <month>11</month>
  <day>24</day>
  <hour>14</hour>
  <minute>49</minute>
  <second>17</second>
</time>
>>>

Discussion
In REST-based interfaces, you are typically writing programs that respond to common
HTTP requests. However, unlike a full-fledged website, you’re often just pushing data
around. This data might be encoded in a variety of standard formats such as XML, JSON,
or CSV. Although it seems minimal, providing an API in this manner can be a very
useful thing for a wide variety of applications.
For example, long-running programs might use a REST API to implement monitoring
or diagnostics. Big data applications can use REST to build a query/data extraction
system. REST can even be used to control hardware devices, such as robots, sensors,
mills, or lightbulbs. What’s more, REST APIs are well supported by various client-side
programming environments, such as Javascript, Android, iOS, and so forth. Thus, hav‐
ing such an interface can be a way to encourage the development of more complex
applications that interface with your code.
For implementing a simple REST interface, it is often easy enough to base your code on
the Python WSGI standard. WSGI is supported by the standard library, but also by most
third-party web frameworks. Thus, if you use it, there is a lot of flexibility in how your
code might be used later.
In WSGI, you simply implement applications in the form of a callable that accepts this
calling convention:

import cgi

def wsgi_app(environ, start_response):
    ...

The environ argument is a dictionary that contains values inspired by the CGI interface
provided by various web servers such as Apache [see Internet RFC 3875]. To extract
different fields, you would write code like this:

def wsgi_app(environ, start_response):
    method = environ['REQUEST_METHOD']
    path = environ['PATH_INFO']
    # Parse the query parameters
    params = cgi.FieldStorage(environ['wsgi.input'], environ=environ)
    ...

11.5. Creating a Simple REST-Based Interface 

A few common values are shown here. environ['REQUEST_METHOD'] is the type of re‐
quest (e.g., GET, POST, HEAD, etc.). environ['PATH_INFO'] is the path or the resource
being requested. The call to cgi.FieldStorage() extracts supplied query parameters
from the request and puts them into a dictionary-like object for later use.
The start_response argument is a function that must be called to initiate a response.
The first argument is the resulting HTTP status. The second argument is a list of (name,
value) tuples that make up the HTTP headers of the response. For example:

def wsgi_app(environ, start_response):
    ...
    start_response('200 OK', [('Content-type', 'text/plain')])

To return data, an WSGI application must return a sequence of byte strings. This can
be done using a list like this:

def wsgi_app(environ, start_response):
    ...
    start_response('200 OK', [('Content-type', 'text/plain')])
    resp = []
    resp.append(b'Hello World\n')
    resp.append(b'Goodbye!\n')
    return resp

Alternatively, you can use yield:

def wsgi_app(environ, start_response):
    ...
    start_response('200 OK', [('Content-type', 'text/plain')])
    yield b'Hello World\n'
    yield b'Goodbye!\n'

It’s important to emphasize that byte strings must be used in the result. If the response
consists of text, it will need to be encoded into bytes first. Of course, there is no re‐
quirement that the returned value be text—you could easily write an application func‐
tion that creates images.
Although WSGI applications are commonly defined as a function, as shown, an instance
may also be used as long as it implements a suitable __call__() method. For example:

class WSGIApplication:
    def __init__(self):
        ...
    def __call__(self, environ, start_response)
       ...

This technique has been used to create the PathDispatcher class in the recipe. The
dispatcher does nothing more than manage a dictionary mapping (method, path) pairs
to handler functions. When a request arrives, the method and path are extracted and
used to dispatch to a handler. In addition, any query variables are parsed and put into

a dictionary that is stored as environ['params'] (this latter step is so common, it makes
a lot of sense to simply do it in the dispatcher in order to avoid a lot of replicated code).
To use the dispatcher, you simply create an instance and register various WSGI-style
application functions with it, as shown in the recipe. Writing these functions should be
extremely straightforward, as you follow the rules concerning the start_response()
function and produce output as byte strings.
One thing to consider when writing such functions is the careful use of string templates.
Nobody likes to work with code that is a tangled mess of print() functions, XML, and
various formatting operations. In the solution, triple-quoted string templates are being
defined and used internally. This particular approach makes it easier to change the
format of the output later (just change the template as opposed to any of the code that
uses it).
Finally, an important part of using WSGI is that nothing in the implementation is spe‐
cific to a particular web server. That is actually the whole idea—since the standard is
server and framework neutral, you should be able to plug your application into a wide
variety of servers. In the recipe, the following code is used for testing:

if __name__ == '__main__':
    from wsgiref.simple_server import make_server

    # Create the dispatcher and register functions
    dispatcher = PathDispatcher()
    ...

    # Launch a basic server
    httpd = make_server('', 8080, dispatcher)
    print('Serving on port 8080...')
    httpd.serve_forever()

This will create a simple server that you can use to see if your implementation works.
Later on, when you’re ready to scale things up to a larger level, you will change this code
to work with a particular server.
WSGI is an intentionally minimal specification. As such, it doesn’t provide any support
for more advanced concepts such as authentication, cookies, redirection, and so forth.
These are not hard to implement yourself. However, if you want just a bit more support,
you might consider third-party libraries, such as WebOb or Paste. 

11.5. Creating a Simple REST-Based Interface 

11.6. Implementing a Simple Remote Procedure Call with
XML-RPC
Problem
You want an easy way to execute functions or methods in Python programs running on
remote machines.

Solution
Perhaps the easiest way to implement a simple remote procedure call mechanism is to
use XML-RPC. Here is an example of a simple server that implements a simple key-
value store:

from xmlrpc.server import SimpleXMLRPCServer

class KeyValueServer:
    _rpc_methods_ = ['get', 'set', 'delete', 'exists', 'keys']
    def __init__(self, address):
        self._data = {}
        self._serv = SimpleXMLRPCServer(address, allow_none=True)
        for name in self._rpc_methods_:
            self._serv.register_function(getattr(self, name))

    def get(self, name):
        return self._data[name]

    def set(self, name, value):
        self._data[name] = value

    def delete(self, name):
        del self._data[name]

    def exists(self, name):
        return name in self._data

    def keys(self):
        return list(self._data)

    def serve_forever(self):
        self._serv.serve_forever()

# Example
if __name__ == '__main__':
    kvserv = KeyValueServer(('', 15000))
    kvserv.serve_forever()

Here is how you would access the server remotely from a client:

>>> from xmlrpc.client import ServerProxy
>>> s = ServerProxy('http://localhost:15000', allow_none=True)
>>> s.set('foo', 'bar')
>>> s.set('spam', [1, 2, 3])
>>> s.keys()
['spam', 'foo']
>>> s.get('foo')
'bar'
>>> s.get('spam')
[1, 2, 3]
>>> s.delete('spam')
>>> s.exists('spam')
False
>>>

Discussion
XML-RPC can be an extremely easy way to set up a simple remote procedure call service.
All you need to do is create a server instance, register functions with it using the regis
ter_function() method, and then launch it using the serve_forever() method. This
recipe packages it up into a class to put all of the code together, but there is no such
requirement. For example, you could create a server by trying something like this:

from xmlrpc.server import SimpleXMLRPCServer
def add(x,y):
    return x+y

serv = SimpleXMLRPCServer(('', 15000))
serv.register_function(add)
serv.serve_forever()

Functions exposed via XML-RPC only work with certain kinds of data such as strings,
numbers, lists, and dictionaries. For everything else, some study is required. For in‐
stance,  if  you  pass  an  instance  through  XML-RPC,  only  its  instance  dictionary  is
handled:

>>> class Point:
...     def __init__(self, x, y):
...             self.x = x
...             self.y = y
...
>>> p = Point(2, 3)
>>> s.set('foo', p)
>>> s.get('foo')
{'x': 2, 'y': 3}
>>>

Similarly, handling of binary data is a bit different than you expect:

>>> s.set('foo', b'Hello World')
>>> s.get('foo')
<xmlrpc.client.Binary object at 0x10131d410>

11.6. Implementing a Simple Remote Procedure Call with XML-RPC 

>>> _.data
b'Hello World'
>>>

As a general rule, you probably shouldn’t expose an XML-RPC service to the rest of the
world as a public API. It often works best on internal networks where you might want
to write simple distributed programs involving a few different machines.
A downside to XML-RPC is its performance. The SimpleXMLRPCServer implementa‐
tion is only single threaded, and wouldn’t be appropriate for scaling a large application,
although it can be made to run multithreaded, as shown in Recipe 11.2. Also, since
XML-RPC  serializes  all  data  as  XML,  it’s  inherently  slower  than  other  approaches.
However, one benefit of this encoding is that it’s understood by a variety of other pro‐
gramming languages. By using it, clients written in languages other than Python will be
able to access your service.
Despite its limitations, XML-RPC is worth knowing about if you ever have the need to
make a quick and dirty remote procedure call system. Oftentimes, the simple solution
is good enough.
11.7. Communicating Simply Between Interpreters
Problem
You are running multiple instances of the Python interpreter, possibly on different ma‐
chines, and you would like to exchange data between interpreters using messages.

Solution
It is easy to communicate between interpreters if you use the multiprocessing.con
nection module. Here is a simple example of writing an echo server:

from multiprocessing.connection import Listener
import traceback

def echo_client(conn):
    try:
        while True:
            msg = conn.recv()
            conn.send(msg)
    except EOFError:
        print('Connection closed')

def echo_server(address, authkey):
    serv = Listener(address, authkey=authkey)
    while True:
        try:
            client = serv.accept()

            echo_client(client)
        except Exception:
            traceback.print_exc()

echo_server(('', 25000), authkey=b'peekaboo')

Here  is  a  simple  example  of  a  client  connecting  to  the  server  and  sending  various
messages:

>>> from multiprocessing.connection import Client
>>> c = Client(('localhost', 25000), authkey=b'peekaboo')
>>> c.send('hello')
>>> c.recv()
'hello'
>>> c.send(42)
>>> c.recv()
42
>>> c.send([1, 2, 3, 4, 5])
>>> c.recv()
[1, 2, 3, 4, 5]
>>>

Unlike a low-level socket, messages are kept intact (each object sent using send() is
received in its entirety with recv()). In addition, objects are serialized using pickle.
So, any object compatible with pickle can be sent or received over the connection.

Discussion
There are many packages and libraries related to implementing various forms of mes‐
sage passing, such as ZeroMQ, Celery, and so forth. As an alternative, you might also
be inclined to implement a message layer on top of low-level sockets. However, some‐
times you just want a simple solution. The multiprocessing.connection library is just
that—using a few simple primitives, you can easily connect interpreters together and
have them exchange messages.
If you know that the interpreters are going to be running on the same machine, you can
use alternative forms of networking, such as UNIX domain sockets or Windows named
pipes. To create a connection using a UNIX domain socket, simply change the address
to a filename such as this:

s = Listener('/tmp/myconn', authkey=b'peekaboo')

To create a connection using a Windows named pipe, use a filename such as this:

s = Listener(r'\\.\pipe\myconn', authkey=b'peekaboo')

As a general rule, you would not be using multiprocessing to implement public-facing
services. The authkey parameter to Client() and Listener() is there to help authen‐
ticate the end points of the connection. Connection attempts with a bad key raise an
exception. In addition, the module is probably best suited for long-running connections

11.7. Communicating Simply Between Interpreters 

(not a large number of short connections). For example, two interpreters might establish
a connection at startup and keep the connection active for the entire duration of a
problem.
Don’t use multiprocessing if you need more low-level control over aspects of the con‐
nection. For example, if you needed to support timeouts, nonblocking I/O, or anything
similar, you’re probably better off using a different library or implementing such features
on top of sockets instead.
11.8. Implementing Remote Procedure Calls
Problem
You want to implement simple remote procedure call (RPC) on top of a message passing
layer, such as sockets, multiprocessing connections, or ZeroMQ.

Solution
RPC is easy to implement by encoding function requests, arguments, and return values
using  pickle, and passing the pickled byte strings between interpreters. Here is an
example of a simple RPC handler that could be incorporated into a server:

# rpcserver.py

import pickle
class RPCHandler:
    def __init__(self):
        self._functions = { }

    def register_function(self, func):
        self._functions[func.__name__] = func

    def handle_connection(self, connection):
        try:
            while True:
                # Receive a message
                func_name, args, kwargs = pickle.loads(connection.recv())
                # Run the RPC and send a response
                try:
                    r = self._functions[func_name](*args,**kwargs)
                    connection.send(pickle.dumps(r))
                except Exception as e:
                    connection.send(pickle.dumps(e))
        except EOFError:
             pass

To use this handler, you need to add it into a messaging server. There are many possible
choices, but the multiprocessing library provides a simple option. Here is an example
RPC server:

from multiprocessing.connection import Listener
from threading import Thread

def rpc_server(handler, address, authkey):
    sock = Listener(address, authkey=authkey)
    while True:
        client = sock.accept()
        t = Thread(target=handler.handle_connection, args=(client,))
        t.daemon = True
        t.start()

# Some remote functions
def add(x, y):
    return x + y

def sub(x, y):
    return x - y

# Register with a handler
handler = RPCHandler()
handler.register_function(add)
handler.register_function(sub)

# Run the server
rpc_server(handler, ('localhost', 17000), authkey=b'peekaboo')

To access the server from a remote client, you need to create a corresponding RPC proxy
class that forwards requests. For example:

import pickle

class RPCProxy:
    def __init__(self, connection):
        self._connection = connection
    def __getattr__(self, name):
        def do_rpc(*args, **kwargs):
            self._connection.send(pickle.dumps((name, args, kwargs)))
            result = pickle.loads(self._connection.recv())
            if isinstance(result, Exception):
                raise result
            return result
        return do_rpc

To use the proxy, you wrap it around a connection to the server. For example:

>>> from multiprocessing.connection import Client
>>> c = Client(('localhost', 17000), authkey=b'peekaboo')
>>> proxy = RPCProxy(c)
>>> proxy.add(2, 3)

11.8. Implementing Remote Procedure Calls 

5
>>> proxy.sub(2, 3)
-1
>>> proxy.sub([1, 2], 4)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "rpcserver.py", line 37, in do_rpc
    raise result
TypeError: unsupported operand type(s) for -: 'list' and 'int'
>>>

It should be noted that many messaging layers (such as multiprocessing) already se‐
rialize data using pickle. If this is the case, the pickle.dumps() and pickle.loads()
calls can be eliminated.

Discussion
The general idea of the RPCHandler and RPCProxy classes is relatively simple. If a client
wants to call a remote function, such as foo(1, 2, z=3), the proxy class creates a tuple
('foo', (1, 2), {'z': 3}) that contains the function name and arguments. This
tuple is pickled and sent over the connection. This is performed in the do_rpc() closure
that’s returned by the  __getattr__() method of  RPCProxy. The server receives and
unpickles the message, looks up the function name to see if it’s registered, and executes
it with the given arguments. The result (or exception) is then pickled and sent back.
As shown, the example relies on multiprocessing for communication. However, this
approach could be made to work with just about any other messaging system. For ex‐
ample, if you want to implement RPC over ZeroMQ, just replace the connection objects
with an appropriate ZeroMQ socket object.
Given the reliance on pickle, security is a major concern (because a clever hacker can
create messages that make arbitrary functions execute during unpickling). In particular,
you should never allow RPC from untrusted or unauthenticated clients. In particular,
you definitely don’t want to allow access from just any machine on the Internet—this
should really only be used internally, behind a firewall, and not exposed to the rest of
the world.
As an alternative to pickle, you might consider the use of JSON, XML, or some other
data encoding for serialization. For example, this recipe is fairly easy to adapt to JSON
encoding 
if  you  simply  replace  pickle.loads()  and  pickle.dumps()  with
json.loads() and json.dumps(). For example:

# jsonrpcserver.py
import json

class RPCHandler:
    def __init__(self):
        self._functions = { }

    def register_function(self, func):
        self._functions[func.__name__] = func

    def handle_connection(self, connection):
        try:
            while True:
                # Receive a message
                func_name, args, kwargs = json.loads(connection.recv())
                # Run the RPC and send a response
                try:
                    r = self._functions[func_name](*args,**kwargs)
                    connection.send(json.dumps(r))
                except Exception as e:
                    connection.send(json.dumps(str(e)))
        except EOFError:
             pass

# jsonrpcclient.py
import json

class RPCProxy:
    def __init__(self, connection):
        self._connection = connection
    def __getattr__(self, name):
        def do_rpc(*args, **kwargs):
            self._connection.send(json.dumps((name, args, kwargs)))
            result = json.loads(self._connection.recv())
            return result
        return do_rpc

One complicated factor in implementing RPC is how to handle exceptions. At the very
least, the server shouldn’t crash if an exception is raised by a method. However, the
means by which the exception gets reported back to the client requires some study. If
you’re using  pickle, exception instances can often be serialized and reraised in the
client. If you’re using some other protocol, you might have to think of an alternative
approach. At the very least, you would probably want to return the exception string in
the response. This is the approach taken in the JSON example.
For another example of an RPC implementation, it can be useful to look at the imple‐
mentation of the SimpleXMLRPCServer and ServerProxy classes used in XML-RPC, as
described in Recipe 11.6.
11.9. Authenticating Clients Simply
Problem
You want a simple way to authenticate the clients connecting to servers in a distributed
system, but don’t need the complexity of something like SSL.

11.9. Authenticating Clients Simply 

Solution
Simple but effective authentication can be performed by implementing a connection
handshake using the hmac module. Here is sample code:

import hmac
import os

def client_authenticate(connection, secret_key):
    '''
    Authenticate client to a remote service.
    connection represents a network connection.
    secret_key is a key known only to both client/server.
    '''
    message = connection.recv(32)
    hash = hmac.new(secret_key, message)
    digest = hash.digest()
    connection.send(digest)

def server_authenticate(connection, secret_key):
    '''
    Request client authentication.
    '''
    message = os.urandom(32)
    connection.send(message)
    hash = hmac.new(secret_key, message)
    digest = hash.digest()
    response = connection.recv(len(digest))
    return hmac.compare_digest(digest,response)

The general idea is that upon connection, the server presents the client with a message
of random bytes (returned by os.urandom(), in this case). The client and server both
compute a cryptographic hash of the random data using hmac and a secret key known
only to both ends. The client sends its computed digest back to the server, where it is
compared and used to decide whether or not to accept or reject the connection.
Comparison  of  resulting  digests  should  be  performed  using  the  hmac.compare_di
gest() function. This function has been written in a way that avoids timing-analysis-
based attacks and should be used instead of a normal comparison operator (==).
To use these functions, you would incorporate them into existing networking or mes‐
saging code. For example, with sockets, the server code might look something like this:

from socket import socket, AF_INET, SOCK_STREAM

secret_key = b'peekaboo'
def echo_handler(client_sock):
    if not server_authenticate(client_sock, secret_key):
        client_sock.close()
        return
    while True:

        msg = client_sock.recv(8192)
        if not msg:
            break
        client_sock.sendall(msg)

def echo_server(address):
    s = socket(AF_INET, SOCK_STREAM)
    s.bind(address)
    s.listen(5)
    while True:
        c,a = s.accept()
        echo_handler(c)

echo_server(('', 18000))

Within a client, you would do this:

from socket import socket, AF_INET, SOCK_STREAM

secret_key = b'peekaboo'

s = socket(AF_INET, SOCK_STREAM)
s.connect(('localhost', 18000))
client_authenticate(s, secret_key)
s.send(b'Hello World')
resp = s.recv(1024)
...

Discussion
A common use of hmac authentication is in internal messaging systems and interprocess
communication. For example, if you are writing a system that involves multiple pro‐
cesses communicating across a cluster of machines, you can use this approach to make
sure that only allowed processes are allowed to connect to one another. In fact, HMAC-
based authentication is used internally by the multiprocessing library when it sets up
communication with subprocesses.
It’s important to stress that authenticating a connection is not the same as encryption.
Subsequent communication on an authenticated connection is sent in the clear, and
would be visible to anyone inclined to sniff the traffic (although the secret key known
to both sides is never transmitted).
The authentication algorithm used by hmac is based on cryptographic hashing functions,
such as MD5 and SHA-1, and is described in detail in IETF RFC 2104. 

11.9. Authenticating Clients Simply 

11.10. Adding SSL to Network Services
Problem
You want to implement a network service involving sockets where servers and clients
authenticate themselves and encrypt the transmitted data using SSL.

Solution
The ssl module provides support for adding SSL to low-level socket connections. In
particular, the ssl.wrap_socket() function takes an existing socket and wraps an SSL
layer around it. For example, here’s an example of a simple echo server that presents a
server certificate to connecting clients:

from socket import socket, AF_INET, SOCK_STREAM
import ssl

KEYFILE = 'server_key.pem'   # Private key of the server
CERTFILE = 'server_cert.pem' # Server certificate (given to client)

def echo_client(s):
    while True:
        data = s.recv(8192)
        if data == b'':
            break
        s.send(data)
    s.close()
    print('Connection closed')

def echo_server(address):
    s = socket(AF_INET, SOCK_STREAM)
    s.bind(address)
    s.listen(1)

    # Wrap with an SSL layer requiring client certs
    s_ssl = ssl.wrap_socket(s,
                            keyfile=KEYFILE,
                            certfile=CERTFILE,
                            server_side=True
                            )
    # Wait for connections
    while True:
        try:
            c,a = s_ssl.accept()
            print('Got connection', c, a)
            echo_client(c)
        except Exception as e:
            print('{}: {}'.format(e.__class__.__name__, e))

echo_server(('', 20000))

Here’s an interactive session that shows how to connect to the server as a client. The
client requires the server to present its certificate and verifies it:

>>> from socket import socket, AF_INET, SOCK_STREAM
>>> import ssl
>>> s = socket(AF_INET, SOCK_STREAM)
>>> s_ssl = ssl.wrap_socket(s,
...                         cert_reqs=ssl.CERT_REQUIRED,
...                         ca_certs = 'server_cert.pem')
>>> s_ssl.connect(('localhost', 20000))
>>> s_ssl.send(b'Hello World?')
12
>>> s_ssl.recv(8192)
b'Hello World?'
>>>

The problem with all of this low-level socket hacking is that it doesn’t play well with
existing network services already implemented in the standard library. For example,
most server code (HTTP, XML-RPC, etc.) is actually based on the socketserver library.
Client code is also implemented at a higher level. It is possible to add SSL to existing
services, but a slightly different approach is needed.
First, for servers, SSL can be added through the use of a mixin class like this:

import ssl

class SSLMixin:
    '''
    Mixin class that adds support for SSL to existing servers based
    on the socketserver module.
    '''
    def __init__(self, *args,
                 keyfile=None, certfile=None, ca_certs=None,
                 cert_reqs=ssl.NONE,
                 **kwargs):
        self._keyfile = keyfile
        self._certfile = certfile
        self._ca_certs = ca_certs
        self._cert_reqs = cert_reqs
        super().__init__(*args, **kwargs)

    def get_request(self):
        client, addr = super().get_request()
        client_ssl = ssl.wrap_socket(client,
                                     keyfile = self._keyfile,
                                     certfile = self._certfile,
                                     ca_certs = self._ca_certs,
                                     cert_reqs = self._cert_reqs,
                                     server_side = True)
        return client_ssl, addr

11.10. Adding SSL to Network Services 

To use this mixin class, you can mix it with other server classes. For example, here’s an
example of defining an XML-RPC server that operates over SSL:

# XML-RPC server with SSL

from xmlrpc.server import SimpleXMLRPCServer

class SSLSimpleXMLRPCServer(SSLMixin, SimpleXMLRPCServer):
    pass

Here’s the XML-RPC server from Recipe 11.6 modified only slightly to use SSL:

import ssl
from xmlrpc.server import SimpleXMLRPCServer
from sslmixin import SSLMixin

class SSLSimpleXMLRPCServer(SSLMixin, SimpleXMLRPCServer):
    pass

class KeyValueServer:
    _rpc_methods_ = ['get', 'set', 'delete', 'exists', 'keys']
    def __init__(self, *args, **kwargs):
        self._data = {}
        self._serv = SSLSimpleXMLRPCServer(*args, allow_none=True, **kwargs)
        for name in self._rpc_methods_:
            self._serv.register_function(getattr(self, name))

    def get(self, name):
        return self._data[name]

    def set(self, name, value):
        self._data[name] = value

    def delete(self, name):
        del self._data[name]

    def exists(self, name):
        return name in self._data

    def keys(self):
        return list(self._data)

    def serve_forever(self):
        self._serv.serve_forever()

if __name__ == '__main__':
    KEYFILE='server_key.pem'    # Private key of the server
    CERTFILE='server_cert.pem'  # Server certificate
    kvserv = KeyValueServer(('', 15000),
                            keyfile=KEYFILE,
                            certfile=CERTFILE),
    kvserv.serve_forever()

To use this server, you can connect using the normal xmlrpc.client module. Just spec‐
ify a https: in the URL. For example:

>>> from xmlrpc.client import ServerProxy
>>> s = ServerProxy('https://localhost:15000', allow_none=True)
>>> s.set('foo','bar')
>>> s.set('spam', [1, 2, 3])
>>> s.keys()
['spam', 'foo']
>>> s.get('foo')
'bar'
>>> s.get('spam')
[1, 2, 3]
>>> s.delete('spam')
>>> s.exists('spam')
False
>>>

One complicated issue with SSL clients is performing extra steps to verify the server
certificate or to present a server with client credentials (such as a client certificate).
Unfortunately, there seems to be no standardized way to accomplish this, so research is
often required. However, here is an example of how to set up a secure XML-RPC con‐
nection that verifies the server’s certificate:

from xmlrpc.client import SafeTransport, ServerProxy
import ssl

class VerifyCertSafeTransport(SafeTransport):
    def __init__(self, cafile, certfile=None, keyfile=None):
        SafeTransport.__init__(self)
        self._ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLSv1)
        self._ssl_context.load_verify_locations(cafile)
        if cert:
            self._ssl_context.load_cert_chain(certfile, keyfile)
        self._ssl_context.verify_mode = ssl.CERT_REQUIRED

    def make_connection(self, host):
        # Items in the passed dictionary are passed as keyword
        # arguments to the http.client.HTTPSConnection() constructor.
        # The context argument allows an ssl.SSLContext instance to
        # be passed with information about the SSL configuration
        s = super().make_connection((host, {'context': self._ssl_context}))

        return s

# Create the client proxy
s = ServerProxy('https://localhost:15000',
                transport=VerifyCertSafeTransport('server_cert.pem'),
                allow_none=True)

11.10. Adding SSL to Network Services 

As shown, the server presents a certificate to the client and the client verifies it. This
verification can go both directions. If the server wants to verify the client, change the
server startup to the following:
if __name__ == '__main__':
    KEYFILE='server_key.pem'   # Private key of the server
    CERTFILE='server_cert.pem' # Server certificate
    CA_CERTS='client_cert.pem' # Certificates of accepted clients

    kvserv = KeyValueServer(('', 15000),
                            keyfile=KEYFILE,
                            certfile=CERTFILE,
                            ca_certs=CA_CERTS,
                            cert_reqs=ssl.CERT_REQUIRED,
                            )
    kvserv.serve_forever()

To make the XML-RPC client present its certificates, change the ServerProxy initiali‐
zation to this:

# Create the client proxy
s = ServerProxy('https://localhost:15000',
                transport=VerifyCertSafeTransport('server_cert.pem',
                                                  'client_cert.pem',
                                                  'client_key.pem'),
                allow_none=True)

Discussion
Getting this recipe to work will test your system configuration skills and understanding
of SSL. Perhaps the biggest challenge is simply getting the initial configuration of keys,
certificates, and other matters in order.
To clarify what’s required, each endpoint of an SSL connection typically has a private
key and a signed certificate file. The certificate file contains the public key and is pre‐
sented to the remote peer on each connection. For public servers, certificates are nor‐
mally signed by a certificate authority such as Verisign, Equifax, or similar organization
(something that costs money). To verify server certificates, clients maintain a file con‐
taining the certificates of trusted certificate authorities. For example, web browsers
maintain certificates corresponding to the major certificate authorities and use them to
verify the integrity of certificates presented by web servers during HTTPS connections.
For the purposes of this recipe, you can create what’s known as a self-signed certificate.
Here’s how you do it:

bash % openssl req -new -x509 -days 365 -nodes -out server_cert.pem \
           -keyout server_key.pem
Generating a 1024 bit RSA private key
..........................................++++++
...++++++

writing new private key to 'server_key.pem'
 -----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
 -----
Country Name (2 letter code) [AU]:US
State or Province Name (full name) [Some-State]:Illinois
Locality Name (eg, city) []:Chicago
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Dabeaz, LLC
Organizational Unit Name (eg, section) []:
Common Name (eg, YOUR name) []:localhost
Email Address []:
bash %

When creating the certificate, the values for the various fields are often arbitrary. How‐
ever, the “Common Name” field often contains the DNS hostname of servers. If you’re
just testing things out on your own machine, use “localhost.” Otherwise, use the domain
name of the machine that’s going to run the server.
As a result of this configuration, you will have a server_key.pem file that contains the
private key. It looks like this:

    -----BEGIN RSA PRIVATE KEY-----
    MIICXQIBAAKBgQCZrCNLoEyAKF+f9UNcFaz5Osa6jf7qkbUl8si5xQrY3ZYC7juu
    nL1dZLn/VbEFIITaUOgvBtPv1qUWTJGwga62VSG1oFE0ODIx3g2Nh4sRf+rySsx2
    L4442nx0z4O5vJQ7k6eRNHAZUUnCL50+YvjyLyt7ryLSjSuKhCcJsbZgPwIDAQAB
    AoGAB5evrr7eyL4160tM5rHTeATlaLY3UBOe5Z8XN8Z6gLiB/ucSX9AysviVD/6F
    3oD6z2aL8jbeJc1vHqjt0dC2dwwm32vVl8mRdyoAsQpWmiqXrkvP4Bsl04VpBeHw
    Qt8xNSW9SFhceL3LEvw9M8i9MV39viih1ILyH8OuHdvJyFECQQDLEjl2d2ppxND9
    PoLqVFAirDfX2JnLTdWbc+M11a9Jdn3hKF8TcxfEnFVs5Gav1MusicY5KB0ylYPb
    YbTvqKc7AkEAwbnRBO2VYEZsJZp2X0IZqP9ovWokkpYx+PE4+c6MySDgaMcigL7v
    WDIHJG1CHudD09GbqENasDzyb2HAIW4CzQJBAKDdkv+xoW6gJx42Auc2WzTcUHCA
    eXR/+BLpPrhKykzbvOQ8YvS5W764SUO1u1LWs3G+wnRMvrRvlMCZKgggBjkCQQCG
    Jewto2+a+WkOKQXrNNScCDE5aPTmZQc5waCYq4UmCZQcOjkUOiN3ST1U5iuxRqfb
    V/yX6fw0qh+fLWtkOs/JAkA+okMSxZwqRtfgOFGBfwQ8/iKrnizeanTQ3L6scFXI
    CHZXdJ3XQ6qUmNxNn7iJ7S/LDawo1QfWkCfD9FYoxBlg
    -----END RSA PRIVATE KEY-----

The server certificate in server_cert.pem looks similar:

    -----BEGIN CERTIFICATE-----
    MIIC+DCCAmGgAwIBAgIJAPMd+vi45js3MA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNV
    BAYTAlVTMREwDwYDVQQIEwhJbGxpbm9pczEQMA4GA1UEBxMHQ2hpY2FnbzEUMBIG
    A1UEChMLRGFiZWF6LCBMTEMxEjAQBgNVBAMTCWxvY2FsaG9zdDAeFw0xMzAxMTEx
    ODQyMjdaFw0xNDAxMTExODQyMjdaMFwxCzAJBgNVBAYTAlVTMREwDwYDVQQIEwhJ
    bGxpbm9pczEQMA4GA1UEBxMHQ2hpY2FnbzEUMBIGA1UEChMLRGFiZWF6LCBMTEMx
    EjAQBgNVBAMTCWxvY2FsaG9zdDCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEA
    mawjS6BMgChfn/VDXBWs+TrGuo3+6pG1JfLIucUK2N2WAu47rpy9XWS5/1WxBSCE
    2lDoLwbT79alFkyRsIGutlUhtaBRNDgyMd4NjYeLEX/q8krMdi+OONp8dM+DubyU

11.10. Adding SSL to Network Services 

    O5OnkTRwGVFJwi+dPmL48i8re68i0o0rioQnCbG2YD8CAwEAAaOBwTCBvjAdBgNV
    HQ4EFgQUrtoLHHgXiDZTr26NMmgKJLJLFtIwgY4GA1UdIwSBhjCBg4AUrtoLHHgX
    iDZTr26NMmgKJLJLFtKhYKReMFwxCzAJBgNVBAYTAlVTMREwDwYDVQQIEwhJbGxp
    bm9pczEQMA4GA1UEBxMHQ2hpY2FnbzEUMBIGA1UEChMLRGFiZWF6LCBMTEMxEjAQ
    BgNVBAMTCWxvY2FsaG9zdIIJAPMd+vi45js3MAwGA1UdEwQFMAMBAf8wDQYJKoZI
    hvcNAQEFBQADgYEAFci+dqvMG4xF8UTnbGVvZJPIzJDRee6Nbt6AHQo9pOdAIMAu
    WsGCplSOaDNdKKzl+b2UT2Zp3AIW4Qd51bouSNnR4M/gnr9ZD1ZctFd3jS+C5XRp
    D3vvcW5lAnCCC80P6rXy7d7hTeFu5EYKtRGXNvVNd/06NALGDflrrOwxF3Y=
    -----END CERTIFICATE-----

In server-related code, both the private key and certificate file will be presented to the
various SSL-related wrapping functions. The certificate is what gets presented to clients.
The private key should be protected and remains on the server.
In client-related code, a special file of valid certificate authorities needs to be maintained
to verify the server’s certificate. If you have no such file, then at the very least, you can
put a copy of the server’s certificate on the client machine and use that as a means for
verification. During connection, the server will present its certificate, and then you’ll
use the stored certificate you already have to verify that it’s correct.
Servers can also elect to verify the identity of clients. To do that, clients need to have
their own private key and certificate key. The server would also need to maintain a file
of trusted certificate authorities for verifying the client certificates.
If you intend to add SSL support to a network service for real, this recipe really only
gives a small taste of how to set it up. You will definitely want to consult the documen‐
tation for more of the finer points. Be prepared to spend a significant amount of time
experimenting with it to get things to work.
11.11. Passing a Socket File Descriptor Between Processes
Problem
You have multiple Python interpreter processes running and you want to pass an open
file descriptor from one interpreter to the other. For instance, perhaps there is a server
process that is responsible for receiving connections, but the actual servicing of clients
is to be handled by a different interpreter.

Solution
To pass a file descriptor between processes, you first need to connect the processes
together. On Unix machines, you might use a Unix domain socket, whereas on Win‐
dows,  you  could  use  a  named  pipe.  However,  rather  than  deal  with  such  low-level
mechanics,  it  is  often  easier  to  use  the  multiprocessing  module  to  set  up  such  a
connection.

Once a connection is established, you can use the send_handle() and recv_handle()
functions in multiprocessing.reduction to send file descriptors between processes.
The following example illustrates the basics:

import multiprocessing
from multiprocessing.reduction import recv_handle, send_handle
import socket

def worker(in_p, out_p):
    out_p.close()
    while True:
        fd = recv_handle(in_p)
        print('CHILD: GOT FD', fd)
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM, fileno=fd) as s:
            while True:
                msg = s.recv(1024)
                if not msg:
                    break
                print('CHILD: RECV {!r}'.format(msg))
                s.send(msg)

def server(address, in_p, out_p, worker_pid):
    in_p.close()
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)
    s.bind(address)
    s.listen(1)
    while True:
        client, addr = s.accept()
        print('SERVER: Got connection from', addr)
        send_handle(out_p, client.fileno(), worker_pid)
        client.close()

if __name__ == '__main__':
    c1, c2 = multiprocessing.Pipe()
    worker_p = multiprocessing.Process(target=worker, args=(c1,c2))
    worker_p.start()

    server_p = multiprocessing.Process(target=server,
                  args=(('', 15000), c1, c2, worker_p.pid))
    server_p.start()

    c1.close()
    c2.close()

In this example, two processes are spawned and connected by a multiprocessing Pipe
object. The server process opens a socket and waits for client connections. The worker
process merely waits to receive a file descriptor on the pipe using recv_handle(). When
the server receives a connection, it sends the resulting socket file descriptor to the worker

11.11. Passing a Socket File Descriptor Between Processes 

using send_handle(). The worker takes over the socket and echoes data back to the
client until the connection is closed.
If you connect to the running server using Telnet or a similar tool, here is an example
of what you might see:

    bash % python3 passfd.py
    SERVER: Got connection from ('127.0.0.1', 55543)
    CHILD: GOT FD 7
    CHILD: RECV b'Hello\r\n'
    CHILD: RECV b'World\r\n'

The most important part of this example is the fact that the client socket accepted in the
server is actually serviced by a completely different process. The server merely hands it
off, closes it, and waits for the next connection.

Discussion
Passing file descriptors between processes is something that many programmers don’t
even realize is possible. However, it can sometimes be a useful tool in building scalable
systems. For example, on a multicore machine, you could have multiple instances of the
Python interpreter and use file descriptor passing to more evenly balance the number
of clients being handled by each interpreter.
The send_handle() and recv_handle() functions shown in the solution really only
work with multiprocessing connections. Instead of using a pipe, you can connect in‐
terpreters as shown in Recipe 11.7, and it will work as long as you use UNIX domain
sockets or Windows pipes. For example, you could implement the server and worker
as completely separate programs to be started separately. Here is the implementation of
the server:

# servermp.py
from multiprocessing.connection import Listener
from multiprocessing.reduction import send_handle
import socket

def server(work_address, port):
    # Wait for the worker to connect
    work_serv = Listener(work_address, authkey=b'peekaboo')
    worker = work_serv.accept()
    worker_pid = worker.recv()

    # Now run a TCP/IP server and send clients to worker
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)
    s.bind(('', port))
    s.listen(1)
    while True:
        client, addr = s.accept()
        print('SERVER: Got connection from', addr)

        send_handle(worker, client.fileno(), worker_pid)
        client.close()

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 3:
        print('Usage: server.py server_address port', file=sys.stderr)
        raise SystemExit(1)

    server(sys.argv[1], int(sys.argv[2]))

To run this server, you would run a command such as python3 servermp.py /tmp/
servconn 15000. Here is the corresponding client code:

# workermp.py

from multiprocessing.connection import Client
from multiprocessing.reduction import recv_handle
import os
from socket import socket, AF_INET, SOCK_STREAM

def worker(server_address):
    serv = Client(server_address, authkey=b'peekaboo')
    serv.send(os.getpid())
    while True:
        fd = recv_handle(serv)
        print('WORKER: GOT FD', fd)
        with socket(AF_INET, SOCK_STREAM, fileno=fd) as client:
            while True:
                msg = client.recv(1024)
                if not msg:
                    break
                print('WORKER: RECV {!r}'.format(msg))
                client.send(msg)

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 2:
        print('Usage: worker.py server_address', file=sys.stderr)
        raise SystemExit(1)

    worker(sys.argv[1])

To run the worker, you would type python3 workermp.py /tmp/servconn. The result‐
ing operation should be exactly the same as the example that used Pipe().
Under the covers, file descriptor passing involves creating a UNIX domain socket and
the sendmsg() method of sockets. Since this technique is not widely known, here is a
different implementation of the server that shows how to pass descriptors using sockets:

# server.py
import socket

11.11. Passing a Socket File Descriptor Between Processes 

import struct

def send_fd(sock, fd):
    '''
    Send a single file descriptor.
    '''
    sock.sendmsg([b'x'],
                 [(socket.SOL_SOCKET, socket.SCM_RIGHTS, struct.pack('i', fd))])
    ack = sock.recv(2)
    assert ack == b'OK'

def server(work_address, port):
    # Wait for the worker to connect
    work_serv = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    work_serv.bind(work_address)
    work_serv.listen(1)
    worker, addr = work_serv.accept()

    # Now run a TCP/IP server and send clients to worker
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)
    s.bind(('',port))
    s.listen(1)
    while True:
        client, addr = s.accept()
        print('SERVER: Got connection from', addr)
        send_fd(worker, client.fileno())
        client.close()

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 3:
        print('Usage: server.py server_address port', file=sys.stderr)
        raise SystemExit(1)

    server(sys.argv[1], int(sys.argv[2]))

Here is an implementation of the worker using sockets:

# worker.py
import socket
import struct

def recv_fd(sock):
    '''
    Receive a single file descriptor
    '''
    msg, ancdata, flags, addr = sock.recvmsg(1,
                                     socket.CMSG_LEN(struct.calcsize('i')))

    cmsg_level, cmsg_type, cmsg_data = ancdata[0]
    assert cmsg_level == socket.SOL_SOCKET and cmsg_type == socket.SCM_RIGHTS
    sock.sendall(b'OK')

    return struct.unpack('i', cmsg_data)[0]

def worker(server_address):
    serv = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    serv.connect(server_address)
    while True:
        fd = recv_fd(serv)
        print('WORKER: GOT FD', fd)
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM, fileno=fd) as client:
            while True:
                msg = client.recv(1024)
                if not msg:
                    break
                print('WORKER: RECV {!r}'.format(msg))
                client.send(msg)

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 2:
        print('Usage: worker.py server_address', file=sys.stderr)
        raise SystemExit(1)

    worker(sys.argv[1])

If you are going to use file-descriptor passing in your program, it is advisable to read
more about it in an advanced text, such as Unix Network Programming by W. Richard
Stevens  (Prentice  Hall,  1990).  Passing  file  descriptors  on  Windows  uses  a  different
technique than Unix (not shown). For that platform, it is advisable to study the source
code to multiprocessing.reduction in close detail to see how it works.
11.12. Understanding Event-Driven I/O
Problem
You have heard about packages based on “event-driven” or “asynchronous” I/O, but
you’re not entirely sure what it means, how it actually works under the covers, or how
it might impact your program if you use it.

Solution
At a fundamental level, event-driven I/O is a technique that takes basic I/O operations
(e.g., reads and writes) and converts them into events that must be handled by your
program. For example, whenever data was received on a socket, it turns into a “receive”
event that is handled by some sort of callback method or function that you supply to
respond to it. As a possible starting point, an event-driven framework might start with
a base class that implements a series of basic event handler methods like this:

11.12. Understanding Event-Driven I/O 

class EventHandler:
    def fileno(self):
        'Return the associated file descriptor'
        raise NotImplemented('must implement')

    def wants_to_receive(self):
        'Return True if receiving is allowed'
        return False

    def handle_receive(self):
        'Perform the receive operation'
        pass

    def wants_to_send(self):
        'Return True if sending is requested'
        return False

    def handle_send(self):
        'Send outgoing data'
        pass

Instances of this class then get plugged into an event loop that looks like this:

import select

def event_loop(handlers):
    while True:
        wants_recv = [h for h in handlers if h.wants_to_receive()]
        wants_send = [h for h in handlers if h.wants_to_send()]
        can_recv, can_send, _ = select.select(wants_recv, wants_send, [])
        for h in can_recv:
            h.handle_receive()
        for h in can_send:
            h.handle_send()

That’s it! The key to the event loop is the select() call, which polls file descriptors for
activity. Prior to calling select(), the event loop simply queries all of the handlers to
see which ones want to receive or send. It then supplies the resulting lists to select().
As a result, select() returns the list of objects that are ready to receive or send. The
corresponding handle_receive() or handle_send() methods are triggered.
To write applications, specific instances of EventHandler classes are created. For ex‐
ample, here are two simple handlers that illustrate two UDP-based network services:

import socket
import time

class UDPServer(EventHandler):
    def __init__(self, address):
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.sock.bind(address)

    def fileno(self):
        return self.sock.fileno()

    def wants_to_receive(self):
        return True

class UDPTimeServer(UDPServer):
    def handle_receive(self):
        msg, addr = self.sock.recvfrom(1)
        self.sock.sendto(time.ctime().encode('ascii'), addr)

class UDPEchoServer(UDPServer):
    def handle_receive(self):
        msg, addr = self.sock.recvfrom(8192)
        self.sock.sendto(msg, addr)

if __name__ == '__main__':
    handlers = [ UDPTimeServer(('',14000)), UDPEchoServer(('',15000))  ]
    event_loop(handlers)

To test this code, you can try connecting to it from another Python interpreter:

>>> from socket import *
>>> s = socket(AF_INET, SOCK_DGRAM)
>>> s.sendto(b'',('localhost',14000))
0
>>> s.recvfrom(128)
(b'Tue Sep 18 14:29:23 2012', ('127.0.0.1', 14000))
>>> s.sendto(b'Hello',('localhost',15000))
5
>>> s.recvfrom(128)
(b'Hello', ('127.0.0.1', 15000))
>>>

Implementing a TCP server is somewhat more complex, since each client involves the
instantiation of a new handler object. Here is an example of a TCP echo client.

class TCPServer(EventHandler):
    def __init__(self, address, client_handler, handler_list):
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)
        self.sock.bind(address)
        self.sock.listen(1)
        self.client_handler = client_handler
        self.handler_list = handler_list

    def fileno(self):
        return self.sock.fileno()

    def wants_to_receive(self):
        return True

11.12. Understanding Event-Driven I/O 

    def handle_receive(self):
        client, addr = self.sock.accept()
        # Add the client to the event loop's handler list
        self.handler_list.append(self.client_handler(client, self.handler_list))

class TCPClient(EventHandler):
    def __init__(self, sock, handler_list):
        self.sock = sock
        self.handler_list = handler_list
        self.outgoing = bytearray()

    def fileno(self):
        return self.sock.fileno()

    def close(self):
        self.sock.close()
        # Remove myself from the event loop's handler list
        self.handler_list.remove(self)

    def wants_to_send(self):
        return True if self.outgoing else False

    def handle_send(self):
        nsent = self.sock.send(self.outgoing)
        self.outgoing = self.outgoing[nsent:]

class TCPEchoClient(TCPClient):
    def wants_to_receive(self):
        return True

    def handle_receive(self):
        data = self.sock.recv(8192)
        if not data:
            self.close()
        else:
            self.outgoing.extend(data)

if __name__ == '__main__':
   handlers = []
   handlers.append(TCPServer(('',16000), TCPEchoClient, handlers))
   event_loop(handlers)

The key to the TCP example is the addition and removal of clients from the handler list.
On each connection, a new handler is created for the client and added to the list. When
the connection is closed, each client must take care to remove themselves from the list.
If you run this program and try connecting with Telnet or some similar tool, you’ll see
it echoing received data back to you. It should easily handle multiple clients.

Discussion
Virtually all event-driven frameworks operate in a manner that is similar to that shown
in the solution. The actual implementation details and overall software architecture
might vary greatly, but at the core, there is a polling loop that checks sockets for activity
and which performs operations in response.
One potential benefit of event-driven I/O is that it can handle a very large number of
simultaneous  connections  without  ever  using  threads  or  processes.  That  is,  the  se
lect() call (or equivalent) can be used to monitor hundreds or thousands of sockets
and respond to events occuring on any of them. Events are handled one at a time by the
event loop, without the need for any other concurrency primitives.
The downside to event-driven I/O is that there is no true concurrency involved. If any
of the event handler methods blocks or performs a long-running calculation, it blocks
the progress of everything. There is also the problem of calling out to library functions
that aren’t written in an event-driven style. There is always the risk that some library
call will block, causing the event loop to stall.
Problems with blocking or long-running calculations can be solved by sending the work
out to a separate thread or process. However, coordinating threads and processes with
an event loop is tricky. Here is an example of code that will do it using the  concur
rent.futures module:

from concurrent.futures import ThreadPoolExecutor
import os

class ThreadPoolHandler(EventHandler):
    def __init__(self, nworkers):
        if os.name == 'posix':
            self.signal_done_sock, self.done_sock = socket.socketpair()
        else:
            server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            server.bind(('127.0.0.1', 0))
            server.listen(1)
            self.signal_done_sock = socket.socket(socket.AF_INET,
                                                  socket.SOCK_STREAM)
            self.signal_done_sock.connect(server.getsockname())
            self.done_sock, _ = server.accept()
            server.close()

        self.pending = []
        self.pool = ThreadPoolExecutor(nworkers)

    def fileno(self):
        return self.done_sock.fileno()

    # Callback that executes when the thread is done
    def _complete(self, callback, r):

11.12. Understanding Event-Driven I/O 

        self.pending.append((callback, r.result()))
        self.signal_done_sock.send(b'x')

    # Run a function in a thread pool
    def run(self, func, args=(), kwargs={},*,callback):
        r = self.pool.submit(func, *args, **kwargs)
        r.add_done_callback(lambda r: self._complete(callback, r))

    def wants_to_receive(self):
        return True

    # Run callback functions of completed work
    def handle_receive(self):
        # Invoke all pending callback functions
        for callback, result in self.pending:
            callback(result)
            self.done_sock.recv(1)
        self.pending = []

In this code, the run() method is used to submit work to the pool along with a callback
function that should be triggered upon completion. The actual work is then submitted
to a ThreadPoolExecutor instance. However, a really tricky problem concerns the co‐
ordination of the computed result and the event loop. To do this, a pair of sockets are
created under the covers and used as a kind of signaling mechanism. When work is
completed by the thread pool, it executes the _complete() method in the class. This
method queues up the pending callback and result before writing a byte of data on one
of these sockets. The fileno() method is programmed to return the other socket. Thus,
when this byte is written, it will signal to the event loop that something has happened.
The handle_receive() method, when triggered, will then execute all of the callback
functions for previously submitted work. Frankly, it’s enough to make one’s head spin.
Here is a simple server that shows how to use the thread pool to carry out a long-running
calculation:

# A really bad Fibonacci implementation
def fib(n):
    if n < 2:
        return 1
    else:
        return fib(n - 1) + fib(n - 2)

class UDPFibServer(UDPServer):
    def handle_receive(self):
        msg, addr = self.sock.recvfrom(128)
        n = int(msg)
        pool.run(fib, (n,), callback=lambda r: self.respond(r, addr))

    def respond(self, result, addr):
        self.sock.sendto(str(result).encode('ascii'), addr)

if __name__ == '__main__':
    pool = ThreadPoolHandler(16)
    handlers = [ pool, UDPFibServer(('',16000))]
    event_loop(handlers)

To try this server, simply run it and try some experiments with another Python program:

from socket import *
sock = socket(AF_INET, SOCK_DGRAM)
for x in range(40):
    sock.sendto(str(x).encode('ascii'), ('localhost', 16000))
    resp = sock.recvfrom(8192)
    print(resp[0])

You should be able to run this program repeatedly from many different windows and
have it operate without stalling other programs, even though it gets slower and slower
as the numbers get larger.
Having gone through this recipe, should you use its code? Probably not. Instead, you
should look for a more fully developed framework that accomplishes the same task.
However, if you understand the basic concepts presented here, you’ll understand the
core techniques used to make such frameworks operate. As an alternative to callback-
based programming, event-driven code will sometimes use coroutines. See Recipe 12.12
for an example.
11.13. Sending and Receiving Large Arrays
Problem
You want to send and receive large arrays of contiguous data across a network connec‐
tion, making as few copies of the data as possible.

Solution
The following functions utilize memoryviews to send and receive large arrays:

# zerocopy.py

def send_from(arr, dest):
    view = memoryview(arr).cast('B')
    while len(view):
        nsent = dest.send(view)
        view = view[nsent:]

def recv_into(arr, source):
    view = memoryview(arr).cast('B')
    while len(view):
        nrecv = source.recv_into(view)
        view = view[nrecv:]

11.13. Sending and Receiving Large Arrays 

To test the program, first create a server and client program connected over a socket.
In the server:

>>> from socket import *
>>> s = socket(AF_INET, SOCK_STREAM)
>>> s.bind(('', 25000))
>>> s.listen(1)
>>> c,a = s.accept()
>>>

In the client (in a separate interpreter):

>>> from socket import *
>>> c = socket(AF_INET, SOCK_STREAM)
>>> c.connect(('localhost', 25000))
>>>

Now, the whole idea of this recipe is that you can blast a huge array through the con‐
nection. In this case, arrays might be created by the array module or perhaps numpy.
For example:
# Server
>>> import numpy
>>> a = numpy.arange(0.0, 50000000.0)
>>> send_from(a, c)
>>>

# Client
>>> import numpy
>>> a = numpy.zeros(shape=50000000, dtype=float)
>>> a[0:10]
array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])
>>> recv_into(a, c)
>>> a[0:10]
array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])
>>>

Discussion
In data-intensive distributed computing and parallel programming applications, it’s not
uncommon to write programs that need to send/receive large chunks of data. However,
to do this, you somehow need to reduce the data down to raw bytes for use with low-
level network functions. You may also need to slice the data into chunks, since most
network-related functions aren’t able to send or receive huge blocks of data entirely all
at once.
One approach is to serialize the data in some way—possibly by converting into a byte
string. However, this usually ends up making a copy of the data. Even if you do this
piecemeal, your code still ends up making a lot of little copies.

This recipe gets around this by playing a sneaky trick with memoryviews. Essentially, a
memoryview is an overlay of an existing array. Not only that, memoryviews can be cast
to different types to allow interpretation of the data in a different manner. This is the
purpose of the following statement:
view = memoryview(arr).cast('B')

It takes an array arr and casts into a memoryview of unsigned bytes.
In this form, the view can be passed to socket-related functions, such as sock.send()
or send.recv_into(). Under the covers, those methods are able to work directly with
the  memory  region.  For  example,  sock.send()  sends  data  directly  from  memory
without a copy. send.recv_into() uses the memoryview as the input buffer for the
receive operation.
The remaining complication is the fact that the socket functions may only work with
partial data. In general, it will take many different send() and recv_into() calls to
transmit the entire array. Not to worry. After each operation, the view is sliced by the
number of sent or received bytes to produce a new view. The new view is also a memory
overlay. Thus, no copies are made.
One issue here is that the receiver has to know in advance how much data will be sent
so that it can either preallocate an array or verify that it can receive the data into an
existing array. If this is a problem, the sender could always arrange to send the size first,
followed by the array data.

11.13. Sending and Receiving Large Arrays 


CHAPTER 12
Concurrency

Python has long supported different approaches to concurrent programming, including
programming with threads, launching subprocesses, and various tricks involving gen‐
erator functions. In this chapter, recipes related to various aspects of concurrent pro‐
gramming are presented, including common thread programming techniques and ap‐
proaches for parallel processing.
As experienced programmers know, concurrent programming is fraught with potential
peril. Thus, a major focus of this chapter is on recipes that tend to lead to more reliable
and debuggable code.
12.1. Starting and Stopping Threads
Problem
You want to create and destroy threads for concurrent execution of code.

Solution
The threading library can be used to execute any Python callable in its own thread. To
do this, you create a Thread instance and supply the callable that you wish to execute
as a target. Here is a simple example:

# Code to execute in an independent thread
import time
def countdown(n):
    while n > 0:
        print('T-minus', n)
        n -= 1
        time.sleep(5)

# Create and launch a thread
from threading import Thread
t = Thread(target=countdown, args=(10,))
t.start()

When you create a thread instance, it doesn’t start executing until you invoke its start()
method (which invokes the target function with the arguments you supplied).
Threads are executed in their own system-level thread (e.g., a POSIX thread or Windows
threads) that is fully managed by the host operating system. Once started, threads run
independently until the target function returns. You can query a thread instance to see
if it’s still running:

if t.is_alive():
    print('Still running')
else:
    print('Completed')

You can also request to join with a thread, which waits for it to terminate:

    t.join()

The interpreter remains running until all threads terminate. For long-running threads
or background tasks that run forever, you should consider making the thread daemonic.
For example:

t = Thread(target=countdown, args=(10,), daemon=True)
t.start()

Daemonic threads can’t be joined. However, they are destroyed automatically when the
main thread terminates.
Beyond the two operations shown, there aren’t many other things you can do with
threads. For example, there are no operations to terminate a thread, signal a thread,
adjust its scheduling, or perform any other high-level operations. If you want these
features, you need to build them yourself.
If you want to be able to terminate threads, the thread must be programmed to poll for
exit at selected points. For example, you might put your thread in a class such as this:

class CountdownTask:
    def __init__(self):
        self._running = True

    def terminate(self):
        self._running = False

    def run(self, n):
        while self._running and n > 0:
            print('T-minus', n)
            n -= 1
            time.sleep(5)

c = CountdownTask()
t = Thread(target=c.run, args=(10,))
t.start()
...
c.terminate() # Signal termination
t.join()      # Wait for actual termination (if needed)

Polling for thread termination can be tricky to coordinate if threads perform blocking
operations such as I/O. For example, a thread blocked indefinitely on an I/O operation
may never return to check if it’s been killed. To correctly deal with this case, you’ll need
to carefully program thread to utilize timeout loops. For example:

class IOTask:
    def terminate(self):
        self._running = False

    def run(self, sock):
        # sock is a socket
        sock.settimeout(5)        # Set timeout period
        while self._running:
            # Perform a blocking I/O operation w/ timeout
            try:
                data = sock.recv(8192)
                break
            except socket.timeout:
                continue
            # Continued processing
            ...
        # Terminated
        return

Discussion
Due to a global interpreter lock (GIL), Python threads are restricted to an execution
model that only allows one thread to execute in the interpreter at any given time. For
this reason, Python threads should generally not be used for computationally intensive
tasks where you are trying to achieve parallelism on multiple CPUs. They are much
better suited for I/O handling and handling concurrent execution in code that performs
blocking operations (e.g., waiting for I/O, waiting for results from a database, etc.).
Sometimes  you  will  see  threads  defined  via  inheritance  from  the  Thread  class.  For
example:

from threading import Thread

class CountdownThread(Thread):
    def __init__(self, n):
        super().__init__()
        self.n = 0
    def run(self):
        while self.n > 0:

12.1. Starting and Stopping Threads 

            print('T-minus', self.n)
            self.n -= 1
            time.sleep(5)

c = CountdownThread(5)
c.start()

Although this works, it introduces an extra dependency between the code and the 
threading library. That is, you can only use the resulting code in the context of threads,
whereas the technique shown earlier involves writing code with no explicit dependency
on threading. By freeing your code of such dependencies, it becomes usable in other
contexts that may or may not involve threads. For instance, you might be able to execute
your code in a separate process using the multiprocessing module using code like this:

import multiprocessing
c = CountdownTask(5)
p = multiprocessing.Process(target=c.run)
p.start()
...

Again, this only works if the CountdownTask class has been written in a manner that is
neutral to the actual means of concurrency (threads, processes, etc.).
12.2. Determining If a Thread Has Started
Problem
You’ve launched a thread, but want to know when it actually starts running.

Solution
A key feature of threads is that they execute independently and nondeterministically.
This can present a tricky synchronization problem if other threads in the program need
to know if a thread has reached a certain point in its execution before carrying out
further operations. To solve such problems, use the Event object from the threading
library.
Event instances are similar to a “sticky” flag that allows threads to wait for something
to happen. Initially, an event is set to 0. If the event is unset and a thread waits on the
event, it will block (i.e., go to sleep) until the event gets set. A thread that sets the event
will wake up all of the threads that happen to be waiting (if any). If a thread waits on an
event that has already been set, it merely moves on, continuing to execute.
Here is some sample code that uses an Event to coordinate the startup of a thread:

from threading import Thread, Event
import time

# Code to execute in an independent thread
def countdown(n, started_evt):
    print('countdown starting')
    started_evt.set()
    while n > 0:
        print('T-minus', n)
        n -= 1
        time.sleep(5)

# Create the event object that will be used to signal startup
started_evt = Event()

# Launch the thread and pass the startup event
print('Launching countdown')
t = Thread(target=countdown, args=(10,started_evt))
t.start()

# Wait for the thread to start
started_evt.wait()
print('countdown is running')

When you run this code, the “countdown is running” message will always appear after
the “countdown starting” message. This is coordinated by the event that makes the main
thread wait until the countdown() function has first printed the startup message.

Discussion
Event objects are best used for one-time events. That is, you create an event, threads
wait for the event to be set, and once set, the Event is discarded. Although it is possible
to clear an event using its clear() method, safely clearing an event and waiting for it
to be set again is tricky to coordinate, and can lead to missed events, deadlock, or other
problems (in particular, you can’t guarantee that a request to clear an event after setting
it will execute before a released thread cycles back to wait on the event again).
If a thread is going to repeatedly signal an event over and over, you’re probably better
off using a Condition object instead. For example, this code implements a periodic timer
that other threads can monitor to see whenever the timer expires:

import threading
import time

class PeriodicTimer:
    def __init__(self, interval):
        self._interval = interval
        self._flag = 0
        self._cv = threading.Condition()

    def start(self):
        t = threading.Thread(target=self.run)
        t.daemon = True

12.2. Determining If a Thread Has Started 

        t.start()

    def run(self):
        '''
        Run the timer and notify waiting threads after each interval
        '''
        while True:
            time.sleep(self._interval)
            with self._cv:
                 self._flag ^= 1
                 self._cv.notify_all()

    def wait_for_tick(self):
        '''
        Wait for the next tick of the timer
        '''
        with self._cv:
            last_flag = self._flag
            while last_flag == self._flag:
                self._cv.wait()

# Example use of the timer
ptimer = PeriodicTimer(5)
ptimer.start()

# Two threads that synchronize on the timer
def countdown(nticks):
    while nticks > 0:
        ptimer.wait_for_tick()
        print('T-minus', nticks)
        nticks -= 1

def countup(last):
    n = 0
    while n < last:
        ptimer.wait_for_tick()
        print('Counting', n)
        n += 1

threading.Thread(target=countdown, args=(10,)).start()
threading.Thread(target=countup, args=(5,)).start()

A critical feature of Event objects is that they wake all waiting threads. If you are writing
a program where you only want to wake up a single waiting thread, it is probably better
to use a Semaphore or Condition object instead.
For example, consider this code involving semaphores:

# Worker thread
def worker(n, sema):
    # Wait to be signaled
    sema.acquire()

    # Do some work
    print('Working', n)

# Create some threads
sema = threading.Semaphore(0)
nworkers = 10
for n in range(nworkers):
    t = threading.Thread(target=worker, args=(n, sema,))
    t.start()

If you run this, a pool of threads will start, but nothing happens because they’re all
blocked waiting to acquire the semaphore. Each time the semaphore is released, only
one worker will wake up and run. For example:

>>> sema.release()
Working 0
>>> sema.release()
Working 1
>>>

Writing code that involves a lot of tricky synchronization between threads is likely to
make your head explode. A more sane approach is to thread threads as communicating
tasks using queues or as actors. Queues are described in the next recipe. Actors are
described in Recipe 12.10.
12.3. Communicating Between Threads
Problem
You have multiple threads in your program and you want to safely communicate or
exchange data between them.

Solution
Perhaps the safest way to send data from one thread to another is to use a Queue from
the queue library. To do this, you create a Queue instance that is shared by the threads.
Threads then use put() or get() operations to add or remove items from the queue.
For example:

from queue import Queue
from threading import Thread

# A thread that produces data
def producer(out_q):
    while True:
        # Produce some data
        ...
        out_q.put(data)

12.3. Communicating Between Threads 

# A thread that consumes data
def consumer(in_q):
    while True:
        # Get some data
        data = in_q.get()
        # Process the data
        ...

# Create the shared queue and launch both threads
q = Queue()
t1 = Thread(target=consumer, args=(q,))
t2 = Thread(target=producer, args=(q,))
t1.start()
t2.start()

Queue instances already have all of the required locking, so they can be safely shared by
as many threads as you wish.
When using queues, it can be somewhat tricky to coordinate the shutdown of the pro‐
ducer and consumer. A common solution to this problem is to rely on a special sentinel
value, which when placed in the queue, causes consumers to terminate. For example:

from queue import Queue
from threading import Thread

# Object that signals shutdown
_sentinel = object()

# A thread that produces data
def producer(out_q):
    while running:
        # Produce some data
        ...
        out_q.put(data)

    # Put the sentinel on the queue to indicate completion
    out_q.put(_sentinel)

# A thread that consumes data
def consumer(in_q):
    while True:
        # Get some data
        data = in_q.get()

        # Check for termination
        if data is _sentinel:
            in_q.put(_sentinel)
            break

        # Process the data
        ...

A subtle feature of this example is that the consumer, upon receiving the special sentinel
value, immediately places it back onto the queue. This propagates the sentinel to other
consumers threads that might be listening on the same queue—thus shutting them all
down one after the other.
Although queues are the most common thread communication mechanism, you can
build your own data structures as long as you add the required locking and synchroni‐
zation. The most common way to do this is to wrap your data structures with a condition
variable. For example, here is how you might build a thread-safe priority queue, as
discussed in Recipe 1.5.

import heapq
import threading

class PriorityQueue:
    def __init__(self):
        self._queue = []
        self._count = 0
        self._cv = threading.Condition()
    def put(self, item, priority):
        with self._cv:
            heapq.heappush(self._queue, (-priority, self._count, item))
            self._count += 1
            self._cv.notify()

    def get(self):
        with self._cv:
            while len(self._queue) == 0:
                self._cv.wait()
            return heapq.heappop(self._queue)[-1]

Thread communication with a queue is a one-way and nondeterministic process. In
general, there is no way to know when the receiving thread has actually received a
message and worked on it. However, Queue objects do provide some basic completion
features, as illustrated by the task_done() and join() methods in this example:

from queue import Queue
from threading import Thread

# A thread that produces data
def producer(out_q):
    while running:
        # Produce some data
        ...
        out_q.put(data)

# A thread that consumes data
def consumer(in_q):
    while True:
        # Get some data
        data = in_q.get()

12.3. Communicating Between Threads 

        # Process the data
        ...
        # Indicate completion
        in_q.task_done()

# Create the shared queue and launch both threads
q = Queue()
t1 = Thread(target=consumer, args=(q,))
t2 = Thread(target=producer, args=(q,))
t1.start()
t2.start()

# Wait for all produced items to be consumed
q.join()

If a thread needs to know immediately when a consumer thread has processed a par‐
ticular item of data, you should pair the sent data with an Event object that allows the
producer to monitor its progress. For example:

from queue import Queue
from threading import Thread, Event

# A thread that produces data
def producer(out_q):
    while running:
        # Produce some data
        ...
        # Make an (data, event) pair and hand it to the consumer
        evt = Event()
        out_q.put((data, evt))
        ...
        # Wait for the consumer to process the item
        evt.wait()

# A thread that consumes data
def consumer(in_q):
    while True:
        # Get some data
        data, evt = in_q.get()
        # Process the data
        ...
        # Indicate completion
        evt.set()

Discussion
Writing threaded programs based on simple queuing is often a good way to maintain
sanity. If you can break everything down to simple thread-safe queuing, you’ll find that
you don’t need to litter your program with locks and other low-level synchronization.
Also, communicating with queues often leads to designs that can be scaled up to other
kinds of message-based communication patterns later on. For instance, you might be

able to split your program into multiple processes, or even a distributed system, without
changing much of its underlying queuing architecture.
One caution with thread queues is that putting an item in a queue doesn’t make a copy
of the item. Thus, communication actually involves passing an object reference between
threads. If you are concerned about shared state, it may make sense to only pass im‐
mutable data structures (e.g., integers, strings, or tuples) or to make deep copies of the
queued items. For example:
from queue import Queue
from threading import Thread
import copy

# A thread that produces data
def producer(out_q):
    while True:
        # Produce some data
        ...
        out_q.put(copy.deepcopy(data))

# A thread that consumes data
def consumer(in_q):
    while True:
        # Get some data
        data = in_q.get()
        # Process the data
        ...

Queue objects provide a few additional features that may prove to be useful in certain
contexts. If you create a Queue with an optional size, such as Queue(N), it places a limit
on the number of items that can be enqueued before the put() blocks the producer.
Adding an upper bound to a queue might make sense if there is mismatch in speed
between a producer and consumer. For instance, if a producer is generating items at a
much faster rate than they can be consumed. On the other hand, making a queue block
when it’s full can also have an unintended cascading effect throughout your program,
possibly causing it to deadlock or run poorly. In general, the problem of “flow control”
between communicating threads is a much harder problem than it seems. If you ever
find yourself trying to fix a problem by fiddling with queue sizes, it could be an indicator
of a fragile design or some other inherent scaling problem.
Both the get() and put() methods support nonblocking and timeouts. For example:

import queue
q = queue.Queue()

try:
    data = q.get(block=False)
except queue.Empty:
    ...

12.3. Communicating Between Threads 

try:
    q.put(item, block=False)
except queue.Full:
    ...

try:
    data = q.get(timeout=5.0)
except queue.Empty:
    ...

Both of these options can be used to avoid the problem of just blocking indefinitely on
a particular queuing operation. For example, a nonblocking put() could be used with
a fixed-sized queue to implement different kinds of handling code for when a queue is
full. For example, issuing a log message and discarding:

def producer(q):
    ...
    try:
        q.put(item, block=False)
    except queue.Full:
        log.warning('queued item %r discarded!', item)

A timeout is useful if you’re trying to make consumer threads periodically give up on
operations such as q.get() so that they can check things such as a termination flag, as
described in Recipe 12.1.

_running = True

def consumer(q):
    while _running:
        try:
            item = q.get(timeout=5.0)
            # Process item
            ...
        except queue.Empty:
            pass

Lastly, there are utility methods q.qsize(), q.full(), q.empty() that can tell you the
current size and status of the queue. However, be aware that all of these are unreliable
in a multithreaded environment. For example, a call to q.empty() might tell you that
the queue is empty, but in the time that has elapsed since making the call, another thread
could have added an item to the queue. Frankly, it’s best to write your code not to rely
on such functions.

12.4. Locking Critical Sections
Problem
Your program uses threads and you want to lock critical sections of code to avoid race
conditions.

Solution
To make mutable objects safe to use by multiple threads, use Lock objects in the thread
ing library, as shown here:

import threading

class SharedCounter:
    '''
    A counter object that can be shared by multiple threads.
    '''
    def __init__(self, initial_value = 0):
        self._value = initial_value
        self._value_lock = threading.Lock()

    def incr(self,delta=1):
        '''
        Increment the counter with locking
        '''
        with self._value_lock:
             self._value += delta

    def decr(self,delta=1):
        '''
        Decrement the counter with locking
        '''
        with self._value_lock:
             self._value -= delta

A Lock guarantees mutual exclusion when used with the with statement—that is, only
one thread is allowed to execute the block of statements under the with statement at a
time. The with statement acquires the lock for the duration of the indented statements
and releases the lock when control flow exits the indented block.

Discussion
Thread scheduling is inherently nondeterministic. Because of this, failure to use locks
in  threaded  programs  can  result  in  randomly  corrupted  data  and  bizarre  behavior
known as a “race condition.” To avoid this, locks should always be used whenever shared
mutable state is accessed by multiple threads.

12.4. Locking Critical Sections 

In older Python code, it is common to see locks explicitly acquired and released. For
example, in this variant of the last example:

import threading

class SharedCounter:
    '''
    A counter object that can be shared by multiple threads.
    '''
    def __init__(self, initial_value = 0):
        self._value = initial_value
        self._value_lock = threading.Lock()

    def incr(self,delta=1):
        '''
        Increment the counter with locking
        '''
        self._value_lock.acquire()
        self._value += delta
        self._value_lock.release()

    def decr(self,delta=1):
        '''
        Decrement the counter with locking
        '''
        self._value_lock.acquire()
        self._value -= delta
        self._value_lock.release()

The with statement is more elegant and less prone to error—especially in situations
where a programmer might forget to call the release() method or if a program happens
to raise an exception while holding a lock (the with statement guarantees that locks are
always released in both cases).
To avoid the potential for deadlock, programs that use locks should be written in a way
such that each thread is only allowed to acquire one lock at a time. If this is not possible,
you may need to introduce more advanced deadlock avoidance into your program, as
described in Recipe 12.5.
In the threading library, you’ll find other synchronization primitives, such as RLock
and Semaphore objects. As a general rule of thumb, these are more special purpose and
should not be used for simple locking of mutable state. An RLock or re-entrant lock
object is a lock that can be acquired multiple times by the same thread. It is primarily
used to implement code based locking or synchronization based on a construct known
as a “monitor.” With this kind of locking, only one thread is allowed to use an entire
function or the methods of a class while the lock is held. For example, you could im‐
plement the SharedCounter class like this:

import threading

class SharedCounter:
    '''
    A counter object that can be shared by multiple threads.
    '''
    _lock = threading.RLock()
    def __init__(self, initial_value = 0):
        self._value = initial_value

    def incr(self,delta=1):
        '''
        Increment the counter with locking
        '''
        with SharedCounter._lock:
            self._value += delta

    def decr(self,delta=1):
        '''
        Decrement the counter with locking
        '''
        with SharedCounter._lock:
             self.incr(-delta)

In this variant of the code, there is just a single class-level lock shared by all instances
of the class. Instead of the lock being tied to the per-instance mutable state, the lock is
meant to synchronize the methods of the class. Specifically, this lock ensures that only
one thread is allowed to be using the methods of the class at once. However, unlike a
standard lock, it is OK for methods that already have the lock to call other methods that
also use the lock (e.g., see the decr() method).
One feature of this implementation is that only one lock is created, regardless of how
many counter instances are created. Thus, it is much more memory-efficient in situa‐
tions where there are a large number of counters. However, a possible downside is that
it may cause more lock contention in programs that use a large number of threads and
make frequent counter updates.
A Semaphore object is a synchronization primitive based on a shared counter. If the
counter is nonzero, the with statement decrements the count and a thread is allowed to
proceed. The counter is incremented upon the conclusion of the  with block. If the
counter is zero, progress is blocked until the counter is incremented by another thread.
Although a semaphore can be used in the same manner as a standard Lock, the added
complexity in implementation negatively impacts performance. Instead of simple lock‐
ing, Semaphore objects are more useful for applications involving signaling between
threads or throttling. For example, if you want to limit the amount of concurrency in a
part of code, you might use a semaphore, as follows:

from threading import Semaphore
import urllib.request

12.4. Locking Critical Sections 

# At most, five threads allowed to run at once
_fetch_url_sema = Semaphore(5)

def fetch_url(url):
    with _fetch_url_sema:
        return urllib.request.urlopen(url)

If you’re interested in the underlying theory and implementation of thread synchroni‐
zation primitives, consult almost any textbook on operating systems.
12.5. Locking with Deadlock Avoidance
Problem
You’re writing a multithreaded program where threads need to acquire more than one
lock at a time while avoiding deadlock.

Solution
In multithreaded programs, a common source of deadlock is due to threads that attempt
to acquire multiple locks at once. For instance, if a thread acquires the first lock, but
then blocks trying to acquire the second lock, that thread can potentially block the
progress of other threads and make the program freeze.
One solution to deadlock avoidance is to assign each lock in the program a unique
number, and to enforce an ordering rule that only allows multiple locks to be acquired
in ascending order. This is surprisingly easy to implement using a context manager as
follows:

import threading
from contextlib import contextmanager

# Thread-local state to stored information on locks already acquired
_local = threading.local()

@contextmanager
def acquire(*locks):
    # Sort locks by object identifier
    locks = sorted(locks, key=lambda x: id(x))

    # Make sure lock order of previously acquired locks is not violated
    acquired = getattr(_local,'acquired',[])
    if acquired and max(id(lock) for lock in acquired) >= id(locks[0]):
        raise RuntimeError('Lock Order Violation')

    # Acquire all of the locks
    acquired.extend(locks)
    _local.acquired = acquired

    try:
        for lock in locks:
            lock.acquire()
        yield
    finally:
        # Release locks in reverse order of acquisition
        for lock in reversed(locks):
            lock.release()
        del acquired[-len(locks):]

To use this context manager, you simply allocate lock objects in the normal way, but use
the  acquire()  function  whenever  you  want  to  work  with  one  or  more  locks.  For
example:

import threading
x_lock = threading.Lock()
y_lock = threading.Lock()

def thread_1():
    while True:
        with acquire(x_lock, y_lock):
            print('Thread-1')

def thread_2():
    while True:
        with acquire(y_lock, x_lock):
            print('Thread-2')

t1 = threading.Thread(target=thread_1)
t1.daemon = True
t1.start()

t2 = threading.Thread(target=thread_2)
t2.daemon = True
t2.start()

If you run this program, you’ll find that it happily runs forever without deadlock—even
though the acquisition of locks is specified in a different order in each function.
The key to this recipe lies in the first statement that sorts the locks according to object
identifier. By sorting the locks, they always get acquired in a consistent order regardless
of how the user might have provided them to acquire().
The solution uses thread-local storage to solve a subtle problem with detecting potential
deadlock if multiple acquire() operations are nested. For example, suppose you wrote
the code like this:

import threading
x_lock = threading.Lock()
y_lock = threading.Lock()

def thread_1():

12.5. Locking with Deadlock Avoidance 

    while True:
        with acquire(x_lock):
            with acquire(y_lock):
                print('Thread-1')

def thread_2():
    while True:
        with acquire(y_lock):
            with acquire(x_lock):
                print('Thread-2')

t1 = threading.Thread(target=thread_1)
t1.daemon = True
t1.start()

t2 = threading.Thread(target=thread_2)
t2.daemon = True
t2.start()

If you run this version of the program, one of the threads will crash with an exception
such as this:

Exception in thread Thread-1:
Traceback (most recent call last):
  File "/usr/local/lib/python3.3/threading.py", line 639, in _bootstrap_inner
    self.run()
  File "/usr/local/lib/python3.3/threading.py", line 596, in run
    self._target(*self._args, **self._kwargs)
  File "deadlock.py", line 49, in thread_1
    with acquire(y_lock):
  File "/usr/local/lib/python3.3/contextlib.py", line 48, in __enter__
    return next(self.gen)
  File "deadlock.py", line 15, in acquire
    raise RuntimeError("Lock Order Violation")
RuntimeError: Lock Order Violation
>>>

This crash is caused by the fact that each thread remembers the locks it has already
acquired. The acquire() function checks the list of previously acquired locks and en‐
forces the ordering constraint that previously acquired locks must have an object ID
that is less than the new locks being acquired.

Discussion
The issue of deadlock is a well-known problem with programs involving threads (as
well as a common subject in textbooks on operating systems). As a rule of thumb, as
long as you can ensure that threads can hold only one lock at a time, your program will
be deadlock free. However, once multiple locks are being acquired at the same time, all
bets are off.

Detecting and recovering from deadlock is an extremely tricky problem with few elegant
solutions. For example, a common deadlock detection and recovery scheme involves
the use of a watchdog timer. As threads run, they periodically reset the timer, and as
long as everything is running smoothly, all is well. However, if the program deadlocks,
the watchdog timer will eventually expire. At that point, the program “recovers” by
killing and then restarting itself.
Deadlock avoidance is a different strategy where locking operations are carried out in
a manner that simply does not allow the program to enter a deadlocked state. The
solution in which locks are always acquired in strict order of ascending object ID can
be mathematically proven to avoid deadlock, although the proof is left as an exercise to
the reader (the gist of it is that by acquiring locks in a purely increasing order, you can’t
get cyclic locking dependencies, which are a necessary condition for deadlock to occur).
As a final example, a classic thread deadlock problem is the so-called “dining philoso‐
pher’s problem.” In this problem, five philosophers sit around a table on which there
are five bowls of rice and five chopsticks. Each philosopher represents an independent
thread and each chopstick represents a lock. In the problem, philosophers either sit and
think or they eat rice. However, in order to eat rice, a philosopher needs two chopsticks.
Unfortunately, if all of the philosophers reach over and grab the chopstick to their left,
they’ll all just sit there with one stick and eventually starve to death. It’s a gruesome
scene.
Using the solution, here is a simple deadlock free implementation of the dining philos‐
opher’s problem:

import threading

# The philosopher thread
def philosopher(left, right):
    while True:
        with acquire(left,right):
             print(threading.currentThread(), 'eating')

# The chopsticks (represented by locks)
NSTICKS = 5
chopsticks = [threading.Lock() for n in range(NSTICKS)]

# Create all of the philosophers
for n in range(NSTICKS):
    t = threading.Thread(target=philosopher,
                         args=(chopsticks[n],chopsticks[(n+1) % NSTICKS]))
    t.start()

Last, but not least, it should be noted that in order to avoid deadlock, all locking oper‐
ations must be carried out using our acquire() function. If some fragment of code
decided to acquire a lock directly, then the deadlock avoidance algorithm wouldn’t work.

12.5. Locking with Deadlock Avoidance 

12.6. Storing Thread-Specific State
Problem
You need to store state that’s specific to the currently executing thread and not visible
to other threads.

Solution
Sometimes in multithreaded programs, you need to store data that is only specific to
the currently executing thread. To do this, create a thread-local storage object using
threading.local(). Attributes stored and read on this object are only visible to the
executing thread and no others.
As an interesting practical example of using thread-local storage, consider the LazyCon
nection context-manager class that was first defined in Recipe 8.3. Here is a slightly
modified version that safely works with multiple threads:

from socket import socket, AF_INET, SOCK_STREAM
import threading

class LazyConnection:
    def __init__(self, address, family=AF_INET, type=SOCK_STREAM):
        self.address = address
        self.family = AF_INET
        self.type = SOCK_STREAM
        self.local = threading.local()

    def __enter__(self):
        if hasattr(self.local, 'sock'):
            raise RuntimeError('Already connected')
        self.local.sock = socket(self.family, self.type)
        self.local.sock.connect(self.address)
        return self.local.sock

    def __exit__(self, exc_ty, exc_val, tb):
        self.local.sock.close()
        del self.local.sock

In this code, carefully observe the use of the self.local attribute. It is initialized as an
instance of  threading.local(). The other methods then manipulate a socket that’s
stored as self.local.sock. This is enough to make it possible to safely use an instance
of LazyConnection in multiple threads. For example:

from functools import partial
def test(conn):
    with conn as s:
        s.send(b'GET /index.html HTTP/1.0\r\n')
        s.send(b'Host: www.python.org\r\n')

        s.send(b'\r\n')
        resp = b''.join(iter(partial(s.recv, 8192), b''))

    print('Got {} bytes'.format(len(resp)))

if __name__ == '__main__':
    conn = LazyConnection(('www.python.org', 80))

    t1 = threading.Thread(target=test, args=(conn,))
    t2 = threading.Thread(target=test, args=(conn,))
    t1.start()
    t2.start()
    t1.join()
    t2.join()

The reason it works is that each thread actually creates its own dedicated socket con‐
nection (stored as self.local.sock). Thus, when the different threads perform socket
operations, they don’t interfere with one another as they are being performed on dif‐
ferent sockets.

Discussion
Creating and manipulating thread-specific state is not a problem that often arises in
most programs. However, when it does, it commonly involves situations where an object
being used by multiple threads needs to manipulate some kind of dedicated system
resource, such as a socket or file. You can’t just have a single socket object shared by
everyone because chaos would ensue if multiple threads ever started reading and writing
on it at the same time. Thread-local storage fixes this by making such resources only
visible in the thread where they’re being used.
In this recipe, the use of threading.local() makes the LazyConnection class support
one connection per thread, as opposed to one connection for the entire process. It’s a
subtle but interesting distinction.
Under the covers, an instance of  threading.local() maintains a separate instance
dictionary for each thread. All of the usual instance operations of getting, setting, and
deleting values just manipulate the per-thread dictionary. The fact that each thread uses
a separate dictionary is what provides the isolation of data.
12.7. Creating a Thread Pool
Problem
You want to create a pool of worker threads for serving clients or performing other kinds
of work.

12.7. Creating a Thread Pool 

Solution
The concurrent.futures library has a ThreadPoolExecutor class that can be used for
this purpose. Here is an example of a simple TCP server that uses a thread-pool to serve
clients:

from socket import AF_INET, SOCK_STREAM, socket
from concurrent.futures import ThreadPoolExecutor

def echo_client(sock, client_addr):
    '''
    Handle a client connection
    '''
    print('Got connection from', client_addr)
    while True:
        msg = sock.recv(65536)
        if not msg:
            break
        sock.sendall(msg)
    print('Client closed connection')
    sock.close()

def echo_server(addr):
    pool = ThreadPoolExecutor(128)
    sock = socket(AF_INET, SOCK_STREAM)
    sock.bind(addr)
    sock.listen(5)
    while True:
        client_sock, client_addr = sock.accept()
        pool.submit(echo_client, client_sock, client_addr)

echo_server(('',15000))

If you want to manually create your own thread pool, it’s usually easy enough to do it
using a Queue. Here is a slightly different, but manual implementation of the same code:

from socket import socket, AF_INET, SOCK_STREAM
from threading import Thread
from queue import Queue

def echo_client(q):
    '''
    Handle a client connection
    '''
    sock, client_addr = q.get()
    print('Got connection from', client_addr)
    while True:
        msg = sock.recv(65536)
        if not msg:
            break
        sock.sendall(msg)
    print('Client closed connection')

    sock.close()

def echo_server(addr, nworkers):
    # Launch the client workers
    q = Queue()
    for n in range(nworkers):
        t = Thread(target=echo_client, args=(q,))
        t.daemon = True
        t.start()

    # Run the server
    sock = socket(AF_INET, SOCK_STREAM)
    sock.bind(addr)
    sock.listen(5)
    while True:
        client_sock, client_addr = sock.accept()
        q.put((client_sock, client_addr))

echo_server(('',15000), 128)

One advantage of using ThreadPoolExecutor over a manual implementation is that it
makes it easier for the submitter to receive results from the called function. For example,
you could write code like this:

from concurrent.futures import ThreadPoolExecutor
import urllib.request

def fetch_url(url):
    u = urllib.request.urlopen(url)
    data = u.read()
    return data

pool = ThreadPoolExecutor(10)
# Submit work to the pool
a = pool.submit(fetch_url, 'http://www.python.org')
b = pool.submit(fetch_url, 'http://www.pypy.org')

# Get the results back
x = a.result()
y = b.result()

The result objects in the example handle all of the blocking and coordination needed
to get data back from the worker thread. Specifically, the operation a.result() blocks
until the corresponding function has been executed by the pool and returned a value.

Discussion
Generally, you should avoid writing programs that allow unlimited growth in the num‐
ber of threads. For example, take a look at the following server:

from threading import Thread
from socket import socket, AF_INET, SOCK_STREAM

12.7. Creating a Thread Pool 

def echo_client(sock, client_addr):
    '''
    Handle a client connection
    '''
    print('Got connection from', client_addr)
    while True:
        msg = sock.recv(65536)
        if not msg:
            break
        sock.sendall(msg)
    print('Client closed connection')
    sock.close()

def echo_server(addr, nworkers):
    # Run the server
    sock = socket(AF_INET, SOCK_STREAM)
    sock.bind(addr)
    sock.listen(5)
    while True:
        client_sock, client_addr = sock.accept()
        t = Thread(target=echo_client, args=(client_sock, client_addr))
        t.daemon = True
        t.start()

echo_server(('',15000))

Although this works, it doesn’t prevent some asynchronous hipster from launching an
attack on the server that makes it create so many threads that your program runs out
of resources and crashes (thus further demonstrating the “evils” of using threads). By
using a pre-initialized thread pool, you can carefully put an upper limit on the amount
of supported concurrency.
You might be concerned with the effect of creating a large number of threads. However,
modern  systems  should  have  no  trouble  creating  pools  of  a  few  thousand  threads.
Moreover, having a thousand threads just sitting around waiting for work isn’t going to
have much, if any, impact on the performance of other code (a sleeping thread does just
that—nothing at all). Of course, if all of those threads wake up at the same time and
start hammering on the CPU, that’s a different story—especially in light of the Global
Interpreter Lock (GIL). Generally, you only want to use thread pools for I/O-bound
processing.
One possible concern with creating large thread pools might be memory use. For ex‐
ample, if you create 2,000 threads on OS X, the system shows the Python process using
up more than 9 GB of virtual memory. However, this is actually somewhat misleading.
When creating a thread, the operating system reserves a region of virtual memory to
hold the thread’s execution stack (often as large as 8 MB). Only a small fragment of this
memory is actually mapped to real memory, though. Thus, if you look a bit closer, you
might find the Python process is using far less real memory (e.g., for 2,000 threads, only

70 MB of real memory is used, not 9 GB). If the size of the virtual memory is a concern,
you can dial it down using the threading.stack_size() function. For example:

import threading
threading.stack_size(65536)

If you add this call and repeat the experiment of creating 2,000 threads, you’ll find that
the Python process is now only using about 210 MB of virtual memory, although the
amount of real memory in use remains about the same. Note that the thread stack size
must be at least 32,768 bytes, and is usually restricted to be a multiple of the system
memory page size (4096, 8192, etc.).
12.8. Performing Simple Parallel Programming
Problem
You have a program that performs a lot of CPU-intensive work, and you want to make
it run faster by having it take advantage of multiple CPUs.

Solution
The concurrent.futures library provides a ProcessPoolExecutor class that can be
used to execute computationally intensive functions in a separately running instance of
the Python interpreter. However, in order to use it, you first need to have some com‐
putationally intensive work. Let’s illustrate with a simple yet practical example.
Suppose you have an entire directory of gzip-compressed Apache web server logs:

logs/
   20120701.log.gz
   20120702.log.gz
   20120703.log.gz
   20120704.log.gz
   20120705.log.gz
   20120706.log.gz
   ...

Further suppose each log file contains lines like this:

124.115.6.12 - - [10/Jul/2012:00:18:50 -0500] "GET /robots.txt ..." 200 71
210.212.209.67 - - [10/Jul/2012:00:18:51 -0500] "GET /ply/ ..." 200 11875
210.212.209.67 - - [10/Jul/2012:00:18:51 -0500] "GET /favicon.ico ..." 404 369
61.135.216.105 - - [10/Jul/2012:00:20:04 -0500] "GET /blog/atom.xml ..." 304 -
...

Here is a simple script that takes this data and identifies all hosts that have accessed the
robots.txt file:

12.8. Performing Simple Parallel Programming 

# findrobots.py

import gzip
import io
import glob

def find_robots(filename):
    '''
    Find all of the hosts that access robots.txt in a single log file
    '''
    robots = set()
    with gzip.open(filename) as f:
        for line in io.TextIOWrapper(f,encoding='ascii'):
            fields = line.split()
            if fields[6] == '/robots.txt':
                robots.add(fields[0])
    return robots

def find_all_robots(logdir):
    '''
    Find all hosts across and entire sequence of files
    '''
    files = glob.glob(logdir+'/*.log.gz')
    all_robots = set()
    for robots in map(find_robots, files):
        all_robots.update(robots)
    return all_robots

if __name__ == '__main__':
    robots = find_all_robots('logs')
    for ipaddr in robots:
        print(ipaddr)

The preceding program is written in the commonly used map-reduce style. The function
find_robots() is mapped across a collection of filenames and the results are combined
into a single result (the all_robots set in the find_all_robots() function).
Now, suppose you want to modify this program to use multiple CPUs. It turns out to
be easy—simply replace the map() operation with a similar operation carried out on a
process pool from the concurrent.futures library. Here is a slightly modified version
of the code:

# findrobots.py

import gzip
import io
import glob
from concurrent import futures

def find_robots(filename):
    '''
    Find all of the hosts that access robots.txt in a single log file

    '''
    robots = set()
    with gzip.open(filename) as f:
        for line in io.TextIOWrapper(f,encoding='ascii'):
            fields = line.split()
            if fields[6] == '/robots.txt':
                robots.add(fields[0])
    return robots

def find_all_robots(logdir):
    '''
    Find all hosts across and entire sequence of files
    '''
    files = glob.glob(logdir+'/*.log.gz')
    all_robots = set()
    with futures.ProcessPoolExecutor() as pool:
        for robots in pool.map(find_robots, files):
            all_robots.update(robots)
    return all_robots

if __name__ == '__main__':
    robots = find_all_robots('logs')
    for ipaddr in robots:
        print(ipaddr)

With this modification, the script produces the same result but runs about 3.5 times
faster on our quad-core machine. The actual performance will vary according to the
number of CPUs available on your machine.

Discussion
Typical usage of a ProcessPoolExecutor is as follows:
from concurrent.futures import ProcessPoolExecutor

with ProcessPoolExecutor() as pool:
    ...
    do work in parallel using pool
    ...

Under the covers, a ProcessPoolExecutor creates N independent running Python in‐
terpreters where N is the number of available CPUs detected on the system. You can
change the number of processes created by supplying an optional argument to Proces
sPoolExecutor(N). The pool runs until the last statement in the with block is executed,
at which point the process pool is shut down. However, the program will wait until all
submitted work has been processed.
Work to be submitted to a pool must be defined in a function. There are two methods
for submission. If you are are trying to parallelize a list comprehension or a  map()
operation, you use pool.map():

12.8. Performing Simple Parallel Programming 

# A function that performs a lot of work
def work(x):
    ...
    return result

# Nonparallel code
results = map(work, data)

# Parallel implementation
with ProcessPoolExecutor() as pool:
    results = pool.map(work, data)

Alternatively, you can manually submit single tasks using the pool.submit() method:

# Some function
def work(x):
    ...
    return result

with ProcessPoolExecutor() as pool:
    ...
    # Example of submitting work to the pool
    future_result = pool.submit(work, arg)

    # Obtaining the result (blocks until done)
    r = future_result.result()
    ...

If you manually submit a job, the result is an instance of Future. To obtain the actual
result, you call its result() method. This blocks until the result is computed and re‐
turned by the pool.
Instead of blocking, you can also arrange to have a callback function triggered upon
completion instead. For example:

def when_done(r):
    print('Got:', r.result())

with ProcessPoolExecutor() as pool:
     future_result = pool.submit(work, arg)
     future_result.add_done_callback(when_done)

The user-supplied callback function receives an instance of Future that must be used
to obtain the actual result (i.e., by calling its result() method).
Although process pools can be easy to use, there are a number of important consider‐
ations to be made in designing larger programs. In no particular order:

• This technique for parallelization only works well for problems that can be trivially

decomposed into independent parts.

• Work must be submitted in the form of simple functions. Parallel execution of

instance methods, closures, or other kinds of constructs are not supported.

• Function arguments and return values must be compatible with pickle. Work is
carried out in a separate interpreter using interprocess communication. Thus, data
exchanged between interpreters has to be serialized.

• Functions submitted for work should not maintain persistent state or have side
effects. With the exception of simple things such as logging, you don’t really have
any control over the behavior of child processes once started. Thus, to preserve your
sanity, it is probably best to keep things simple and carry out work in pure-functions
that don’t alter their environment.

• Process pools are created by calling the fork() system call on Unix. This makes a
clone of the Python interpreter, including all of the program state at the time of the
fork. On Windows, an independent copy of the interpreter that does not clone state
is launched. The actual forking process does not occur until the first pool.map()
or pool.submit() method is called.

• Great care should be made when combining process pools and programs that use
threads. In particular, you should probably create and launch process pools prior
to the creation of any threads (e.g., create the pool in the main thread at program
startup).

12.9. Dealing with the GIL (and How to Stop Worrying
About It)
Problem
You’ve heard about the Global Interpreter Lock (GIL), and are worried that it might be
affecting the performance of your multithreaded program.

Solution
Although Python fully supports thread programming, parts of the C implementation
of the interpreter are not entirely thread safe to a level of allowing fully concurrent
execution. In fact, the interpreter is protected by a so-called Global Interpreter Lock
(GIL) that only allows one Python thread to execute at any given time. The most no‐
ticeable effect of the GIL is that multithreaded Python programs are not able to fully
take advantage of multiple CPU cores (e.g., a computationally intensive application
using more than one thread only runs on a single CPU).

12.9. Dealing with the GIL (and How to Stop Worrying About It) 

Before discussing common GIL workarounds, it is important to emphasize that the GIL
tends to only affect programs that are heavily CPU bound (i.e., dominated by compu‐
tation). If your program is mostly doing I/O, such as network communication, threads
are often a sensible choice because they’re mostly going to spend their time sitting
around waiting. In fact, you can create thousands of Python threads with barely a con‐
cern. Modern operating systems have no trouble running with that many threads, so
it’s simply not something you should worry much about.
For CPU-bound programs, you really need to study the nature of the computation being
performed. For instance, careful choice of the underlying algorithm may produce a far
greater speedup than trying to parallelize an unoptimal algorithm with threads. Simi‐
larly, given that Python is interpreted, you might get a far greater speedup simply by
moving  performance-critical  code  into  a  C  extension  module.  Extensions  such  as 
NumPy are also highly effective at speeding up certain kinds of calculations involving
array data. Last, but not least, you might investigate alternative implementations, such
as PyPy, which features optimizations such as a JIT compiler (although, as of this writing,
it does not yet support Python 3).
It’s also worth noting that threads are not necessarily used exclusively for performance.
A CPU-bound program might be using threads to manage a graphical user interface, a
network connection, or provide some other kind of service. In this case, the GIL can
actually present more of a problem, since code that holds it for an excessively long period
will cause annoying stalls in the non-CPU-bound threads. In fact, a poorly written C
extension can actually make this problem worse, even though the computation part of
the code might run faster than before.
Having said all of this, there are two common strategies for working around the limi‐
tations of the GIL. First, if you are working entirely in Python, you can use the multi
processing module to create a process pool and use it like a co-processor. For example,
suppose you have the following thread code:

# Performs a large calculation (CPU bound)
def some_work(args):
    ...
    return result

# A thread that calls the above function
def some_thread():
    while True:
        ...
        r = some_work(args)
        ...

Here’s how you would modify the code to use a pool:

# Processing pool (see below for initiazation)
pool = None

# Performs a large calculation (CPU bound)
def some_work(args):
    ...
    return result

# A thread that calls the above function
def some_thread():
    while True:
        ...
        r = pool.apply(some_work, (args))
        ...

# Initiaze the pool
if __name__ == '__main__':
    import multiprocessing
    pool = multiprocessing.Pool()

This example with a pool works around the GIL using a neat trick. Whenever a thread
wants to perform CPU-intensive work, it hands the work to the pool. The pool, in turn,
hands the work to a separate Python interpreter running in a different process. While
the thread is waiting for the result, it releases the GIL. Moreover, because the calculation
is being performed in a separate interpreter, it’s no longer bound by the restrictions of
the GIL. On a multicore system, you’ll find that this technique easily allows you to take
advantage of all the CPUs.
The second strategy for working around the GIL is to focus on C extension program‐
ming. The general idea is to move computationally intensive tasks to C, independent of
Python, and have the C code release the GIL while it’s working. This is done by inserting
special macros into the C code like this:

#include "Python.h"
...

PyObject *pyfunc(PyObject *self, PyObject *args) {
   ...
   Py_BEGIN_ALLOW_THREADS
   // Threaded C code
   ...
   Py_END_ALLOW_THREADS
   ...
}

If you are using other tools to access C, such as the ctypes library or Cython, you may
not need to do anything. For example, ctypes releases the GIL when calling into C by
default.

Discussion
Many programmers, when faced with thread performance problems, are quick to blame
the GIL for all of their ills. However, doing so is shortsighted and naive. Just as a real-

12.9. Dealing with the GIL (and How to Stop Worrying About It) 

world example, mysterious “stalls” in a multithreaded network program might be caused
by something entirely different (e.g., a stalled DNS lookup) rather than anything related
to the GIL. The bottom line is that you really need to study your code to know if the
GIL is an issue or not. Again, realize that the GIL is mostly concerned with CPU-bound
processing, not I/O.
If you are going to use a process pool as a workaround, be aware that doing so involves
data serialization and communication with a different Python interpreter. For this to
work, the operation to be performed needs to be contained within a Python function
defined by the def statement (i.e., no lambdas, closures, callable instances, etc.), and the
function arguments and return value must be compatible with pickle. Also, the amount
of work to be performed must be sufficiently large to make up for the extra communi‐
cation overhead.
Another subtle aspect of pools is that mixing threads and process pools together can be
a good way to make your head explode. If you are going to use both of these features
together, it is often best to create the process pool as a singleton at program startup,
prior to the creation of any threads. Threads will then use the same process pool for all
of their computationally intensive work.
For C extensions, the most important feature is maintaining isolation from the Python
interpreter process. That is, if you’re going to offload work from Python to C, you need
to make sure the C code operates independently of Python. This means using no Python
data structures and making no calls to Python’s C API. Another consideration is that
you want to make sure your C extension does enough work to make it all worthwhile.
That is, it’s much better if the extension can perform millions of calculations as opposed
to just a few small calculations.
Needless to say, these solutions to working around the GIL don’t apply to all possible
problems. For instance, certain kinds of applications don’t work well if separated into
multiple processes, nor may you want to code parts in C. For these kinds of applications,
you may have to come up with your own solution (e.g., multiple processes accessing
shared memory regions, multiple interpreters running in the same process, etc.). Al‐
ternatively, you might look at some other implementations of the interpreter, such as
PyPy.
See  Recipes  15.7  and  15.10  for  additional  information  on  releasing  the  GIL  in  C
extensions.
12.10. Defining an Actor Task
Problem
You’d like to define tasks with behavior similar to “actors” in the so-called “actor model.”

Solution
The “actor model” is one of the oldest and most simple approaches to concurrency and
distributed computing. In fact, its underlying simplicity is part of its appeal. In a nutshell,
an actor is a concurrently executing task that simply acts upon messages sent to it. In
response to these messages, it may decide to send further messages to other actors.
Communication with actors is one way and asynchronous. Thus, the sender of a message
does not know when a message actually gets delivered, nor does it receive a response
or acknowledgment that the message has been processed.
Actors are straightforward to define using a combination of a thread and a queue. For
example:

from queue import Queue
from threading import Thread, Event

# Sentinel used for shutdown
class ActorExit(Exception):
    pass

class Actor:
    def __init__(self):
        self._mailbox = Queue()

    def send(self, msg):
        '''
        Send a message to the actor
        '''
        self._mailbox.put(msg)

    def recv(self):
        '''
        Receive an incoming message
        '''
        msg = self._mailbox.get()
        if msg is ActorExit:
            raise ActorExit()
        return msg

    def close(self):
        '''
        Close the actor, thus shutting it down
        '''
        self.send(ActorExit)

    def start(self):
        '''
        Start concurrent execution
        '''
        self._terminated = Event()
        t = Thread(target=self._bootstrap)

12.10. Defining an Actor Task 

        t.daemon = True
        t.start()

    def _bootstrap(self):
        try:
            self.run()
        except ActorExit:
            pass
        finally:
            self._terminated.set()

    def join(self):
        self._terminated.wait()

    def run(self):
        '''
        Run method to be implemented by the user
        '''
        while True:
            msg = self.recv()

# Sample ActorTask
class PrintActor(Actor):
    def run(self):
        while True:
            msg = self.recv()
            print('Got:', msg)

# Sample use
p = PrintActor()
p.start()
p.send('Hello')
p.send('World')
p.close()
p.join()

In this example, Actor instances are things that you simply send a message to using
their send() method. Under the covers, this places the message on a queue and hands
it off to an internal thread that runs to process the received messages. The close()
method  is  programmed  to  shut  down  the  actor  by  placing  a  special  sentinel  value
(ActorExit) on the queue. Users define new actors by inheriting from Actor and re‐
defining the run() method to implement their custom processing. The usage of the
ActorExit exception is such that user-defined code can be programmed to catch the
termination request and handle it if appropriate (the exception is raised by the get()
method and propagated).
If you relax the requirement of concurrent and asynchronous message delivery, actor-
like objects can also be minimally defined by generators. For example:

def print_actor():
    while True:

        try:
            msg = yield      # Get a message
            print('Got:', msg)
        except GeneratorExit:
            print('Actor terminating')

# Sample use
p = print_actor()
next(p)     # Advance to the yield (ready to receive)
p.send('Hello')
p.send('World')
p.close()

Discussion
Part of the appeal of actors is their underlying simplicity. In practice, there is just one
core operation, send(). Plus, the general concept of a “message” in actor-based systems
is something that can be expanded in many different directions. For example, you could
pass tagged messages in the form of tuples and have actors take different courses of
action like this:

class TaggedActor(Actor):
    def run(self):
        while True:
             tag, *payload = self.recv()
             getattr(self,'do_'+tag)(*payload)

    # Methods correponding to different message tags
    def do_A(self, x):
        print('Running A', x)

    def do_B(self, x, y):
        print('Running B', x, y)

# Example
a = TaggedActor()
a.start()
a.send(('A', 1))      # Invokes do_A(1)
a.send(('B', 2, 3))   # Invokes do_B(2,3)

As another example, here is a variation of an actor that allows arbitrary functions to be
executed in a worker and results to be communicated back using a special Result object:

from threading import Event
class Result:
    def __init__(self):
        self._evt = Event()
        self._result = None

    def set_result(self, value):
        self._result = value

12.10. Defining an Actor Task 

        self._evt.set()

    def result(self):
        self._evt.wait()
        return self._result

class Worker(Actor):
    def submit(self, func, *args, **kwargs):
        r = Result()
        self.send((func, args, kwargs, r))
        return r

    def run(self):
        while True:
            func, args, kwargs, r = self.recv()
            r.set_result(func(*args, **kwargs))

# Example use
worker = Worker()
worker.start()
r = worker.submit(pow, 2, 3)
print(r.result())

Last, but not least, the concept of “sending” a task a message is something that can be
scaled up into systems involving multiple processes or even large distributed systems.
For example, the send() method of an actor-like object could be programmed to trans‐
mit data on a socket connection or deliver it via some kind of messaging infrastructure
(e.g., AMQP, ZMQ, etc.).
12.11. Implementing Publish/Subscribe Messaging
Problem
You have a program based on communicating threads and want them to implement
publish/subscribe messaging.

Solution
To  implement  publish/subscribe  messaging,  you  typically  introduce  a  separate  “ex‐
change” or “gateway” object that acts as an intermediary for all messages. That is, instead
of directly sending a message from one task to another, a message is sent to the exchange
and it delivers it to one or more attached tasks. Here is one example of a very simple
exchange implementation:

from collections import defaultdict

class Exchange:
    def __init__(self):
        self._subscribers = set()

    def attach(self, task):
        self._subscribers.add(task)

    def detach(self, task):
        self._subscribers.remove(task)

    def send(self, msg):
        for subscriber in self._subscribers:
            subscriber.send(msg)

# Dictionary of all created exchanges
_exchanges = defaultdict(Exchange)

# Return the Exchange instance associated with a given name
def get_exchange(name):
    return _exchanges[name]

An exchange is really nothing more than an object that keeps a set of active subscribers
and provides methods for attaching, detaching, and sending messages. Each exchange
is  identified  by  a  name,  and  the  get_exchange()  function  simply  returns  the  Ex
change instance associated with a given name.
Here is a simple example that shows how to use an exchange:

# Example of a task.  Any object with a send() method

class Task:
    ...
    def send(self, msg):
        ...

task_a = Task()
task_b = Task()

# Example of getting an exchange
exc = get_exchange('name')

# Examples of subscribing tasks to it
exc.attach(task_a)
exc.attach(task_b)

# Example of sending messages
exc.send('msg1')
exc.send('msg2')

# Example of unsubscribing
exc.detach(task_a)
exc.detach(task_b)

12.11. Implementing Publish/Subscribe Messaging 

Although there are many different variations on this theme, the overall idea is the same.
Messages will be delivered to an exchange and the exchange will deliver them to attached
subscribers.

Discussion
The concept of tasks or threads sending messages to one another (often via queues) is
easy to implement and quite popular. However, the benefits of using a public/subscribe
(pub/sub) model instead are often overlooked.
First, the use of an exchange can simplify much of the plumbing involved in setting up
communicating threads. Instead of trying to wire threads together across multiple pro‐
gram modules, you only worry about connecting them to a known exchange. In some
sense, this is similar to how the logging library works. In practice, it can make it easier
to decouple various tasks in the program.
Second, the ability of the exchange to broadcast messages to multiple subscribers opens
up new communication patterns. For example, you could implement systems with re‐
dundant tasks, broadcasting, or fan-out. You could also build debugging and diagnostic
tools that attach themselves to exchanges as ordinary subscribers. For example, here is
a simple diagnostic class that would display sent messages:

class DisplayMessages:
    def __init__(self):
        self.count = 0
    def send(self, msg):
        self.count += 1
        print('msg[{}]: {!r}'.format(self.count, msg))

exc = get_exchange('name')
d = DisplayMessages()
exc.attach(d)

Last, but not least, a notable aspect of the implementation is that it works with a variety
of task-like objects. For example, the receivers of a message could be actors (as described
in Recipe 12.10), coroutines, network connections, or just about anything that imple‐
ments a proper send() method.
One potentially problematic aspect of an exchange concerns the proper attachment and
detachment of subscribers. In order to properly manage resources, every subscriber that
attaches must eventually detach. This leads to a programming model similar to this:

exc = get_exchange('name')
exc.attach(some_task)
try:
    ...
finally:
    exc.detach(some_task)

In some sense, this is similar to the usage of files, locks, and similar objects. Experience
has shown that it is quite easy to forget the final detach() step. To simplify this, you
might consider the use of the context-management protocol. For example, adding a
subscribe() method to the exchange like this:

from contextlib import contextmanager
from collections import defaultdict

class Exchange:
    def __init__(self):
        self._subscribers = set()

    def attach(self, task):
        self._subscribers.add(task)

    def detach(self, task):
        self._subscribers.remove(task)

    @contextmanager
    def subscribe(self, *tasks):
        for task in tasks:
            self.attach(task)
        try:
            yield
        finally:
            for task in tasks:
                self.detach(task)

    def send(self, msg):
        for subscriber in self._subscribers:
            subscriber.send(msg)

# Dictionary of all created exchanges
_exchanges = defaultdict(Exchange)

# Return the Exchange instance associated with a given name
def get_exchange(name):
    return _exchanges[name]

# Example of using the subscribe() method
exc = get_exchange('name')
with exc.subscribe(task_a, task_b):
     ...
     exc.send('msg1')
     exc.send('msg2')
     ...

# task_a and task_b detached here

Finally, it should be noted that there are numerous possible extensions to the exchange
idea. For example, exchanges could implement an entire collection of message channels

12.11. Implementing Publish/Subscribe Messaging 

or apply pattern matching rules to exchange names. Exchanges can also be extended
into distributed computing applications (e.g., routing messages to tasks on different
machines, etc.).
12.12. Using Generators As an Alternative to Threads
Problem
You want to implement concurrency using generators (coroutines) as an alternative to
system threads. This is sometimes known as user-level threading or green threading.

Solution
To implement your own concurrency using generators, you first need a fundamental
insight concerning generator functions and the yield statement. Specifically, the fun‐
damental behavior of yield is that it causes a generator to suspend its execution. By
suspending execution, it is possible to write a scheduler that treats generators as a kind
of “task” and alternates their execution using a kind of cooperative task switching.
To illustrate this idea, consider the following two generator functions using a simple
yield:

# Two simple generator functions
def countdown(n):
    while n > 0:
        print('T-minus', n)
        yield
        n -= 1
    print('Blastoff!')

def countup(n):
    x = 0
    while x < n:
        print('Counting up', x)
        yield
        x += 1

These functions probably look a bit funny using yield all by itself. However, consider
the following code that implements a simple task scheduler:

from collections import deque

class TaskScheduler:
    def __init__(self):
        self._task_queue = deque()

    def new_task(self, task):
        '''
        Admit a newly started task to the scheduler

        '''
        self._task_queue.append(task)

    def run(self):
        '''
        Run until there are no more tasks
        '''
        while self._task_queue:
            task = self._task_queue.popleft()
            try:
                # Run until the next yield statement
                next(task)
                self._task_queue.append(task)
            except StopIteration:
                # Generator is no longer executing
                pass

# Example use
sched = TaskScheduler()
sched.new_task(countdown(10))
sched.new_task(countdown(5))
sched.new_task(countup(15))
sched.run()

In this code, the TaskScheduler class runs a collection of generators in a round-robin
manner—each one running until they reach a  yield statement. For the sample, the
output will be as follows:

T-minus 10
T-minus 5
Counting up 0
T-minus 9
T-minus 4
Counting up 1
T-minus 8
T-minus 3
Counting up 2
T-minus 7
T-minus 2
...

At this point, you’ve essentially implemented the tiny core of an “operating system” if
you will. Generator functions are the tasks and the yield statement is how tasks signal
that they want to suspend. The scheduler simply cycles over the tasks until none are left
executing.
In practice, you probably wouldn’t use generators to implement concurrency for some‐
thing as simple as shown. Instead, you might use generators to replace the use of threads
when implementing actors (see Recipe 12.10) or network servers.

12.12. Using Generators As an Alternative to Threads 

The following code illustrates the use of generators to implement a thread-free version
of actors:

from collections import deque

class ActorScheduler:
    def __init__(self):
        self._actors = { }          # Mapping of names to actors
        self._msg_queue = deque()   # Message queue

    def new_actor(self, name, actor):
        '''
        Admit a newly started actor to the scheduler and give it a name
        '''
        self._msg_queue.append((actor,None))
        self._actors[name] = actor

    def send(self, name, msg):
        '''
        Send a message to a named actor
        '''
        actor = self._actors.get(name)
        if actor:
            self._msg_queue.append((actor,msg))

    def run(self):
        '''
        Run as long as there are pending messages.
        '''
        while self._msg_queue:
            actor, msg = self._msg_queue.popleft()
            try:
                 actor.send(msg)
            except StopIteration:
                 pass

# Example use
if __name__ == '__main__':
    def printer():
        while True:
            msg = yield
            print('Got:', msg)

    def counter(sched):
        while True:
            # Receive the current count
            n = yield
            if n == 0:
                break
            # Send to the printer task
            sched.send('printer', n)
            # Send the next count to the counter task (recursive)

            sched.send('counter', n-1)

    sched = ActorScheduler()
    # Create the initial actors
    sched.new_actor('printer', printer())
    sched.new_actor('counter', counter(sched))

    # Send an initial message to the counter to initiate
    sched.send('counter', 10000)
    sched.run()

The execution of this code might take a bit of study, but the key is the queue of pending
messages. Essentially, the scheduler runs as long as there are messages to deliver. A
remarkable feature is that the counter generator sends messages to itself and ends up
in a recursive cycle not bound by Python’s recursion limit.
Here is an advanced example showing the use of generators to implement a concurrent
network application:

from collections import deque
from select import select

# This class represents a generic yield event in the scheduler
class YieldEvent:
    def handle_yield(self, sched, task):
        pass
    def handle_resume(self, sched, task):
        pass

# Task Scheduler
class Scheduler:
    def __init__(self):
        self._numtasks = 0       # Total num of tasks
        self._ready = deque()    # Tasks ready to run
        self._read_waiting = {}  # Tasks waiting to read
        self._write_waiting = {} # Tasks waiting to write

    # Poll for I/O events and restart waiting tasks
    def _iopoll(self):
        rset,wset,eset = select(self._read_waiting,
                                self._write_waiting,[])
        for r in rset:
            evt, task = self._read_waiting.pop(r)
            evt.handle_resume(self, task)
        for w in wset:
            evt, task = self._write_waiting.pop(w)
            evt.handle_resume(self, task)

    def new(self,task):
        '''
        Add a newly started task to the scheduler
        '''

12.12. Using Generators As an Alternative to Threads 

        self._ready.append((task, None))
        self._numtasks += 1

    def add_ready(self, task, msg=None):
        '''
        Append an already started task to the ready queue.
        msg is what to send into the task when it resumes.
        '''
        self._ready.append((task, msg))

    # Add a task to the reading set
    def _read_wait(self, fileno, evt, task):
        self._read_waiting[fileno] = (evt, task)

    # Add a task to the write set
    def _write_wait(self, fileno, evt, task):
        self._write_waiting[fileno] = (evt, task)

    def run(self):
        '''
        Run the task scheduler until there are no tasks
        '''
        while self._numtasks:
             if not self._ready:
                  self._iopoll()
             task, msg = self._ready.popleft()
             try:
                 # Run the coroutine to the next yield
                 r = task.send(msg)
                 if isinstance(r, YieldEvent):
                     r.handle_yield(self, task)
                 else:
                     raise RuntimeError('unrecognized yield event')
             except StopIteration:
                 self._numtasks -= 1

# Example implementation of coroutine-based socket I/O
class ReadSocket(YieldEvent):
    def __init__(self, sock, nbytes):
        self.sock = sock
        self.nbytes = nbytes
    def handle_yield(self, sched, task):
        sched._read_wait(self.sock.fileno(), self, task)
    def handle_resume(self, sched, task):
        data = self.sock.recv(self.nbytes)
        sched.add_ready(task, data)

class WriteSocket(YieldEvent):
    def __init__(self, sock, data):
        self.sock = sock
        self.data = data
    def handle_yield(self, sched, task):

        sched._write_wait(self.sock.fileno(), self, task)
    def handle_resume(self, sched, task):
        nsent = self.sock.send(self.data)
        sched.add_ready(task, nsent)

class AcceptSocket(YieldEvent):
    def __init__(self, sock):
        self.sock = sock
    def handle_yield(self, sched, task):
        sched._read_wait(self.sock.fileno(), self, task)
    def handle_resume(self, sched, task):
        r = self.sock.accept()
        sched.add_ready(task, r)

# Wrapper around a socket object for use with yield
class Socket(object):
    def __init__(self, sock):
        self._sock = sock
    def recv(self, maxbytes):
        return ReadSocket(self._sock, maxbytes)
    def send(self, data):
        return WriteSocket(self._sock, data)
    def accept(self):
        return AcceptSocket(self._sock)
    def __getattr__(self, name):
        return getattr(self._sock, name)

if __name__ == '__main__':
    from socket import socket, AF_INET, SOCK_STREAM
    import time

    # Example of a function involving generators.  This should
    # be called using line = yield from readline(sock)
    def readline(sock):
        chars = []
        while True:
            c = yield sock.recv(1)
            if not c:
                break
            chars.append(c)
            if c == b'\n':
                break
        return b''.join(chars)

    # Echo server using generators
    class EchoServer:
        def __init__(self,addr,sched):
            self.sched = sched
            sched.new(self.server_loop(addr))

        def server_loop(self,addr):
            s = Socket(socket(AF_INET,SOCK_STREAM))

12.12. Using Generators As an Alternative to Threads 

            s.bind(addr)
            s.listen(5)
            while True:
                c,a = yield s.accept()
                print('Got connection from ', a)
                self.sched.new(self.client_handler(Socket(c)))

        def client_handler(self,client):
            while True:
                line = yield from readline(client)
                if not line:
                    break
                line = b'GOT:' + line
                while line:
                    nsent = yield client.send(line)
                    line = line[nsent:]
            client.close()
            print('Client closed')

    sched = Scheduler()
    EchoServer(('',16000),sched)
    sched.run()

This code will undoubtedly require a certain amount of careful study. However, it is
essentially implementing a small operating system. There is a queue of tasks ready to
run and there are waiting areas for tasks sleeping for I/O. Much of the scheduler involves
moving tasks between the ready queue and the I/O waiting area.

Discussion
When building generator-based concurrency frameworks, it is most common to work
with the more general form of yield:

def some_generator():
    ...
    result = yield data
    ...

Functions that use yield in this manner are more generally referred to as “coroutines.”
Within a scheduler, the yield statement gets handled in a loop as follows:

f = some_generator()

# Initial result. Is None to start since nothing has been computed
result = None
while True:
    try:
        data = f.send(result)
        result = ... do some calculation ...
    except StopIteration:
        break

The logic concerning the result is a bit convoluted. However, the value passed to send()
defines what gets returned when the yield statement wakes back up. So, if a yield is
going to return a result in response to data that was previously yielded, it gets returned
on the next send() operation. If a generator function has just started, sending in a value
of None simply makes it advance to the first yield statement.
In addition to sending in values, it is also possible to execute a close() method on a
generator. This causes a silent GeneratorExit exception to be raised at the yield state‐
ment, which stops execution. If desired, a generator can catch this exception and per‐
form cleanup actions. It’s also possible to use the throw() method of a generator to raise
an arbitrary execution at the yield statement. A task scheduler might use this to com‐
municate errors into running generators.
The yield from statement used in the last example is used to implement coroutines
that serve as subroutines or procedures to be called from other generators. Essentially,
control transparently transfers to the new function. Unlike normal generators, a func‐
tion that is called using yield from can return a value that becomes the result of the
yield from statement. More information about yield from can be found in PEP 380.
Finally, if programming with generators, it is important to stress that there are some
major limitations. In particular, you get none of the benefits that threads provide. For
instance, if you execute any code that is CPU bound or which blocks for I/O, it will
suspend the entire task scheduler until the completion of that operation. To work around
this, your only real option is to delegate the operation to a separate thread or process
where it can run independently. Another limitation is that most Python libraries have
not been written to work well with generator-based threading. If you take this approach,
you may find that you need to write replacements for many standard library functions.
As basic background on coroutines and the techniques utilized in this recipe, see PEP
342 and “A Curious Course on Coroutines and Concurrency”.
PEP 3156 also has a modern take on asynchronous I/O involving coroutines. In practice,
it  is  extremelyunlikely  that  you  will  write  a  low-level  coroutine  scheduler  yourself.
However, ideas surrounding coroutines are the basis for many popular libraries, in‐
cluding gevent, greenlet, Stackless Python, and similar projects.
12.13. Polling Multiple Thread Queues
Problem
You have a collection of thread queues, and you would like to be able to poll them for
incoming items, much in the same way as you might poll a collection of network con‐
nections for incoming data.

12.13. Polling Multiple Thread Queues 

Solution
A common solution to polling problems involves a little-known trick involving a hidden
loopback network connection. Essentially, the idea is as follows: for each queue (or any
object) that you want to poll, you create a pair of connected sockets. You then write on
one of the sockets to signal the presence of data. The other sockect is then passed to
select() or a similar function to poll for the arrival of data. Here is some sample code
that illustrates this idea:

import queue
import socket
import os

class PollableQueue(queue.Queue):
    def __init__(self):
        super().__init__()
        # Create a pair of connected sockets
        if os.name == 'posix':
            self._putsocket, self._getsocket = socket.socketpair()
        else:
            # Compatibility on non-POSIX systems
            server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            server.bind(('127.0.0.1', 0))
            server.listen(1)
            self._putsocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self._putsocket.connect(server.getsockname())
            self._getsocket, _ = server.accept()
            server.close()

    def fileno(self):
        return self._getsocket.fileno()

    def put(self, item):
        super().put(item)
        self._putsocket.send(b'x')

    def get(self):
        self._getsocket.recv(1)
        return super().get()

In this code, a new kind of Queue instance is defined where there is an underlying pair
of connected sockets. The socketpair() function on Unix machines can establish such
sockets easily. On Windows, you have to fake it using code similar to that shown (it
looks a bit weird, but a server socket is created and a client immediately connects to it
afterward). The normal get() and put() methods are then redefined slightly to perform
a small bit of I/O on these sockets. The put() method writes a single byte of data to one
of the sockets after putting data on the queue. The get() method reads a single byte of
data from the other socket when removing an item from the queue.

The fileno() method is what makes the queue pollable using a function such as se
lect(). Essentially, it just exposes the underlying file descriptor of the socket used by
the get() function.
Here is an example of some code that defines a consumer which monitors multiple
queues for incoming items:

import select
import threading

def consumer(queues):
    '''
    Consumer that reads data on multiple queues simultaneously
    '''
    while True:
        can_read, _, _ = select.select(queues,[],[])
        for r in can_read:
            item = r.get()
            print('Got:', item)

q1 = PollableQueue()
q2 = PollableQueue()
q3 = PollableQueue()
t = threading.Thread(target=consumer, args=([q1,q2,q3],))
t.daemon = True
t.start()

# Feed data to the queues
q1.put(1)
q2.put(10)
q3.put('hello')
q2.put(15)
...

If you try it, you’ll find that the consumer indeed receives all of the put items, regardless
of which queues they are placed in.

Discussion
The problem of polling non-file-like objects, such as queues, is often a lot trickier than
it looks. For instance, if you don’t use the socket technique shown, your only option is
to write code that cycles through the queues and uses a timer, like this:

import time
def consumer(queues):
    while True:
        for q in queues:
            if not q.empty():
                item = q.get()
                print('Got:', item)

12.13. Polling Multiple Thread Queues 

        # Sleep briefly to avoid 100% CPU
        time.sleep(0.01)

This might work for certain kinds of problems, but it’s clumsy and introduces other
weird performance problems. For example, if new data is added to a queue, it won’t be
detected for as long as 10 milliseconds (an eternity on a modern processor).
You run into even further problems if the preceding polling is mixed with the polling
of other objects, such as network sockets. For example, if you want to poll both sockets
and queues at the same time, you might have to use code like this:

import select

def event_loop(sockets, queues):
    while True:
        # polling with a timeout
        can_read, _, _ = select.select(sockets, [], [], 0.01)
        for r in can_read:
            handle_read(r)
        for q in queues:
            if not q.empty():
                item = q.get()
                print('Got:', item)

The solution shown solves a lot of these problems by simply putting queues on equal
status with sockets. A single select() call can be used to poll for activity on both. It is
not necessary to use timeouts or other time-based hacks to periodically check. More‐
over, if data gets added to a queue, the consumer will be notified almost instantaneously.
Although there is a tiny amount of overhead associated with the underlying I/O, it often
is worth it to have better response time and simplified coding.
12.14. Launching a Daemon Process on Unix
Problem
You would like to write a program that runs as a proper daemon process on Unix or
Unix-like systems.

Solution
Creating a proper daemon process requires a precise sequence of system calls and careful
attention to detail. The following code shows how to define a daemon process along
with the ability to easily stop it once launched:

#!/usr/bin/env python3
# daemon.py

import os
import sys

import atexit
import signal

def daemonize(pidfile, *, stdin='/dev/null',
                          stdout='/dev/null',
                          stderr='/dev/null'):

    if os.path.exists(pidfile):
        raise RuntimeError('Already running')

    # First fork (detaches from parent)
    try:
        if os.fork() > 0:
            raise SystemExit(0)   # Parent exit
    except OSError as e:
        raise RuntimeError('fork #1 failed.')

    os.chdir('/')
    os.umask(0)
    os.setsid()
    # Second fork (relinquish session leadership)
    try:
        if os.fork() > 0:
            raise SystemExit(0)
    except OSError as e:
        raise RuntimeError('fork #2 failed.')

    # Flush I/O buffers
    sys.stdout.flush()
    sys.stderr.flush()

    # Replace file descriptors for stdin, stdout, and stderr
    with open(stdin, 'rb', 0) as f:
        os.dup2(f.fileno(), sys.stdin.fileno())
    with open(stdout, 'ab', 0) as f:
        os.dup2(f.fileno(), sys.stdout.fileno())
    with open(stderr, 'ab', 0) as f:
        os.dup2(f.fileno(), sys.stderr.fileno())

    # Write the PID file
    with open(pidfile,'w') as f:
        print(os.getpid(),file=f)

    # Arrange to have the PID file removed on exit/signal
    atexit.register(lambda: os.remove(pidfile))

    # Signal handler for termination (required)
    def sigterm_handler(signo, frame):
        raise SystemExit(1)

    signal.signal(signal.SIGTERM, sigterm_handler)

12.14. Launching a Daemon Process on Unix 

def main():
    import time
    sys.stdout.write('Daemon started with pid {}\n'.format(os.getpid()))
    while True:
        sys.stdout.write('Daemon Alive! {}\n'.format(time.ctime()))
        time.sleep(10)

if __name__ == '__main__':
    PIDFILE = '/tmp/daemon.pid'

    if len(sys.argv) != 2:
        print('Usage: {} [start|stop]'.format(sys.argv[0]), file=sys.stderr)
        raise SystemExit(1)

    if sys.argv[1] == 'start':
        try:
            daemonize(PIDFILE,
                      stdout='/tmp/daemon.log',
                      stderr='/tmp/dameon.log')
        except RuntimeError as e:
            print(e, file=sys.stderr)
            raise SystemExit(1)

        main()

    elif sys.argv[1] == 'stop':
        if os.path.exists(PIDFILE):
            with open(PIDFILE) as f:
                os.kill(int(f.read()), signal.SIGTERM)
        else:
            print('Not running', file=sys.stderr)
            raise SystemExit(1)

    else:
        print('Unknown command {!r}'.format(sys.argv[1]), file=sys.stderr)
        raise SystemExit(1)

To launch the daemon, the user would use a command like this:

bash % daemon.py start
bash % cat /tmp/daemon.pid
2882
bash % tail -f /tmp/daemon.log
Daemon started with pid 2882
Daemon Alive! Fri Oct 12 13:45:37 2012
Daemon Alive! Fri Oct 12 13:45:47 2012
...

Daemon processes run entirely in the background, so the command returns immedi‐
ately. However, you can view its associated pid file and log, as just shown. To stop the
daemon, use:

bash % daemon.py stop
bash %

Discussion
This recipe defines a function daemonize() that should be called at program startup to
make the program run as a daemon. The signature to daemonize() is using keyword-
only arguments to make the purpose of the optional arguments more clear when used.
This forces the user to use a call such as this:

daemonize('daemon.pid',
          stdin='/dev/null,
          stdout='/tmp/daemon.log',
          stderr='/tmp/daemon.log')

As opposed to a more cryptic call such as:
# Illegal. Must use keyword arguments
daemonize('daemon.pid',
          '/dev/null', '/tmp/daemon.log','/tmp/daemon.log')

The steps involved in creating a daemon are fairly cryptic, but the general idea is as
follows. First, a daemon has to detach itself from its parent process. This is the purpose
of the first os.fork() operation and immediate termination by the parent.
After the child has been orphaned, the call to  os.setsid() creates an entirely new
process session and sets the child as the leader. This also sets the child as the leader of
a new process group and makes sure there is no controlling terminal. If this all sounds
a bit too magical, it has to do with getting the daemon to detach properly from the
terminal and making sure that things like signals don’t interfere with its operation.
The calls to os.chdir() and os.umask(0) change the current working directory and
reset the file mode mask. Changing the directory is usually a good idea so that the
daemon is no longer working in the directory from which it was launched.
The second call to os.fork() is by far the more mysterious operation here. This step
makes the daemon process give up the ability to acquire a new controlling terminal and
provides even more isolation (essentially, the daemon gives up its session leadership
and thus no longer has the permission to open controlling terminals). Although you
could probably omit this step, it’s typically recommended.
Once the daemon process has been properly detached, it performs steps to reinitialize
the standard I/O streams to point at files specified by the user. This part is actually
somewhat tricky. References to file objects associated with the standard I/O streams are
found in multiple places in the interpreter (sys.stdout, sys.__stdout__, etc.). Simply
closing sys.stdout and reassigning it is not likely to work correctly, because there’s no
way to know if it will fix all uses of sys.stdout. Instead, a separate file object is opened,
and the os.dup2() call is used to have it replace the file descriptor currently being used

12.14. Launching a Daemon Process on Unix 

by sys.stdout. When this happens, the original file for sys.stdout will be closed and
the new one takes its place. It must be emphasized that any file encoding or text handling
already applied to the standard I/O streams will remain in place.
A common practice with daemon processes is to write the process ID of the daemon in
a file for later use by other programs. The last part of the daemonize() function writes
this file, but also arranges to have the file removed on program termination. The atex
it.register() function registers a function to execute when the Python interpreter
terminates. The definition of a signal handler for SIGTERM is also required for a graceful
termination. The signal handler merely raises SystemExit() and nothing more. This
might look unnecessary, but without it, termination signals kill the interpreter without
performing the cleanup actions registered with  atexit.register(). An example of
code that kills the daemon can be found in the handling of the stop command at the
end of the program.
More information about writing daemon processes can be found in Advanced Pro‐
gramming in the UNIX Environment, 2nd Edition, by W. Richard Stevens and Stephen
A. Rago (Addison-Wesley, 2005). Although focused on C programming, all of the ma‐
terial is easily adapted to Python, since all of the required POSIX functions are available
in the standard library.

CHAPTER 13
Utility Scripting and System Administration

A lot of people use Python as a replacement for shell scripts, using it to automate com‐
mon system tasks, such as manipulating files, configuring systems, and so forth. The
main goal of this chapter is to describe features related to common tasks encountered
when writing scripts. For example, parsing command-line options, manipulating files
on the filesystem, getting useful system configuration data, and so forth. Chapter 5 also
contains general information related to files and directories.
13.1. Accepting Script Input via Redirection, Pipes, or
Input Files
Problem
You want a script you’ve written to be able to accept input using whatever mechanism
is easiest for the user. This should include piping output from a command to the script,
redirecting a file into the script, or just passing a filename, or list of filenames, to the
script on the command line.

Solution
Python’s built-in fileinput module makes this very simple and concise. If you have a
script that looks like this:
#!/usr/bin/env python3
import fileinput

with fileinput.input() as f_input:
    for line in f_input:
        print(line, end='')

Then you can already accept input to the script in all of the previously mentioned ways.
If you save this script as filein.py and make it executable, you can do all of the following
and get the expected output:

$ ls | ./filein.py          # Prints a directory listing to stdout.
$ ./filein.py /etc/passwd   # Reads /etc/passwd to stdout.
$ ./filein.py < /etc/passwd # Reads /etc/passwd to stdout.

Discussion
The  fileinput.input() function creates and returns an instance of the  FileInput
class. In addition to containing a few handy helper methods, the instance can also be
used as a context manager. So, to put all of this together, if we wrote a script that expected
to be printing output from several files at once, we might have it include the filename
and line number in the output, like this:

>>> import fileinput
>>> with fileinput.input('/etc/passwd') as f:
>>>     for line in f:
...         print(f.filename(), f.lineno(), line, end='')
...
/etc/passwd 1 ##
/etc/passwd 2 # User Database
/etc/passwd 3 #

<other output omitted>

Using it as a context manager ensures that the file is closed when it’s no longer being
used, and we leveraged a few handy FileInput helper methods here to get some extra
information in the output.
13.2. Terminating a Program with an Error Message
Problem
You want your program to terminate by printing a message to standard error and re‐
turning a nonzero status code.

Solution
To have a program terminate in this manner, raise a SystemExit exception, but supply
the error message as an argument. For example:

raise SystemExit('It failed!')

This will cause the supplied message to be printed to sys.stderr and the program to
exit with a status code of 1.

Discussion
This is a small recipe, but it solves a common problem that arises when writing scripts.
Namely, to terminate a program, you might be inclined to write code like this:

import sys
sys.stderr.write('It failed!\n')
raise SystemExit(1)

None of the extra steps involving import or writing to sys.stderr are neccessary if you
simply supply the message to SystemExit() instead.
13.3. Parsing Command-Line Options
Problem
You want to write a program that parses options supplied on the command line (found
in sys.argv).

Solution
The argparse module can be used to parse command-line options. A simple example
will help to illustrate the essential features:

# search.py
'''
Hypothetical command-line tool for searching a collection of
files for one or more text patterns.
'''
import argparse
parser = argparse.ArgumentParser(description='Search some files')

parser.add_argument(dest='filenames',metavar='filename', nargs='*')

parser.add_argument('-p', '--pat',metavar='pattern', required=True,
                    dest='patterns', action='append',
                    help='text pattern to search for')

parser.add_argument('-v', dest='verbose', action='store_true',
                    help='verbose mode')

parser.add_argument('-o', dest='outfile', action='store',
                    help='output file')

parser.add_argument('--speed', dest='speed', action='store',
                    choices={'slow','fast'}, default='slow',
                    help='search speed')

args = parser.parse_args()

13.3. Parsing Command-Line Options 

# Output the collected arguments
print(args.filenames)
print(args.patterns)
print(args.verbose)
print(args.outfile)
print(args.speed)

This program defines a command-line parser with the following usage:

bash % python3 search.py -h
usage: search.py [-h] [-p pattern] [-v] [-o OUTFILE] [--speed {slow,fast}]
                 [filename [filename ...]]

Search some files

positional arguments:
  filename

optional arguments:
  -h, --help            show this help message and exit
  -p pattern, --pat pattern
                        text pattern to search for
  -v                    verbose mode
  -o OUTFILE            output file
  --speed {slow,fast}   search speed

The following session shows how data shows up in the program. Carefully observe the
output of the print() statements.

bash % python3 search.py foo.txt bar.txt
usage: search.py [-h] -p pattern [-v] [-o OUTFILE] [--speed {fast,slow}]
                 [filename [filename ...]]
search.py: error: the following arguments are required: -p/--pat

bash % python3 search.py -v -p spam --pat=eggs foo.txt bar.txt
filenames = ['foo.txt', 'bar.txt']
patterns  = ['spam', 'eggs']
verbose   = True
outfile   = None
speed     = slow

bash % python3 search.py -v -p spam --pat=eggs foo.txt bar.txt -o results
filenames = ['foo.txt', 'bar.txt']
patterns  = ['spam', 'eggs']
verbose   = True
outfile   = results
speed     = slow

bash % python3 search.py -v -p spam --pat=eggs foo.txt bar.txt -o results \
             --speed=fast
filenames = ['foo.txt', 'bar.txt']
patterns  = ['spam', 'eggs']
verbose   = True
outfile   = results
speed     = fast

Further processing of the options is up to the program. Replace the print() functions
with something more interesting.

Discussion
The argparse module is one of the largest modules in the standard library, and has a
huge number of configuration options. This recipe shows an essential subset that can
be used and extended to get started.
To parse options, you first create an ArgumentParser instance and add declarations for
the options you want to support it using the add_argument() method. In each add_ar
gument() call, the dest argument specifies the name of an attribute where the result of
parsing will be placed. The metavar argument is used when generating help messages.
The action argument specifies the processing associated with the argument and is often
store for storing a value or append for collecting multiple argument values into a list.
The following argument collects all of the extra command-line arguments into a list. It’s
being used to make a list of filenames in the example:

parser.add_argument(dest='filenames',metavar='filename', nargs='*')

The following argument sets a Boolean flag depending on whether or not the argument
was provided:

parser.add_argument('-v', dest='verbose', action='store_true',
                    help='verbose mode')

The following argument takes a single value and stores it as a string:

parser.add_argument('-o', dest='outfile', action='store',
                    help='output file')

The following argument specification allows an argument to be repeated multiple times
and all of the values append into a list. The required flag means that the argument must
be supplied at least once. The use of -p and --pat mean that either argument name is
acceptable.

parser.add_argument('-p', '--pat',metavar='pattern', required=True,
                    dest='patterns', action='append',
                    help='text pattern to search for')

Finally, the following argument specification takes a value, but checks it against a set of
possible choices.

parser.add_argument('--speed', dest='speed', action='store',
                    choices={'slow','fast'}, default='slow',
                    help='search speed')

Once the options have been given, you simply execute the parser.parse() method.
This will process the sys.argv value and return an instance with the results. The results

13.3. Parsing Command-Line Options 

for each argument are placed into an attribute with the name given in the dest parameter
to add_argument().
There are several other approaches for parsing command-line options. For example,
you might be inclined to manually process sys.argv yourself or use the getopt module
(which is modeled after a similarly named C library). However, if you take this approach,
you’ll simply end up replicating much of the code that argparse already provides. You
may also encounter code that uses the  optparse library to parse options. Although
optparse is very similar to argparse, the latter is more modern and should be preferred
in new projects.
13.4. Prompting for a Password at Runtime
Problem
You’ve written a script that requires a password, but since the script is meant for inter‐
active use, you’d like to prompt the user for a password rather than hardcode it into the
script.

Solution
Python’s getpass module is precisely what you need in this situation. It will allow you
to very easily prompt for a password without having the keyed-in password displayed
on the user’s terminal. Here’s how it’s done:

import getpass

user = getpass.getuser()
passwd = getpass.getpass()

if svc_login(user, passwd):    # You must write svc_login()
   print('Yay!')
else:
   print('Boo!')

In this code, the svc_login() function is code that you must write to further process
the password entry. Obviously, the exact handling is application-specific.

Discussion
Note in the preceding code that getpass.getuser() doesn’t prompt the user for their
username. Instead, it uses the current user’s login name, according to the user’s shell
environment, or as a last resort, according to the local system’s password database (on
platforms that support the pwd module).

If you want to explicitly prompt the user for their username, which can be more reliable,
use the built-in input function:

user = input('Enter your username: ')

It’s also important to remember that some systems may not support the hiding of the
typed password input to the getpass() method. In this case, Python does all it can to
forewarn you of problems (i.e., it alerts you that passwords will be shown in cleartext)
before moving on.
13.5. Getting the Terminal Size
Problem
You need to get the terminal size in order to properly format the output of your program.

Solution
Use the os.get_terminal_size() function to do this:

>>> import os
>>> sz = os.get_terminal_size()
>>> sz
os.terminal_size(columns=80, lines=24)
>>> sz.columns
80
>>> sz.lines
24
>>>

Discussion
There are many other possible approaches for obtaining the terminal size, ranging from
reading environment variables to executing low-level system calls involving ioctl()
and TTYs. Frankly, why would you bother with that when this one simple call will
suffice?
13.6. Executing an External Command and Getting Its
Output
Problem
You want to execute an external command and collect its output as a Python string.

13.5. Getting the Terminal Size 

Solution
Use the subprocess.check_output() function. For example:

import subprocess
out_bytes = subprocess.check_output(['netstat','-a'])

This runs the specified command and returns its output as a byte string. If you need to
interpret the resulting bytes as text, add a further decoding step. For example:

out_text = out_bytes.decode('utf-8')

If the executed command returns a nonzero exit code, an exception is raised. Here is
an example of catching errors and getting the output created along with the exit code:

try:
    out_bytes = subprocess.check_output(['cmd','arg1','arg2'])
except subprocess.CalledProcessError as e:
    out_bytes = e.output       # Output generated before error
    code      = e.returncode   # Return code

By default, check_output() only returns output written to standard output. If you want
both standard output and error collected, use the stderr argument:

out_bytes = subprocess.check_output(['cmd','arg1','arg2'],
                                    stderr=subprocess.STDOUT)

If you need to execute a command with a timeout, use the timeout argument:

try:
    out_bytes = subprocess.check_output(['cmd','arg1','arg2'], timeout=5)
except subprocess.TimeoutExpired as e:
    ...

Normally, commands are executed without the assistance of an underlying shell (e.g.,
sh, bash, etc.). Instead, the list of strings supplied are given to a low-level system com‐
mand, such as os.execve(). If you want the command to be interpreted by a shell,
supply it using a simple string and give the shell=True argument. This is sometimes
useful if you’re trying to get Python to execute a complicated shell command involving
pipes, I/O redirection, and other features. For example:

out_bytes = subprocess.check_output('grep python | wc > out', shell=True)

Be aware that executing commands under the shell is a potential security risk if argu‐
ments are derived from user input. The shlex.quote() function can be used to properly
quote arguments for inclusion in shell commands in this case.

Discussion
The check_output() function is the easiest way to execute an external command and
get its output. However, if you need to perform more advanced communication with a

subprocess, such as sending it input, you’ll need to take a difference approach. For that,
use the subprocess.Popen class directly. For example:

import subprocess

# Some text to send
text = b'''
hello world
this is a test
goodbye
'''

# Launch a command with pipes
p = subprocess.Popen(['wc'],
          stdout = subprocess.PIPE,
          stdin = subprocess.PIPE)

# Send the data and get the output
stdout, stderr = p.communicate(text)

# To interpret as text, decode
out = stdout.decode('utf-8')
err = stderr.decode('utf-8')

The subprocess module is not suitable for communicating with external commands
that expect to interact with a proper TTY. For example, you can’t use it to automate tasks
that ask the user to enter a password (e.g., a ssh session). For that, you would need to
turn to a third-party module, such as those based on the popular “expect” family of tools
(e.g., pexpect or similar).
13.7. Copying or Moving Files and Directories
Problem
You need to copy or move files and directories around, but you don’t want to do it by
calling out to shell commands.

Solution
The shutil module has portable implementations of functions for copying files and
directories. The usage is extremely straightforward. For example:

import shutil

# Copy src to dst. (cp src dst)
shutil.copy(src, dst)

# Copy files, but preserve metadata (cp -p src dst)
shutil.copy2(src, dst)

13.7. Copying or Moving Files and Directories 

# Copy directory tree (cp -R src dst)
shutil.copytree(src, dst)

# Move src to dst (mv src dst)
shutil.move(src, dst)

The arguments to these functions are all strings supplying file or directory names. The
underlying semantics try to emulate that of similar Unix commands, as shown in the
comments.
By default, symbolic links are followed by these commands. For example, if the source
file is a symbolic link, then the destination file will be a copy of the file the link points
to. If you want to copy the symbolic link instead, supply the follow_symlinks keyword
argument like this:

shutil.copy2(src, dst, follow_symlinks=False)

If you want to preserve symbolic links in copied directories, do this:

shutil.copytree(src, dst, symlinks=True)

The copytree() optionally allows you to ignore certain files and directories during the
copy process. To do this, you supply an ignore function that takes a directory name
and filename listing as input, and returns a list of names to ignore as a result. For ex‐
ample:

def ignore_pyc_files(dirname, filenames):
    return [name in filenames if name.endswith('.pyc')]

shutil.copytree(src, dst, ignore=ignore_pyc_files)

Since ignoring filename patterns is common, a utility function ignore_patterns() has
already been provided to do it. For example:

shutil.copytree(src, dst, ignore=shutil.ignore_patterns('*~','*.pyc'))

Discussion
Using  shutil to copy files and directories is mostly straightforward. However, one
caution concerning file metadata is that functions such as copy2() only make a best
effort in preserving this data. Basic information, such as access times, creation times,
and permissions, will always be preserved, but preservation of owners, ACLs, resource
forks, and other extended file metadata may or may not work depending on the un‐
derlying operating system and the user’s own access permissions. You probably wouldn’t
want to use a function like shutil.copytree() to perform system backups.
When working with filenames, make sure you use the functions in  os.path for the
greatest portability (especially if working with both Unix and Windows). For example:

>>> filename = '/Users/guido/programs/spam.py'
>>> import os.path
>>> os.path.basename(filename)
'spam.py'
>>> os.path.dirname(filename)
'/Users/guido/programs'
>>> os.path.split(filename)
('/Users/guido/programs', 'spam.py')
>>> os.path.join('/new/dir', os.path.basename(filename))
'/new/dir/spam.py'
>>> os.path.expanduser('~/guido/programs/spam.py')
'/Users/guido/programs/spam.py'
>>>

One tricky bit about copying directories with copytree() is the handling of errors. For
example, in the process of copying, the function might encounter broken symbolic links,
files that can’t be accessed due to permission problems, and so on. To deal with this, all
exceptions encountered are collected into a list and grouped into a single exception that
gets raised at the end of the operation. Here is how you would handle it:

try:
    shutil.copytree(src, dst)
except shutil.Error as e:
    for src, dst, msg in e.args[0]:
         # src is source name
         # dst is destination name
         # msg is error message from exception
         print(dst, src, msg)

If  you  supply  the  ignore_dangling_symlinks=True  keyword  argument,  then  copy
tree() will ignore dangling symlinks.
The functions shown in this recipe are probably the most commonly used. However,
shutil has many more operations related to copying data. The documentation is def‐
initely worth a further look. See the Python documentation.
13.8. Creating and Unpacking Archives
Problem
You need to create or unpack archives in common formats (e.g., .tar, .tgz, or .zip).

Solution
The shutil module has two functions—make_archive() and unpack_archive()—that
do exactly what you want. For example:

>>> import shutil
>>> shutil.unpack_archive('Python-3.3.0.tgz')

13.8. Creating and Unpacking Archives 

>>> shutil.make_archive('py33','zip','Python-3.3.0')
'/Users/beazley/Downloads/py33.zip'
>>>

The second argument to make_archive() is the desired output format. To get a list of
supported archive formats, use get_archive_formats(). For example:

>>> shutil.get_archive_formats()
[('bztar', "bzip2'ed tar-file"), ('gztar', "gzip'ed tar-file"),
 ('tar', 'uncompressed tar file'), ('zip', 'ZIP file')]
>>>

Discussion
Python has other library modules for dealing with the low-level details of various archive
formats (e.g., tarfile, zipfile, gzip, bz2, etc.). However, if all you’re trying to do is
make or extract an archive, there’s really no need to go so low level. You can just use
these high-level functions in shutil instead.
The functions have a variety of additional options for logging, dryruns, file permissions,
and so forth. Consult the shutil library documentation for further details.
13.9. Finding Files by Name
Problem
You need to write a script that involves finding files, like a file renaming script or a log
archiver utility, but you’d rather not have to call shell utilities from within your Python
script, or you want to provide specialized behavior not easily available by “shelling out.”

Solution
To search for files, use the os.walk() function, supplying it with the top-level directory.
Here is an example of a function that finds a specific filename and prints out the full
path of all matches:

#!/usr/bin/env python3.3
import os

def findfile(start, name):
    for relpath, dirs, files in os.walk(start):
        if name in files:
            full_path = os.path.join(start, relpath, name)
            print(os.path.normpath(os.path.abspath(full_path)))

if __name__ == '__main__':
    findfile(sys.argv[1], sys.argv[2])

Save this script as findfile.py and run it from the command line, feeding in the starting
point and the name as positional arguments, like this:

bash % ./findfile.py . myfile.txt

Discussion
The os.walk() method traverses the directory hierarchy for us, and for each directory
it enters, it returns a 3-tuple, containing the relative path to the directory it’s inspecting,
a list containing all of the directory names in that directory, and a list of filenames in
that directory.
For each tuple, you simply check if the target filename is in the  files list. If it is,
os.path.join() is used to put together a path. To avoid the possibility of weird looking
paths like ././foo//bar, two additional functions are used to fix the result. The first is
os.path.abspath(), which takes a path that might be relative and forms the absolute
path, and the second is os.path.normpath(), which will normalize the path, thereby
resolving issues with double slashes, multiple references to the current directory, and 
so on.
Although this script is pretty simple compared to the features of the find utility found
on UNIX platforms, it has the benefit of being cross-platform. Furthermore, a lot of
additional functionality can be added in a portable manner without much more work.
To illustrate, here is a function that prints out all of the files that have a recent modifi‐
cation time:

#!/usr/bin/env python3.3

import os
import time

def modified_within(top, seconds):
    now = time.time()
    for path, dirs, files in os.walk(top):
        for name in files:
            fullpath = os.path.join(path, name)
            if os.path.exists(fullpath):
                mtime = os.path.getmtime(fullpath)
                if mtime > (now - seconds):
                    print(fullpath)

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 3:
        print('Usage: {} dir seconds'.format(sys.argv[0]))
        raise SystemExit(1)

    modified_within(sys.argv[1], float(sys.argv[2]))

13.9. Finding Files by Name 

It wouldn’t take long for you to build far more complex operations on top of this little
function using various features of the os, os.path, glob, and similar modules. See Rec‐
ipes 5.11 and 5.13 for related recipes.
13.10. Reading Configuration Files
Problem
You  want  to  read  configuration  files  written  in  the  common  .ini  configuration  file
format.

Solution
The configparser module can be used to read configuration files. For example, suppose
you have this configuration file:

; config.ini
; Sample configuration file

[installation]
library=%(prefix)s/lib
include=%(prefix)s/include
bin=%(prefix)s/bin
prefix=/usr/local

# Setting related to debug configuration
[debug]
log_errors=true
show_warnings=False

[server]
port: 8080
nworkers: 32
pid-file=/tmp/spam.pid
root=/www/root
signature:
    =================================
    Brought to you by the Python Cookbook
    =================================

Here is an example of how to read it and extract values:

>>> from configparser import ConfigParser
>>> cfg = ConfigParser()
>>> cfg.read('config.ini')
['config.ini']
>>> cfg.sections()
['installation', 'debug', 'server']
>>> cfg.get('installation','library')
'/usr/local/lib'
>>> cfg.getboolean('debug','log_errors')

True
>>> cfg.getint('server','port')
8080
>>> cfg.getint('server','nworkers')
32
>>> print(cfg.get('server','signature'))

=================================
Brought to you by the Python Cookbook
=================================
>>>

If desired, you can also modify the configuration and write it back to a file using the
cfg.write() method. For example:

>>> cfg.set('server','port','9000')
>>> cfg.set('debug','log_errors','False')
>>> import sys
>>> cfg.write(sys.stdout)
[installation]
library = %(prefix)s/lib
include = %(prefix)s/include
bin = %(prefix)s/bin
prefix = /usr/local

[debug]
log_errors = False
show_warnings = False

[server]
port = 9000
nworkers = 32
pid-file = /tmp/spam.pid
root = /www/root
signature =
          =================================
          Brought to you by the Python Cookbook
          =================================
>>>

Discussion
Configuration files are well suited as a human-readable format for specifying configu‐
ration data to your program. Within each config file, values are grouped into different
sections (e.g., “installation,” “debug,” and “server,” in the example). Each section then
specifies values for various variables in that section.
There are several notable differences between a config file and using a Python source
file for the same purpose. First, the syntax is much more permissive and “sloppy.” For
example, both of these assignments are equivalent:

13.10. Reading Configuration Files 

prefix=/usr/local
prefix: /usr/local

The names used in a config file are also assumed to be case-insensitive. For example:

>>> cfg.get('installation','PREFIX')
'/usr/local'
>>> cfg.get('installation','prefix')
'/usr/local'
>>>

When parsing values, methods such as getboolean() look for any reasonable value.
For example, these are all equivalent:

    log_errors = true
    log_errors = TRUE
    log_errors = Yes
    log_errors = 1

Perhaps the most significant difference between a config file and Python code is that,
unlike scripts, configuration files are not executed in a top-down manner. Instead, the
file is read in its entirety. If variable substitutions are made, they are done after the fact.
For example, in this part of the config file, it doesn’t matter that the prefix variable is
assigned after other variables that happen to use it:

    [installation]
    library=%(prefix)s/lib
    include=%(prefix)s/include
    bin=%(prefix)s/bin
    prefix=/usr/local

An easily overlooked feature of ConfigParser is that it can read multiple configuration
files together and merge their results into a single configuration. For example, suppose
a user made their own configuration file that looked like this:

    ; ~/.config.ini
    [installation]
    prefix=/Users/beazley/test

    [debug]
    log_errors=False

This file can be merged with the previous configuration by reading it separately. For
example:

>>> # Previously read configuration
>>> cfg.get('installation', 'prefix')
'/usr/local'

>>> # Merge in user-specific configuration
>>> import os
>>> cfg.read(os.path.expanduser('~/.config.ini'))
['/Users/beazley/.config.ini']

>>> cfg.get('installation', 'prefix')
'/Users/beazley/test'
>>> cfg.get('installation', 'library')
'/Users/beazley/test/lib'
>>> cfg.getboolean('debug', 'log_errors')
False
>>>

Observe how the override of the prefix variable affects other related variables, such as
the setting of library. This works because variable interpolation is performed as late
as possible. You can see this by trying the following experiment:

>>> cfg.get('installation','library')
'/Users/beazley/test/lib'
>>> cfg.set('installation','prefix','/tmp/dir')
>>> cfg.get('installation','library')
'/tmp/dir/lib'
>>>

Finally, it’s important to note that Python does not support the full range of features you
might find in an .ini file used by other programs (e.g., applications on Windows). Make
sure you consult the configparser documentation for the finer details of the syntax
and supported features. 
13.11. Adding Logging to Simple Scripts
Problem
You want scripts and simple programs to write diagnostic information to log files.

Solution
The easiest way to add logging to simple programs is to use the logging module. For
example:

import logging

def main():
    # Configure the logging system
    logging.basicConfig(
        filename='app.log',
        level=logging.ERROR
    )

    # Variables (to make the calls that follow work)
    hostname = 'www.python.org'
    item = 'spam'
    filename = 'data.csv'
    mode = 'r'

13.11. Adding Logging to Simple Scripts 

    # Example logging calls (insert into your program)
    logging.critical('Host %s unknown', hostname)
    logging.error("Couldn't find %r", item)
    logging.warning('Feature is deprecated')
    logging.info('Opening file %r, mode=%r', filename, mode)
    logging.debug('Got here')

if __name__ == '__main__':
    main()

The five logging calls (critical(), error(), warning(), info(), debug()) represent
different severity levels in decreasing order. The level argument to basicConfig() is
a filter. All messages issued at a level lower than this setting will be ignored.
The argument to each logging operation is a message string followed by zero or more
arguments. When making the final log message, the % operator is used to format the
message string using the supplied arguments.
If you run this program, the contents of the file app.log will be as follows:

    CRITICAL:root:Host www.python.org unknown
    ERROR:root:Could not find 'spam'

If you want to change the output or level of output, you can change the parameters to
the basicConfig() call. For example:

logging.basicConfig(
     filename='app.log',
     level=logging.WARNING,
     format='%(levelname)s:%(asctime)s:%(message)s')

As a result, the output changes to the following:

    CRITICAL:2012-11-20 12:27:13,595:Host www.python.org unknown
    ERROR:2012-11-20 12:27:13,595:Could not find 'spam'
    WARNING:2012-11-20 12:27:13,595:Feature is deprecated

As shown, the logging configuration is hardcoded directly into the program. If you want
to configure it from a configuration file, change the basicConfig() call to the following:

import logging
import logging.config

def main():
    # Configure the logging system
    logging.config.fileConfig('logconfig.ini')
    ...

Now make a configuration file logconfig.ini that looks like this:

    [loggers]
    keys=root

    [handlers]
    keys=defaultHandler

    [formatters]
    keys=defaultFormatter

    [logger_root]
    level=INFO
    handlers=defaultHandler
    qualname=root

    [handler_defaultHandler]
    class=FileHandler
    formatter=defaultFormatter
    args=('app.log', 'a')

    [formatter_defaultFormatter]
    format=%(levelname)s:%(name)s:%(message)s

If you want to make changes to the configuration, you can simply edit the logcon‐
fig.ini file as appropriate.

Discussion
Ignoring for the moment that there are about a million advanced configuration options
for the logging module, this solution is quite sufficient for simple programs and scripts.
Simply make sure that you execute the basicConfig() call prior to making any logging
calls, and your program will generate logging output.
If you want the logging messages to route to standard error instead of a file, don’t supply
any filename information to basicConfig(). For example, simply do this:

logging.basicConfig(level=logging.INFO)

One subtle aspect of basicConfig() is that it can only be called once in your program.
If you later need to change the configuration of the logging module, you need to obtain
the root logger and make changes to it directly. For example:

logging.getLogger().level = logging.DEBUG

It must be emphasized that this recipe only shows a basic use of the logging module.
There are significantly more advanced customizations that can be made. An excellent
resource for such customization is the “Logging Cookbook”.

13.11. Adding Logging to Simple Scripts 

13.12. Adding Logging to Libraries
Problem
You would like to add a logging capability to a library, but don’t want it to interfere with
programs that don’t use logging.

Solution
For libraries that want to perform logging, you should create a dedicated logger object,
and initially configure it as follows:

# somelib.py

import logging
log = logging.getLogger(__name__)
log.addHandler(logging.NullHandler())

# Example function (for testing)
def func():
    log.critical('A Critical Error!')
    log.debug('A debug message')

With this configuration, no logging will occur by default. For example:

>>> import somelib
>>> somelib.func()
>>>

However, if the logging system gets configured, log messages will start to appear. For
example:

>>> import logging
>>> logging.basicConfig()
>>> somelib.func()
CRITICAL:somelib:A Critical Error!
>>>

Discussion
Libraries present a special problem for logging, since information about the environ‐
ment in which they are used isn’t known. As a general rule, you should never write
library code that tries to configure the logging system on its own or which makes as‐
sumptions about an already existing logging configuration. Thus, you need to take great
care to provide isolation.
The call to getLogger(__name__) creates a logger module that has the same name as
the calling module. Since all modules are unique, this creates a dedicated logger that is
likely to be separate from other loggers.

The log.addHandler(logging.NullHandler()) operation attaches a null handler to
the just created logger object. A null handler ignores all logging messages by default.
Thus, if the library is used and logging is never configured, no messages or warnings
will appear.
One subtle feature of this recipe is that the logging of individual libraries can be inde‐
pendently configured, regardless of other logging settings. For example, consider the
following code:

>>> import logging
>>> logging.basicConfig(level=logging.ERROR)
>>> import somelib
>>> somelib.func()
CRITICAL:somelib:A Critical Error!

>>> # Change the logging level for 'somelib' only
>>> logging.getLogger('somelib').level=logging.DEBUG
>>> somelib.func()
CRITICAL:somelib:A Critical Error!
DEBUG:somelib:A debug message
>>>

Here, the root logger has been configured to only output messages at the ERROR level or
higher. However, the level of the logger for somelib has been separately configured to
output debugging messages. That setting takes precedence over the global setting.
The ability to change the logging settings for a single module like this can be a useful
debugging tool, since you don’t have to change any of the global logging settings—simply
change the level for the one module where you want more output.
The “Logging HOWTO” has more information about configuring the logging module
and other useful tips.
13.13. Making a Stopwatch Timer
Problem
You want to be able to record the time it takes to perform various tasks.

Solution
The time module contains various functions for performing timing-related functions.
However, it’s often useful to put a higher-level interface on them that mimics a stop
watch. For example:

13.13. Making a Stopwatch Timer 

import time

class Timer:
    def __init__(self, func=time.perf_counter):
        self.elapsed = 0.0
        self._func = func
        self._start = None

    def start(self):
        if self._start is not None:
            raise RuntimeError('Already started')
        self._start = self._func()

    def stop(self):
        if self._start is None:
            raise RuntimeError('Not started')
        end = self._func()
        self.elapsed += end - self._start
        self._start = None

    def reset(self):
        self.elapsed = 0.0

    @property
    def running(self):
        return self._start is not None

    def __enter__(self):
        self.start()
        return self

    def __exit__(self, *args):
        self.stop()

This class defines a timer that can be started, stopped, and reset as needed by the user.
It keeps track of the total elapsed time in the elapsed attribute. Here is an example that
shows how it can be used:

def countdown(n):
    while n > 0:
        n -= 1

# Use 1: Explicit start/stop
t = Timer()
t.start()
countdown(1000000)
t.stop()
print(t.elapsed)

# Use 2: As a context manager
with t:
    countdown(1000000)

print(t.elapsed)

with Timer() as t2:
    countdown(1000000)
print(t2.elapsed)

Discussion
This recipe provides a simple yet very useful class for making timing measurements and
tracking  elapsed  time.  It’s  also  a  nice  illustration  of  how  to  support  the  context-
management protocol and the with statement.
One issue in making timing measurements concerns the underlying time function used
to do it. As a general rule, the accuracy of timing measurements made with functions
such as  time.time() or  time.clock() varies according to the operating system. In
contrast, the time.perf_counter() function always uses the highest-resolution timer
available on the system.
As shown, the time recorded by the Timer class is made according to wall-clock time,
and includes all time spent sleeping. If you only want the amount of CPU time used by
the process, use time.process_time() instead. For example:

t = Timer(time.process_time)
with t:
    countdown(1000000)
print(t.elapsed)

Both the time.perf_counter() and time.process_time() return a “time” in fractional
seconds. However, the actual value of the time doesn’t have any particular meaning. To
make sense of the results, you have to call the functions twice and compute a time
difference.
More examples of timing and profiling are given in Recipe 14.13.
13.14. Putting Limits on Memory and CPU Usage
Problem
You want to place some limits on the memory or CPU use of a program running on
Unix system.

Solution
The resource module can be used to perform both tasks. For example, to restrict CPU
time, do the following:

13.14. Putting Limits on Memory and CPU Usage 

import signal
import resource
import os

def time_exceeded(signo, frame):
    print("Time's up!")
    raise SystemExit(1)

def set_max_runtime(seconds):
    # Install the signal handler and set a resource limit
    soft, hard = resource.getrlimit(resource.RLIMIT_CPU)
    resource.setrlimit(resource.RLIMIT_CPU, (seconds, hard))
    signal.signal(signal.SIGXCPU, time_exceeded)

if __name__ == '__main__':
    set_max_runtime(15)
    while True:
        pass

When this runs, the SIGXCPU signal is generated when the time expires. The program
can then clean up and exit.
To restrict memory use, put a limit on the total address space in use. For example:

import resource

def limit_memory(maxsize):
    soft, hard = resource.getrlimit(resource.RLIMIT_AS)
    resource.setrlimit(resource.RLIMIT_AS, (maxsize, hard))

With a memory limit in place, programs will start generating MemoryError exceptions
when no more memory is available.

Discussion
In this recipe, the setrlimit() function is used to set a soft and hard limit on a particular
resource. The soft limit is a value upon which the operating system will typically restrict
or notify the process via a signal. The hard limit represents an upper bound on the values
that may be used for the soft limit. Typically, this is controlled by a system-wide pa‐
rameter set by the system administrator. Although the hard limit can be lowered, it can
never be raised by user processes (even if the process lowered itself).
The setrlimit() function can additionally be used to set limits on things such as the
number of child processes, number of open files, and similar system resources. Consult
the documentation for the resource module for further details.
Be aware that this recipe only works on Unix systems, and that it might not work on all
of them. For example, when tested, it works on Linux but not on OS X.

13.15. Launching a Web Browser
Problem
You want to launch a browser from a script and have it point to some URL that you
specify.

Solution
The webbrowser module can be used to launch a browser in a platform-independent
manner. For example:

>>> import webbrowser
>>> webbrowser.open('http://www.python.org')
True
>>>

This opens the requested page using the default browser. If you want a bit more control
over how the page gets opened, you can use one of the following functions:

>>> # Open the page in a new browser window
>>> webbrowser.open_new('http://www.python.org')
True
>>>

>>> # Open the page in a new browser tab
>>> webbrowser.open_new_tab('http://www.python.org')
True
>>>

These will try to open the page in a new browser window or tab, if possible and supported
by the browser.
If you want to open a page in a specific browser, you can use the webbrowser.get()
function to specify a particular browser. For example:

>>> c = webbrowser.get('firefox')
>>> c.open('http://www.python.org')
True
>>> c.open_new_tab('http://docs.python.org')
True
>>>

A full list of supported browser names can be found in the Python documentation.

13.15. Launching a Web Browser 

Discussion
Being able to easily launch a browser can be a useful operation in many scripts. For
example, maybe a script performs some kind of deployment to a server and you’d like
to have it quickly launch a browser so you can verify that it’s working. Or maybe a
program writes data out in the form of HTML pages and you’d just like to fire up a
browser to see the result. Either way, the webbrowser module is a simple solution.

CHAPTER 14
Testing, Debugging, and Exceptions

Testing rocks, but debugging? Not so much. The fact that there’s no compiler to analyze
your code before Python executes it makes testing a critical part of development. The
goal of this chapter is to discuss some common problems related to testing, debugging,
and exception handling. It is not meant to be a gentle introduction to test-driven de‐
velopment or the unittest module. Thus, some familiarity with testing concepts is 
assumed.
14.1. Testing Output Sent to stdout
Problem
You  have  a  program  that  has  a  method  whose  output  goes  to  standard  Output
(sys.stdout). This almost always means that it emits text to the screen. You’d like to
write a test for your code to prove that, given the proper input, the proper output is
displayed.

Solution
Using the unittest.mock module’s patch() function, it’s pretty simple to mock out
sys.stdout for just a single test, and put it back again, without messy temporary vari‐
ables or leaking mocked-out state between test cases.
Consider, as an example, the following function in a module mymodule:

# mymodule.py

def urlprint(protocol, host, domain):
    url = '{}://{}.{}'.format(protocol, host, domain)
    print(url)

The built-in print function, by default, sends output to sys.stdout. In order to test
that output is actually getting there, you can mock it out using a stand-in object, and
then make assertions about what happened. Using the unittest.mock module’s patch()
method makes it convenient to replace objects only within the context of a running test,
returning things to their original state immediately after the test is complete. Here’s the
test code for mymodule:

from io import StringIO
from unittest import TestCase
from unittest.mock import patch
import mymodule

class TestURLPrint(TestCase):
    def test_url_gets_to_stdout(self):
        protocol = 'http'
        host = 'www'
        domain = 'example.com'
        expected_url = '{}://{}.{}\n'.format(protocol, host, domain)

        with patch('sys.stdout', new=StringIO()) as fake_out:
            mymodule.urlprint(protocol, host, domain)
            self.assertEqual(fake_out.getvalue(), expected_url)

Discussion
The urlprint() function takes three arguments, and the test starts by setting up dummy
arguments for each one. The expected_url variable is set to a string containing the
expected output.
To run the test, the unittest.mock.patch() function is used as a context manager to
replace the value of sys.stdout with a StringIO object as a substitute. The fake_out
variable is the mock object that’s created in this process. This can be used inside the
body of the with statement to perform various checks. When the with statement com‐
pletes, patch conveniently puts everything back the way it was before the test ever ran.
It’s worth noting that certain C extensions to Python may write directly to standard
output, bypassing the setting of sys.stdout. This recipe won’t help with that scenario,
but it should work fine with pure Python code (if you need to capture I/O from such C
extensions, you can do it by opening a temporary file and performing various tricks
involving file descriptors to have standard output temporarily redirected to that file).
More information about capturing IO in a string and StringIO objects can be found in
Recipe 5.6. 

14.2. Patching Objects in Unit Tests
Problem
You’re writing unit tests and need to apply patches to selected objects in order to make
assertions about how they were used in the test (e.g., assertions about being called with
certain parameters, access to selected attributes, etc.).

Solution
The unittest.mock.patch() function can be used to help with this problem. It’s a little
unusual, but patch() can be used as a decorator, a context manager, or stand-alone. For
example, here’s an example of how it’s used as a decorator:

from unittest.mock import patch
import example

@patch('example.func')
def test1(x, mock_func):
    example.func(x)       # Uses patched example.func
    mock_func.assert_called_with(x)
It can also be used as a context manager:

with patch('example.func') as mock_func:
    example.func(x)      # Uses patched example.func
    mock_func.assert_called_with(x)

Last, but not least, you can use it to patch things manually:

p = patch('example.func')
mock_func = p.start()
example.func(x)
mock_func.assert_called_with(x)
p.stop()

If necessary, you can stack decorators and context managers to patch multiple objects.
For example:

@patch('example.func1')
@patch('example.func2')
@patch('example.func3')
def test1(mock1, mock2, mock3):
    ...

def test2():
    with patch('example.patch1') as mock1, \
         patch('example.patch2') as mock2, \
         patch('example.patch3') as mock3:
    ...

14.2. Patching Objects in Unit Tests 

Discussion
patch() works by taking an existing object with the fully qualified name that you pro‐
vide and replacing it with a new value. The original value is then restored after the
completion of the decorated function or context manager. By default, values are replaced
with MagicMock instances. For example:

>>> x = 42
>>> with patch('__main__.x'):
...     print(x)
...
<MagicMock name='x' id='4314230032'>
>>> x
42
>>>

However, you can actually replace the value with anything that you wish by supplying
it as a second argument to patch():

>>> x
42
>>> with patch('__main__.x', 'patched_value'):
...     print(x)
...
patched_value
>>> x
42
>>>

The MagicMock instances that are normally used as replacement values are meant to
mimic callables and instances. They record information about usage and allow you to
make assertions. For example:

>>> from unittest.mock import MagicMock
>>> m = MagicMock(return_value = 10)
>>> m(1, 2, debug=True)
10
>>> m.assert_called_with(1, 2, debug=True)
>>> m.assert_called_with(1, 2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File ".../unittest/mock.py", line 726, in assert_called_with
    raise AssertionError(msg)
AssertionError: Expected call: mock(1, 2)
Actual call: mock(1, 2, debug=True)
>>>

>>> m.upper.return_value = 'HELLO'
>>> m.upper('hello')
'HELLO'
>>> assert m.upper.called

>>> m.split.return_value = ['hello', 'world']
>>> m.split('hello world')
['hello', 'world']
>>> m.split.assert_called_with('hello world')
>>>

>>> m['blah']
<MagicMock name='mock.__getitem__()' id='4314412048'>
>>> m.__getitem__.called
True
>>> m.__getitem__.assert_called_with('blah')
>>>

Typically, these kinds of operations are carried out in a unit test. For example, suppose
you have some function like this:

# example.py
from urllib.request import urlopen
import csv

def dowprices():
    u = urlopen('http://finance.yahoo.com/d/quotes.csv?s=@^DJI&f=sl1')
    lines = (line.decode('utf-8') for line in u)
    rows = (row for row in csv.reader(lines) if len(row) == 2)
    prices = { name:float(price) for name, price in rows }
    return prices

Normally, this function uses urlopen() to go fetch data off the Web and parse it. To
unit test it, you might want to give it a more predictable dataset of your own creation,
however. Here’s an example using patching:

import unittest
from unittest.mock import patch
import io
import example

sample_data = io.BytesIO(b'''\
"IBM",91.1\r
"AA",13.25\r
"MSFT",27.72\r
\r
''')

class Tests(unittest.TestCase):
    @patch('example.urlopen', return_value=sample_data)
    def test_dowprices(self, mock_urlopen):
        p = example.dowprices()
        self.assertTrue(mock_urlopen.called)
        self.assertEqual(p,
                         {'IBM': 91.1,
                          'AA': 13.25,
                          'MSFT' : 27.72})

14.2. Patching Objects in Unit Tests 

if __name__ == '__main__':
    unittest.main()

In this example, the urlopen() function in the example module is replaced with a mock
object that returns a BytesIO() containing sample data as a substitute.
An important but subtle facet of this test is the patching of example.urlopen instead of
urllib.request.urlopen. When you are making patches, you have to use the names
as they are used in the code being tested. Since the example code uses from urllib.re
quest import urlopen, the urlopen() function used by the dowprices() function is
actually located in example.
This recipe has really only given a very small taste of what’s possible with the  uni
ttest.mock module. The official documentation is a must-read for more advanced
features.
14.3. Testing for Exceptional Conditions in Unit Tests
Problem
You want to write a unit test that cleanly tests if an exception is raised.

Solution
To test for exceptions, use the assertRaises() method. For example, if you want to test
that a function raised a ValueError exception, use this code:

import unittest

# A simple function to illustrate
def parse_int(s):
    return int(s)

class TestConversion(unittest.TestCase):
    def test_bad_int(self):
        self.assertRaises(ValueError, parse_int, 'N/A')

If you need to test the exception’s value in some way, then a different approach is needed.
For example:

import errno

class TestIO(unittest.TestCase):
    def test_file_not_found(self):
        try:
            f = open('/file/not/found')
        except IOError as e:
            self.assertEqual(e.errno, errno.ENOENT)

        else:
            self.fail('IOError not raised')

Discussion
The assertRaises() method provides a convenient way to test for the presence of an
exception. A common pitfall is to write tests that manually try to do things with excep‐
tions on their own. For instance:

class TestConversion(unittest.TestCase):
    def test_bad_int(self):
        try:
            r = parse_int('N/A')
        except ValueError as e:
            self.assertEqual(type(e), ValueError)

The problem with such approaches is that it is easy to forget about corner cases, such
as that when no exception is raised at all. To do that, you need to add an extra check for
that situation, as shown here:

class TestConversion(unittest.TestCase):
    def test_bad_int(self):
        try:
            r = parse_int('N/A')
        except ValueError as e:
            self.assertEqual(type(e), ValueError)
        else:
            self.fail('ValueError not raised')

The assertRaises() method simply takes care of these details, so you should prefer to
use it.
The one limitation of assertRaises() is that it doesn’t provide a means for testing the
value of the exception object that’s created. To do that, you have to manually test it, as
shown. Somewhere in between these two extremes, you might consider using the as
sertRaisesRegex() method, which allows you to test for an exception and perform a
regular expression match against the exception’s string representation at the same time.
For example:

class TestConversion(unittest.TestCase):
    def test_bad_int(self):
        self.assertRaisesRegex(ValueError, 'invalid literal .*',
                                       parse_int, 'N/A')

A little-known fact about assertRaises() and assertRaisesRegex() is that they can
also be used as context managers:

class TestConversion(unittest.TestCase):
    def test_bad_int(self):
        with self.assertRaisesRegex(ValueError, 'invalid literal .*'):
            r = parse_int('N/A')

14.3. Testing for Exceptional Conditions in Unit Tests 

This form can be useful if your test involves multiple steps (e.g., setup) besides that of
simply executing a callable.
14.4. Logging Test Output to a File
Problem
You want the results of running unit tests written to a file instead of printed to standard
output.

Solution
A very common technique for running unit tests is to include a small code fragment
like this at the bottom of your testing file:

import unittest

class MyTest(unittest.TestCase):
    ...

if __name__ == '__main__':
    unittest.main()

This makes the test file executable, and prints the results of running tests to standard
output. If you would like to redirect this output, you need to unwind the main() call a
bit and write your own main() function like this:

import sys
def main(out=sys.stderr, verbosity=2):
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromModule(sys.modules[__name__])
    unittest.TextTestRunner(out,verbosity=verbosity).run(suite)

if __name__ == '__main__':
    with open('testing.out', 'w') as f:
        main(f)

Discussion
The interesting thing about this recipe is not so much the task of getting test results
redirected to a file, but the fact that doing so exposes some notable inner workings of
the unittest module.
At a basic level, the unittest module works by first assembling a test suite. This test
suite consists of the different testing methods you defined. Once the suite has been
assembled, the tests it contains are executed.

These two parts of unit testing are separate from each other. The unittest.TestLoad
er instance created in the solution is used to assemble a test suite. The loadTestsFrom
Module() is one of several methods it defines to gather tests. In this case, it scans a
module for TestCase classes and extracts test methods from them. If you want some‐
thing more fine-grained, the loadTestsFromTestCase() method (not shown) can be
used to pull test methods from an individual class that inherits from TestCase.
The TextTestRunner class is an example of a test runner class. The main purpose of
this class is to execute the tests contained in a test suite. This class is the same test runner
that sits behind the unittest.main() function. However, here we’re giving it a bit of
low-level configuration, including an output file and an elevated verbosity level.
Although this recipe only consists of a few lines of code, it gives a hint as to how you
might further customize the  unittest framework. To customize how test suites are
assembled, you would perform various operations using the TestLoader class. To cus‐
tomize how tests execute, you could make custom test runner classes that emulate the
functionality of TextTestRunner. Both topics are beyond the scope of what can be cov‐
ered here. However, documentation for the unittest module has extensive coverage
of the underlying protocols. 
14.5. Skipping or Anticipating Test Failures
Problem
You want to skip or mark selected tests as an anticipated failure in your unit tests.

Solution
The unittest module has decorators that can be applied to selected test methods to
control their handling. For example:

import unittest
import os
import platform

class Tests(unittest.TestCase):
    def test_0(self):
        self.assertTrue(True)

    @unittest.skip('skipped test')
    def test_1(self):
        self.fail('should have failed!')

    @unittest.skipIf(os.name=='posix', 'Not supported on Unix')
    def test_2(self):
        import winreg

14.5. Skipping or Anticipating Test Failures 

    @unittest.skipUnless(platform.system() == 'Darwin', 'Mac specific test')
    def test_3(self):
        self.assertTrue(True)

    @unittest.expectedFailure
    def test_4(self):
        self.assertEqual(2+2, 5)

if __name__ == '__main__':
    unittest.main()

If you run this code on a Mac, you’ll get this output:

    bash % python3 testsample.py -v
    test_0 (__main__.Tests) ... ok
    test_1 (__main__.Tests) ... skipped 'skipped test'
    test_2 (__main__.Tests) ... skipped 'Not supported on Unix'
    test_3 (__main__.Tests) ... ok
    test_4 (__main__.Tests) ... expected failure

    ----------------------------------------------------------------------
    Ran 5 tests in 0.002s

    OK (skipped=2, expected failures=1)

Discussion
The skip() decorator can be used to skip over a test that you don’t want to run at all.
skipIf() and skipUnless() can be a useful way to write tests that only apply to certain
platforms or Python versions, or which have other dependencies. Use the @expected
Failure decorator to mark tests that are known failures, but for which you don’t want
the test framework to report more information.
The decorators for skipping methods can also be applied to entire testing classes. For
example:

@unittest.skipUnless(platform.system() == 'Darwin', 'Mac specific tests')
class DarwinTests(unittest.TestCase):
    ...

14.6. Handling Multiple Exceptions
Problem
You have a piece of code that can throw any of several different exceptions, and you
need to account for all of the potential exceptions that could be raised without creating
duplicate code or long, meandering code passages.

Solution
If you can handle different exceptions all using a single block of code, they can be
grouped together in a tuple like this:

try:
    client_obj.get_url(url)
except (URLError, ValueError, SocketTimeout):
    client_obj.remove_url(url)

In the preceding example, the remove_url() method will be called if any one of the
listed exceptions occurs. If, on the other hand, you need to handle one of the exceptions
differently, put it into its own except clause:

try:
    client_obj.get_url(url)
except (URLError, ValueError):
    client_obj.remove_url(url)
except SocketTimeout:
    client_obj.handle_url_timeout(url)

Many exceptions are grouped into an inheritance hierarchy. For such exceptions, you
can catch all of them by simply specifying a base class. For example, instead of writing
code like this:

try:
    f = open(filename)
except (FileNotFoundError, PermissionError):
    ...

you could rewrite the except statement as:

try:
    f = open(filename)
except OSError:
    ...

This works because OSError is a base class that’s common to both the FileNotFound
Errorand PermissionError exceptions.

Discussion
Although it’s not specific to handling multiple exceptions per se, it’s worth noting that
you can get a handle to the thrown exception using the as keyword:

try:
    f = open(filename)
except OSError as e:
    if e.errno == errno.ENOENT:
        logger.error('File not found')
    elif e.errno == errno.EACCES:
        logger.error('Permission denied')

14.6. Handling Multiple Exceptions 

    else:
        logger.error('Unexpected error: %d', e.errno)

In this example, the e variable holds an instance of the raised OSError. This is useful if
you need to inspect the exception further, such as processing it based on the value of an
additional status code.
Be aware that except clauses are checked in the order listed and that the first match
executes. It may be a bit pathological, but you can easily create situations where multiple
except clauses might match. For example:

>>> f = open('missing')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
FileNotFoundError: [Errno 2] No such file or directory: 'missing'
>>> try:
...     f = open('missing')
... except OSError:
...     print('It failed')
... except FileNotFoundError:
...     print('File not found')
...
It failed
>>>

Here the  except  FileNotFoundError clause doesn’t execute because the  OSError is
more general, matches the FileNotFoundError exception, and was listed first.
As a debugging tip, if you’re not entirely sure about the class hierarchy of a particular
exception, you can quickly view it by inspecting the exception’s __mro__ attribute. For
example:

>>> FileNotFoundError.__mro__
(<class 'FileNotFoundError'>, <class 'OSError'>, <class 'Exception'>,
 <class 'BaseException'>, <class 'object'>)
>>>

Any one of the listed classes up to BaseException can be used with the except statement.

14.7. Catching All Exceptions
Problem
You want to write code that catches all exceptions.

Solution
To catch all exceptions, write an exception handler for Exception, as shown here:

try:
   ...
except Exception as e:
   ...
   log('Reason:', e)       # Important!

This will catch all exceptions save SystemExit, KeyboardInterrupt, and GeneratorEx
it. If you also want to catch those exceptions, change Exception to BaseException.

Discussion
Catching all exceptions is sometimes used as a crutch by programmers who can’t re‐
member all of the possible exceptions that might occur in complicated operations. As
such, it is also a very good way to write undebuggable code if you are not careful.
Because of this, if you choose to catch all exceptions, it is absolutely critical to log or
report the actual reason for the exception somewhere (e.g., log file, error message print‐
ed to screen, etc.). If you don’t do this, your head will likely explode at some point.
Consider this example:

def parse_int(s):
    try:
        n = int(v)
    except Exception:
        print("Couldn't parse")

If you try this function, it behaves like this:

>>> parse_int('n/a')
Couldn't parse
>>> parse_int('42')
Couldn't parse
>>>

At this point, you might be left scratching your head as to why it doesn’t work. Now
suppose the function had been written like this:

def parse_int(s):
    try:
        n = int(v)
    except Exception as e:
        print("Couldn't parse")
        print('Reason:', e)

In this case, you get the following output, which indicates that a programming mistake
has been made:

>>> parse_int('42')
Couldn't parse
Reason: global name 'v' is not defined
>>>

14.7. Catching All Exceptions 

All things being equal, it’s probably better to be as precise as possible in your exception
handling. However, if you must catch all exceptions, just make sure you give good di‐
agnostic information or propagate the exception so that cause doesn’t get lost.
14.8. Creating Custom Exceptions
Problem
You’re building an application and would like to wrap lower-level exceptions with cus‐
tom ones that have more meaning in the context of your application.

Solution
Creating new exceptions is easy—just define them as classes that inherit from Excep
tion (or one of the other existing exception types if it makes more sense). For example,
if you are writing code related to network programming, you might define some custom
exceptions like this:

class NetworkError(Exception):
    pass

class HostnameError(NetworkError):
    pass

class TimeoutError(NetworkError):
    pass

class ProtocolError(NetworkError):
    pass

Users could then use these exceptions in the normal way. For example:

try:
    msg = s.recv()
except TimeoutError as e:
    ...
except ProtocolError as e:
    ...

Discussion
Custom exception classes should almost always inherit from the built-in Exception
class, or inherit from some locally defined base exception that itself inherits from Ex
ception. Although all exceptions also derive from BaseException, you should not use
this as a base class for new exceptions. BaseException is reserved for system-exiting
exceptions,  such  as  KeyboardInterrupt  or  SystemExit,  and  other  exceptions  that
should signal the application to exit. Therefore, catching these exceptions is not the

intended use case. Assuming you follow this convention, it follows that inheriting from
BaseException causes your custom exceptions to not be caught and to signal an im‐
minent application shutdown! 
Having custom exceptions in your application and using them as shown makes your
application code tell a more coherent story to whoever may need to read the code. One
design consideration involves the grouping of custom exceptions via inheritance. In
complicated applications, it may make sense to introduce further base classes that group
different classes of exceptions together. This gives the user a choice of catching a nar‐
rowly specified error, such as this:

try:
    s.send(msg)
except ProtocolError:
    ...

It also gives the ability to catch a broad range of errors, such as the following:

try:
    s.send(msg)
except NetworkError:
    ...

If you are going to define a new exception that overrides the __init__() method of
Exception, make sure you always call Exception.__init__() with all of the passed
arguments. For example:

class CustomError(Exception):
    def __init__(self, message, status):
        super().__init__(message, status)
        self.message = message
        self.status = status

This might look a little weird, but the default behavior of Exception is to accept all
arguments passed and to store them in the .args attribute as a tuple. Various other
libraries and parts of Python expect all exceptions to have the .args attribute, so if you
skip this step, you might find that your new exception doesn’t behave quite right in
certain contexts. To illustrate the use of .args, consider this interactive session with the
built-in RuntimeError exception, and notice how any number of arguments can be used
with the raise statement:

>>> try:
...     raise RuntimeError('It failed')
... except RuntimeError as e:
...     print(e.args)
...
('It failed',)
>>> try:
...     raise RuntimeError('It failed', 42, 'spam')
... except RuntimeError as e:

14.8. Creating Custom Exceptions 

...     print(e.args)
...
('It failed', 42, 'spam')
>>>

For more information on creating your own exceptions, see the Python documentation.
14.9. Raising an Exception in Response to Another
Exception
Problem
You want to raise an exception in response to catching a different exception, but want
to include information about both exceptions in the traceback.

Solution
To chain exceptions, use the raise from statement instead of a simple raise statement.
This will give you information about both errors. For example:

>>> def example():
...     try:
...             int('N/A')
...     except ValueError as e:
...             raise RuntimeError('A parsing error occurred') from e...
>>> 
example()
Traceback (most recent call last):
  File "<stdin>", line 3, in example
ValueError: invalid literal for int() with base 10: 'N/A'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 5, in example
RuntimeError: A parsing error occurred
>>>

As you can see in the traceback, both exceptions are captured. To catch such an excep‐
tion, you would use a normal except statement. However, you can look at the __cause__
attribute of the exception object to follow the exception chain should you wish. For
example:
try:
    example()
except RuntimeError as e:
    print("It didn't work:", e)

    if e.__cause__:
        print('Cause:', e.__cause__)

An implicit form of chained exceptions occurs when another exception gets raised in‐
side an except block. For example:

>>> def example2():
...     try:
...             int('N/A')
...     except ValueError as e:
...             print("Couldn't parse:", err)
...
>>>
>>> example2()
Traceback (most recent call last):
  File "<stdin>", line 3, in example2
ValueError: invalid literal for int() with base 10: 'N/A'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 5, in example2
NameError: global name 'err' is not defined
>>>

In this example, you get information about both exceptions, but the interpretation is a
bit different. In this case, the NameError exception is raised as the result of a program‐
ming error, not in direct response to the parsing error. For this case, the __cause__
attribute of an exception is not set. Instead, a __context__ attribute is set to the prior
exception.
If, for some reason, you want to suppress chaining, use raise from None:

>>> def example3():
...     try:
...             int('N/A')
...     except ValueError:
...             raise RuntimeError('A parsing error occurred') from None...
>>> 
example3()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 5, in example3
RuntimeError: A parsing error occurred
>>>

14.9. Raising an Exception in Response to Another Exception 

Discussion
In designing code, you should give careful attention to use of the raise statement inside
of  other  except  blocks.  In  most  cases,  such  raise  statements  should  probably  be
changed to raise from statements. That is, you should prefer this style:

try:
   ...
except SomeException as e:
   raise DifferentException() from e

The reason for doing this is that you are explicitly chaining the causes together. That is,
the  DifferentException  is  being  raised  in  direct  response  to  getting  a  SomeExcep
tion. This relationship will be explicitly stated in the resulting traceback.
If you write your code in the following style, you still get a chained exception, but it’s
often not clear if the exception chain was intentional or the result of an unforeseen
programming error:

try:
   ...
except SomeException:
   raise DifferentException()

When you use raise from, you’re making it clear that you meant to raise the second
exception.
Resist the urge to suppress exception information, as shown in the last example. Al‐
though suppressing exception information can lead to smaller tracebacks, it also dis‐
cards information that might be useful for debugging. All things being equal, it’s often
best to keep as much information as possible.
14.10. Reraising the Last Exception
Problem
You caught an exception in an except block, but now you want to reraise it.

Solution
Simply use the raise statement all by itself. For example:

>>> def example():
...     try:
...             int('N/A')
...     except ValueError:
...             print("Didn't work")
...             raise
...

>>> example()
Didn't work
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 3, in example
ValueError: invalid literal for int() with base 10: 'N/A'
>>>

Discussion
This problem typically arises when you need to take some kind of action in response to
an exception (e.g., logging, cleanup, etc.), but afterward, you simply want to propagate
the exception along. A very common use might be in catch-all exception handlers:

try:
   ...
except Exception as e:
   # Process exception information in some way
   ...

   # Propagate the exception
   raise

14.11. Issuing Warning Messages
Problem
You want to have your program issue warning messages (e.g., about deprecated features
or usage problems).

Solution
To have your program issue a warning message, use the warnings.warn() function. For
example:

import warnings

def func(x, y, logfile=None, debug=False):
    if logfile is not None:
         warnings.warn('logfile argument deprecated', DeprecationWarning)
    ...

The arguments to warn() are a warning message along with a warning class, which is
typically  one  of  the  following:  UserWarning,  DeprecationWarning,  SyntaxWarning,
RuntimeWarning, ResourceWarning, or FutureWarning.
The handling of warnings depends on how you have executed the interpreter and other
configuration. For example, if you run Python with the -W all option, you’ll get output
such as the following:

14.11. Issuing Warning Messages 

    bash % python3 -W all example.py
    example.py:5: DeprecationWarning: logfile argument is deprecated
      warnings.warn('logfile argument is deprecated', DeprecationWarning)

Normally, warnings just produce output messages on standard error. If you want to turn
warnings into exceptions, use the -W error option:

    bash % python3 -W error example.py
    Traceback (most recent call last):
      File "example.py", line 10, in <module>
        func(2, 3, logfile='log.txt')
      File "example.py", line 5, in func
        warnings.warn('logfile argument is deprecated', DeprecationWarning)
    DeprecationWarning: logfile argument is deprecated
    bash %

Discussion
Issuing a warning message is often a useful technique for maintaining software and
assisting users with issues that don’t necessarily rise to the level of being a full-fledged
exception. For example, if you’re going to change the behavior of a library or framework,
you can start issuing warning messages for the parts that you’re going to change while
still providing backward compatibility for a time. You can also warn users about prob‐
lematic usage issues in their code.
As another example of a warning in the built-in library, here is an example of a warning
message generated by destroying a file without closing it:

>>> import warnings
>>> warnings.simplefilter('always')
>>> f = open('/etc/passwd')
>>> del f
__main__:1: ResourceWarning: unclosed file <_io.TextIOWrapper name='/etc/passwd'
 mode='r' encoding='UTF-8'>
>>>

By default, not all warning messages appear. The -W option to Python can control the
output  of  warning  messages.  -W  all  will  output  all  warning  messages,  -W  ignore
ignores all warnings, and -W error turns warnings into exceptions. As an alternative,
you  can  can  use  the  warnings.simplefilter()  function  to  control  output,  as  just
shown. An argument of always makes all warning messages appear, ignore ignores all
warnings, and error turns warnings into exceptions.
For simple cases, this is all you really need to issue warning messages. The warnings
module provides a variety of more advanced configuration options related to the fil‐
tering and handling of warning messages. See the Python documentation for more 
information.

14.12. Debugging Basic Program Crashes
Problem
Your program is broken and you’d like some simple strategies for debugging it.

Solution
If your program is crashing with an exception, running your program as python3 -i
someprogram.py can be a useful tool for simply looking around. The -i option starts
an interactive shell as soon as a program terminates. From there, you can explore the
environment. For example, suppose you have this code:

# sample.py

def func(n):
    return n + 10

func('Hello')

Running python3 -i produces the following:

bash % python3 -i sample.py
Traceback (most recent call last):
  File "sample.py", line 6, in <module>
    func('Hello')
  File "sample.py", line 4, in func
    return n + 10
TypeError: Can't convert 'int' object to str implicitly
>>> func(10)
20
>>>

If you don’t see anything obvious, a further step is to launch the Python debugger after
a crash. For example:

>>> import pdb
>>> pdb.pm()
> sample.py(4)func()
-> return n + 10
(Pdb) w
  sample.py(6)<module>()
-> func('Hello')
> sample.py(4)func()
-> return n + 10
(Pdb) print n
'Hello'
(Pdb) q
>>>

14.12. Debugging Basic Program Crashes 

If your code is deeply buried in an environment where it is difficult to obtain an inter‐
active shell (e.g., in a server), you can often catch errors and produce tracebacks yourself.
For example:

import traceback
import sys

try:
    func(arg)
except:
    print('**** AN ERROR OCCURRED ****')
    traceback.print_exc(file=sys.stderr)

If your program isn’t crashing, but it’s producing wrong answers or you’re mystified by
how it works, there is often nothing wrong with just injecting a few print() calls in
places of interest. However, if you’re going to do that, there are a few related techniques
of interest. First, the traceback.print_stack() function will create a stack track of
your program immediately at that point. For example:

>>> def sample(n):
...     if n > 0:
...             sample(n-1)
...     else:
...             traceback.print_stack(file=sys.stderr)
...
>>> sample(5)
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 3, in sample
  File "<stdin>", line 3, in sample
  File "<stdin>", line 3, in sample
  File "<stdin>", line 3, in sample
  File "<stdin>", line 3, in sample
  File "<stdin>", line 5, in sample
>>>

Alternatively, you can also manually launch the debugger at any point in your program
using pdb.set_trace() like this:

import pdb

def func(arg):
    ...
    pdb.set_trace()
    ...

This can be a useful technique for poking around in the internals of a large program
and answering questions about the control flow or arguments to functions. For instance,
once the debugger starts, you can inspect variables using print or type a command such
as w to get the stack traceback.

Discussion
Don’t make debugging more complicated than it needs to be. Simple errors can often
be resolved by merely knowing how to read program tracebacks (e.g., the actual error
is usually the last line of the traceback). Inserting a few selected print() functions in
your code can also work well if you’re in the process of developing it and you simply
want some diagnostics (just remember to remove the statements later).
A common use of the debugger is to inspect variables inside a function that has crashed.
Knowing how to enter the debugger after such a crash has occurred is a useful skill to
know.
Inserting statements such as pdb.set_trace() can be useful if you’re trying to unravel
an extremely complicated program where the underlying control flow isn’t obvious.
Essentially, the program will run until it hits the set_trace() call, at which point it will
immediately enter the debugger. From there, you can try to make more sense of it. 
If you’re using an IDE for Python development, the IDE will typically provide its own
debugging interface on top of or in place of pdb. Consult the manual for your IDE for
more information.
14.13. Profiling and Timing Your Program
Problem
You  would  like  to  find  out  where  your  program  spends  its  time  and  make  timing
measurements.

Solution
If you simply want to time your whole program, it’s usually easy enough to use something
like the Unix time command. For example:

bash % time python3 someprogram.py
real 0m13.937s
user 0m12.162s
sys  0m0.098s
bash %

On the other extreme, if you want a detailed report showing what your program is doing,
you can use the cProfile module:

14.13. Profiling and Timing Your Program 

bash % python3 -m cProfile someprogram.py
         859647 function calls in 16.016 CPU seconds

   Ordered by: standard name

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
   263169    0.080    0.000    0.080    0.000 someprogram.py:16(frange)
      513    0.001    0.000    0.002    0.000 someprogram.py:30(generate_mandel)
   262656    0.194    0.000   15.295    0.000 someprogram.py:32(<genexpr>)
        1    0.036    0.036   16.077   16.077 someprogram.py:4(<module>)
   262144   15.021    0.000   15.021    0.000 someprogram.py:4(in_mandelbrot)
        1    0.000    0.000    0.000    0.000 os.py:746(urandom)
        1    0.000    0.000    0.000    0.000 png.py:1056(_readable)
        1    0.000    0.000    0.000    0.000 png.py:1073(Reader)
        1    0.227    0.227    0.438    0.438 png.py:163(<module>)
      512    0.010    0.000    0.010    0.000 png.py:200(group)
    ...
bash %

More often than not, profiling your code lies somewhere in between these two extremes.
For example, you may already know that your code spends most of its time in a few
selected functions. For selected profiling of functions, a short decorator can be useful.
For example:

# timethis.py

import time
from functools import wraps

def timethis(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        r = func(*args, **kwargs)
        end = time.perf_counter()
        print('{}.{} : {}'.format(func.__module__, func.__name__, end - start))
        return r
    return wrapper

To use this decorator, you simply place it in front of a function definition to get timings
from it. For example:

>>> @timethis
... def countdown(n):
...     while n > 0:
...             n -= 1
...
>>> countdown(10000000)
__main__.countdown : 0.803001880645752
>>>

To time a block of statements, you can define a context manager. For example:

from contextlib import contextmanager

@contextmanager
def timeblock(label):
    start = time.perf_counter()
    try:
        yield
    finally:
        end = time.perf_counter()
        print('{} : {}'.format(label, end - start))
Here is an example of how the context manager works:

>>> with timeblock('counting'):
...     n = 10000000
...     while n > 0:
...             n -= 1
...
counting : 1.5551159381866455
>>>

For studying the performance of small code fragments, the timeit module can be useful.
For example:

>>> from timeit import timeit
>>> timeit('math.sqrt(2)', 'import math')
0.1432319980012835
>>> timeit('sqrt(2)', 'from math import sqrt')
0.10836604500218527
>>>

timeit works by executing the statement specified in the first argument a million times
and measuring the time. The second argument is a setup string that is executed to set
up the environment prior to running the test. If you need to change the number of
iterations, supply a number argument like this:

>>> timeit('math.sqrt(2)', 'import math', number=10000000)
1.434852126003534
>>> timeit('sqrt(2)', 'from math import sqrt', number=10000000)
1.0270336690009572
>>>

Discussion
When making performance measurements, be aware that any results you get are ap‐
proximations. The time.perf_counter() function used in the solution provides the
highest-resolution timer possible on a given platform. However, it still measures wall-
clock time, and can be impacted by many different factors, such as machine load.
If  you  are  interested  in  process  time  as  opposed  to  wall-clock  time,  use  time.pro
cess_time() instead. For example:

14.13. Profiling and Timing Your Program 

from functools import wraps
def timethis(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.process_time()
        r = func(*args, **kwargs)
        end = time.process_time()
        print('{}.{} : {}'.format(func.__module__, func.__name__, end - start))
        return r
    return wrapper

Last, but not least, if you’re going to perform detailed timing analysis, make sure to read
the documentation for the time, timeit, and other associated modules, so that you have
an understanding of important platform-related differences and other pitfalls.
See Recipe 13.13 for a related recipe on creating a stopwatch timer class.
14.14. Making Your Programs Run Faster
Problem
Your program runs too slow and you’d like to speed it up without the assistance of more
extreme solutions, such as C extensions or a just-in-time (JIT) compiler.

Solution
While the first rule of optimization might be to “not do it,” the second rule is almost
certainly “don’t optimize the unimportant.” To that end, if your program is running slow,
you might start by profiling your code as discussed in Recipe 14.13.
More often than not, you’ll find that your program spends its time in a few hotspots,
such as inner data processing loops. Once you’ve identified those locations, you can use
the no-nonsense techniques presented in the following sections to make your program
run faster.

Use functions
A lot of programmers start using Python as a language for writing simple scripts. When
writing scripts, it is easy to fall into a practice of simply writing code with very little
structure. For example:

# somescript.py

import sys
import csv

with open(sys.argv[1]) as f:
     for row in csv.reader(f):

         # Some kind of processing
         ...

A little-known fact is that code defined in the global scope like this runs slower than
code defined in a function. The speed difference has to do with the implementation of
local versus global variables (operations involving locals are faster). So, if you want to
make the program run faster, simply put the scripting statements in a function:

# somescript.py
import sys
import csv

def main(filename):
    with open(filename) as f:
         for row in csv.reader(f):
             # Some kind of processing
             ...

main(sys.argv[1])

The speed difference depends heavily on the processing being performed, but in our
experience, speedups of 15-30% are not uncommon.

Selectively eliminate attribute access
Every use of the dot (.) operator to access attributes comes with a cost. Under the covers,
this triggers special methods, such as __getattribute__() and __getattr__(), which
often lead to dictionary lookups.
You can often avoid attribute lookups by using the from module import name form of
import as well as making selected use of bound methods. To illustrate, consider the
following code fragment:

import math

def compute_roots(nums):
    result = []
    for n in nums:
        result.append(math.sqrt(n))
    return result

# Test
nums = range(1000000)
for n in range(100):
    r = compute_roots(nums)

When tested on our machine, this program runs in about 40 seconds. Now change the
compute_roots() function as follows:

from math import sqrt

def compute_roots(nums):

14.14. Making Your Programs Run Faster 

    result = []
    result_append = result.append
    for n in nums:
        result_append(sqrt(n))
    return result

This version runs in about 29 seconds. The only difference between the two versions of
code is the elimination of attribute access. Instead of using math.sqrt(), the code uses
sqrt(). The result.append() method is additionally placed into a local variable re
sult_append and reused in the inner loop.
However, it must be emphasized that these changes only make sense in frequently ex‐
ecuted code, such as loops. So, this optimization really only makes sense in carefully 
selected places.

Understand locality of variables
As previously noted, local variables are faster than global variables. For frequently ac‐
cessed names, speedups can be obtained by making those names as local as possible.
For  example,  consider  this  modified  version  of  the  compute_roots()  function  just
discussed:

import math

def compute_roots(nums):
    sqrt = math.sqrt
    result = []
    result_append = result.append
    for n in nums:
        result_append(sqrt(n))
    return result

In this version,  sqrt has been lifted from the  math module and placed into a local
variable. If you run this code, it now runs in about 25 seconds (an improvement over
the previous version, which took 29 seconds). That additional speedup is due to a local
lookup of sqrt being a bit faster than a global lookup of sqrt.
Locality arguments also apply when working in classes. In general, looking up a value
such as self.name will be considerably slower than accessing a local variable. In inner
loops, it might pay to lift commonly accessed attributes into a local variable. For example:

# Slower
class SomeClass:
    ...
    def method(self):
         for x in s:
             op(self.value)

# Faster
class SomeClass:

    ...
    def method(self):
         value = self.value
         for x in s:
             op(value)

Avoid gratuitous abstraction
Any time you wrap up code with extra layers of processing, such as decorators, prop‐
erties, or descriptors, you’re going to make it slower. As an example, consider this class:

class A:
    def __init__(self, x, y):
        self.x = x
        self.y = y
    @property
    def y(self):
        return self._y
    @y.setter
    def y(self, value):
        self._y = value

Now, try a simple timing test:

>>> from timeit import timeit
>>> a = A(1,2)
>>> timeit('a.x', 'from __main__ import a')
0.07817923510447145
>>> timeit('a.y', 'from __main__ import a')
0.35766440676525235
>>>

As you can observe, accessing the property y is not just slightly slower than a simple
attribute x, it’s about 4.5 times slower. If this difference matters, you should ask yourself
if the definition of y as a property was really necessary. If not, simply get rid of it and
go back to using a simple attribute instead. Just because it might be common for pro‐
grams in another programming language to use getter/setter functions, that doesn’t
mean you should adopt that programming style for Python.

Use the built-in containers
Built-in data types such as strings, tuples, lists, sets, and dicts are all implemented in C,
and are rather fast. If you’re inclined to make your own data structures as a replacement
(e.g., linked lists, balanced trees, etc.), it may be rather difficult if not impossible to match
the speed of the built-ins. Thus, you’re often better off just using them.

Avoid making unnecessary data structures or copies
Sometimes programmers get carried away with making unnecessary data structures
when they just don’t have to. For example, someone might write code like this:

14.14. Making Your Programs Run Faster 

values = [x for x in sequence]
squares = [x*x for x in values]

Perhaps the thinking here is to first collect a bunch of values into a list and then to start
applying operations such as list comprehensions to it. However, the first list is com‐
pletely unnecessary. Simply write the code like this:

squares = [x*x for x in sequence]

Related to this, be on the lookout for code written by programmers who are overly
paranoid about Python’s sharing of values. Overuse of functions such as copy.deep
copy() may be a sign of code that’s been written by someone who doesn’t fully under‐
stand or trust Python’s memory model. In such code, it may be safe to eliminate many
of the copies.

Discussion
Before optimizing, it’s usually worthwhile to study the algorithms that you’re using first.
You’ll get a much bigger speedup by switching to an O(n log n) algorithm than by
trying to tweak the implementation of an an O(n**2) algorithm.
If you’ve decided that you still must optimize, it pays to consider the big picture. As a
general rule, you don’t want to apply optimizations to every part of your program,
because such changes are going to make the code hard to read and understand. Instead,
focus only on known performance bottlenecks, such as inner loops.
You need to be especially wary interpreting the results of micro-optimizations. For
example, consider these two techniques for creating a dictionary:

a = {
    'name' : 'AAPL',
    'shares' : 100,
    'price' : 534.22
}

b = dict(name='AAPL', shares=100, price=534.22)

The latter choice has the benefit of less typing (you don’t need to quote the key names).
However, if you put the two code fragments in a head-to-head performance battle, you’ll
find that using  dict() runs three times slower! With this knowledge, you might be
inclined to scan your code and replace every use of dict() with its more verbose al‐
ternative. However, a smart programmer will only focus on parts of a program where
it might actually matter, such as an inner loop. In other places, the speed difference just
isn’t going to matter at all.
If, on the other hand, your performance needs go far beyond the simple techniques in
this recipe, you might investigate the use of tools based on just-in-time (JIT) compilation
techniques. For example, the PyPy project is an alternate implementation of the Python

interpreter that analyzes the execution of your program and generates native machine
code for frequently executed parts. It can sometimes make Python programs run an
order of magnitude faster, often approaching (or even exceeding) the speed of code
written in C. Unfortunately, as of this writing, PyPy does not yet fully support Python
3. So, that is something to look for in the future. You might also consider the Numba
project. Numba is a dynamic compiler where you annotate selected Python functions
that you want to optimize with a decorator. Those functions are then compiled into
native machine code through the use of LLVM. It too can produce signficant perfor‐
mance gains. However, like PyPy, support for Python 3 should be viewed as somewhat
experimental.
Last, but not least, the words of John Ousterhout come to mind: “The best performance
improvement is the transition from the nonworking to the working state.” Don’t worry
about optimization until you need to. Making sure your program works correctly is
usually more important than making it run fast (at least initially).

14.14. Making Your Programs Run Faster 


CHAPTER 15
C Extensions

This chapter looks at the problem of accessing C code from Python. Many of Python’s
built-in libraries are written in C, and accessing C is an important part of making Python
talk to existing libraries. It’s also an area that might require the most study if you’re faced
with the problem of porting extension code from Python 2 to 3.
Although Python provides an extensive C programming API, there are actually many
different approaches for dealing with C. Rather than trying to give an exhaustive ref‐
erence for every possible tool or technique, the approach is to focus on a small fragment
of C code along with some representative examples of how to work with the code. The
goal is to provide a series of programming templates that experienced programmers
can expand upon for their own use.
Here is the C code we will work with in most of the recipes:

/* sample.c */_method
#include <math.h>

/* Compute the greatest common divisor */
int gcd(int x, int y) {
    int g = y;
    while (x > 0) {
        g = x;
        x = y % x;
        y = g;
    }
    return g;
}

/* Test if (x0,y0) is in the Mandelbrot set or not */
int in_mandel(double x0, double y0, int n) {
  double x=0,y=0,xtemp;
  while (n > 0) {
    xtemp = x*x - y*y + x0;
    y = 2*x*y + y0;

    x = xtemp;
    n -= 1;
    if (x*x + y*y > 4) return 0;
  }
  return 1;
}

/* Divide two numbers */
int divide(int a, int b, int *remainder) {
  int quot = a / b;
  *remainder = a % b;
  return quot;
}

/* Average values in an array */
double avg(double *a, int n) {
  int i;
  double total = 0.0;
  for (i = 0; i < n; i++) {
    total += a[i];
  }
  return total / n;
}

/* A C data structure */
typedef struct Point {
    double x,y;
} Point;

/* Function involving a C data structure */
double distance(Point *p1, Point *p2) {
   return hypot(p1->x - p2->x, p1->y - p2->y);
}

This code contains a number of different C programming features. First, there are a few
simple functions such as gcd() and is_mandel(). The divide() function is an example
of a C function returning multiple values, one through a pointer argument. The avg()
function performs a data reduction across a C array. The Point and distance() function
involve C structures.
For all of the recipes that follow, assume that the preceding code is found in a file named
sample.c, that definitions are found in a file named sample.h and that it has been com‐
piled into a library libsample that can be linked to other C code. The exact details of
compilation and linking vary from system to system, but that is not the primary focus.
It is assumed that if you’re working with C code, you’ve already figured that out.

15.1. Accessing C Code Using ctypes
Problem
You have a small number of C functions that have been compiled into a shared library
or DLL. You would like to call these functions purely from Python without having to
write additional C code or using a third-party extension tool.

Solution
For small problems involving C code, it is often easy enough to use the ctypes module
that is part of Python’s standard library. In order to use ctypes, you must first make
sure the C code you want to access has been compiled into a shared library that is
compatible with the Python interpreter (e.g., same architecture, word size, compiler,
etc.). For the purposes of this recipe, assume that a shared library, libsample.so, has
been created and that it contains nothing more than the code shown in the chapter
introduction. Further assume that the libsample.so file has been placed in the same
directory as the sample.py file shown next.
To access the resulting library, you make a Python module that wraps around it, such
as the following:
# sample.py
import ctypes
import os

# Try to locate the .so file in the same directory as this file
_file = 'libsample.so'
_path = os.path.join(*(os.path.split(__file__)[:-1] + (_file,)))
_mod = ctypes.cdll.LoadLibrary(_path)

# int gcd(int, int)
gcd = _mod.gcd
gcd.argtypes = (ctypes.c_int, ctypes.c_int)
gcd.restype = ctypes.c_int

# int in_mandel(double, double, int)
in_mandel = _mod.in_mandel
in_mandel.argtypes = (ctypes.c_double, ctypes.c_double, ctypes.c_int)
in_mandel.restype = ctypes.c_int

# int divide(int, int, int *)
_divide = _mod.divide
_divide.argtypes = (ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int))
_divide.restype = ctypes.c_int

def divide(x, y):
    rem = ctypes.c_int()
    quot = _divide(x, y, rem)

15.1. Accessing C Code Using ctypes 

    return quot,rem.value

# void avg(double *, int n)
# Define a special type for the 'double *' argument
class DoubleArrayType:
    def from_param(self, param):
        typename = type(param).__name__
        if hasattr(self, 'from_' + typename):
            return getattr(self, 'from_' + typename)(param)
        elif isinstance(param, ctypes.Array):
            return param
        else:
            raise TypeError("Can't convert %s" % typename)

    # Cast from array.array objects
    def from_array(self, param):
        if param.typecode != 'd':
            raise TypeError('must be an array of doubles')
        ptr, _ = param.buffer_info()
        return ctypes.cast(ptr, ctypes.POINTER(ctypes.c_double))

    # Cast from lists/tuples
    def from_list(self, param):
        val = ((ctypes.c_double)*len(param))(*param)
        return val

    from_tuple = from_list

    # Cast from a numpy array
    def from_ndarray(self, param):
        return param.ctypes.data_as(ctypes.POINTER(ctypes.c_double))

DoubleArray = DoubleArrayType()
_avg = _mod.avg
_avg.argtypes = (DoubleArray, ctypes.c_int)
_avg.restype = ctypes.c_double

def avg(values):
    return _avg(values, len(values))

# struct Point { }
class Point(ctypes.Structure):
    _fields_ = [('x', ctypes.c_double),
                ('y', ctypes.c_double)]

# double distance(Point *, Point *)
distance = _mod.distance
distance.argtypes = (ctypes.POINTER(Point), ctypes.POINTER(Point))
distance.restype = ctypes.c_double

If all goes well, you should be able to load the module and use the resulting C functions.
For example:

>>> import sample
>>> sample.gcd(35,42)
7
>>> sample.in_mandel(0,0,500)
1
>>> sample.in_mandel(2.0,1.0,500)
0
>>> sample.divide(42,8)
(5, 2)
>>> sample.avg([1,2,3])
2.0
>>> p1 = sample.Point(1,2)
>>> p2 = sample.Point(4,5)
>>> sample.distance(p1,p2)
4.242640687119285
>>>

Discussion
There are several aspects of this recipe that warrant some discussion. The first issue
concerns the overall packaging of C and Python code together. If you are using ctypes
to access C code that you have compiled yourself, you will need to make sure that the
shared library gets placed in a location where the sample.py module can find it. One
possibility is to put the resulting .so file in the same directory as the supporting Python
code. This is what’s shown at the first part of this recipe—sample.py looks at the __file__
variable to see where it has been installed, and then constructs a path that points to a
libsample.so file in the same directory.
If the C library is going to be installed elsewhere, then you’ll have to adjust the path
accordingly. If the C library is installed as a standard library on your machine, you might
be able to use the ctypes.util.find_library() function. For example:

>>> from ctypes.util import find_library
>>> find_library('m')
'/usr/lib/libm.dylib'
>>> find_library('pthread')
'/usr/lib/libpthread.dylib'
>>> find_library('sample')
'/usr/local/lib/libsample.so'
>>>

Again, ctypes won’t work at all if it can’t locate the library with the C code. Thus, you’ll
need to spend a few minutes thinking about how you want to install things.
Once you know where the C library is located, you use ctypes.cdll.LoadLibrary()
to load it. The following statement in the solution does this where  _path is the full
pathname to the shared library:

_mod = ctypes.cdll.LoadLibrary(_path)

15.1. Accessing C Code Using ctypes 

Once a library has been loaded, you need to write statements that extract specific sym‐
bols and put type signatures on them. This is what’s happening in code fragments such
as this:

# int in_mandel(double, double, int)
in_mandel = _mod.in_mandel
in_mandel.argtypes = (ctypes.c_double, ctypes.c_double, ctypes.c_int)
in_mandel.restype = ctypes.c_int

In this code, the .argtypes attribute is a tuple containing the input arguments to a
function, and .restype is the return type. ctypes defines a variety of type objects (e.g.,
c_double, c_int, c_short, c_float, etc.) that represent common C data types. Attach‐
ing the type signatures is critical if you want to make Python pass the right kinds of
arguments and convert data correctly (if you don’t do this, not only will the code not
work, but you might cause the entire interpreter process to crash).
A somewhat tricky part of using ctypes is that the original C code may use idioms that
don’t map cleanly to Python. The divide() function is a good example because it returns
a value through one of its arguments. Although that’s a common C technique, it’s often
not clear how it’s supposed to work in Python. For example, you can’t do anything
straightforward like this:

>>> divide = _mod.divide
>>> divide.argtypes = (ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int))
>>> x = 0
>>> divide(10, 3, x)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ctypes.ArgumentError: argument 3: <class 'TypeError'>: expected LP_c_int
instance instead of int
>>>

Even if this did work, it would violate Python’s immutability of integers and probably
cause the entire interpreter to be sucked into a black hole. For arguments involving
pointers, you usually have to construct a compatible ctypes object and pass it in like
this:

>>> x = ctypes.c_int()
>>> divide(10, 3, x)
3
>>> x.value
1
>>>

Here an instance of a ctypes.c_int is created and passed in as the pointer object. Unlike
a normal Python integer, a c_int object can be mutated. The .value attribute can be
used to either retrieve or change the value as desired.

For cases where the C calling convention is “un-Pythonic,” it is common to write a small
wrapper function. In the solution, this code makes the divide() function return the
two results using a tuple instead:
# int divide(int, int, int *)
_divide = _mod.divide
_divide.argtypes = (ctypes.c_int, ctypes.c_int, ctypes.POINTER(ctypes.c_int))
_divide.restype = ctypes.c_int

def divide(x, y):
    rem = ctypes.c_int()
    quot = _divide(x,y,rem)
    return quot, rem.value

The avg() function presents a new kind of challenge. The underlying C code expects
to receive a pointer and a length representing an array. However, from the Python side,
we must consider the following questions: What is an array? Is it a list? A tuple? An
array from the array module? A numpy array? Is it all of these? In practice, a Python
“array” could take many different forms, and maybe you would like to support multiple
possibilities.
The DoubleArrayType class shows how to handle this situation. In this class, a single
method from_param() is defined. The role of this method is to take a single parameter
and narrow it down to a compatible ctypes object (a pointer to a ctypes.c_double, in
the example). Within from_param(), you are free to do anything that you wish. In the
solution, the typename of the parameter is extracted and used to dispatch to a more
specialized method. For example, if a list is passed, the typename is list and a method
from_list() is invoked.
For lists and tuples, the from_list() method performs a conversion to a ctypes array
object. This looks a little weird, but here is an interactive example of converting a list to
a ctypes array:

>>> nums = [1, 2, 3]
>>> a = (ctypes.c_double * len(nums))(*nums)
>>> a
<__main__.c_double_Array_3 object at 0x10069cd40>
>>> a[0]
1.0
>>> a[1]
2.0
>>> a[2]
3.0
>>>

For array objects, the from_array() method extracts the underlying memory pointer
and casts it to a ctypes pointer object. For example:

15.1. Accessing C Code Using ctypes 

>>> import array
>>> a = array.array('d',[1,2,3])
>>> a
array('d', [1.0, 2.0, 3.0])
>>> ptr_ = a.buffer_info()
>>> ptr
4298687200
>>> ctypes.cast(ptr, ctypes.POINTER(ctypes.c_double))
<__main__.LP_c_double object at 0x10069cd40>
>>>

The from_ndarray() shows comparable conversion code for numpy arrays.
By defining the DoubleArrayType class and using it in the type signature of avg(), as
shown, the function can accept a variety of different array-like inputs:

>>> import sample
>>> sample.avg([1,2,3])
2.0
>>> sample.avg((1,2,3))
2.0
>>> import array
>>> sample.avg(array.array('d',[1,2,3]))
2.0
>>> import numpy
>>> sample.avg(numpy.array([1.0,2.0,3.0]))
2.0
>>>

The last part of this recipe shows how to work with a simple C structure. For structures,
you simply define a class that contains the appropriate fields and types like this:

class Point(ctypes.Structure):
    _fields_ = [('x', ctypes.c_double),
                ('y', ctypes.c_double)]

Once defined, you can use the class in type signatures as well as in code that needs to
instantiate and work with the structures. For example:

>>> p1 = sample.Point(1,2)
>>> p2 = sample.Point(4,5)
>>> p1.x
1.0
>>> p1.y
2.0
>>> sample.distance(p1,p2)
4.242640687119285
>>>

A few final comments: ctypes is a useful library to know about if all you’re doing is
accessing a few C functions from Python. However, if you’re trying to access a large
library, you might want to look at alternative approaches, such as Swig (described in
Recipe 15.9) or Cython (described in Recipe 15.10).

The main problem with a large library is that since ctypes isn’t entirely automatic, you’ll
have to spend a fair bit of time writing out all of the type signatures, as shown in the
example. Depending on the complexity of the library, you might also have to write a
large number of small wrapper functions and supporting classes. Also, unless you fully
understand all of the low-level details of the C interface, including memory management
and error handling, it is often quite easy to make Python catastrophically crash with a
segmentation fault, access violation, or some similar error.
As an alternative to ctypes, you might also look at CFFI. CFFI provides much of the
same functionality, but uses C syntax and supports more advanced kinds of C code. As
of this writing, CFFI is still a relatively new project, but its use has been growing rapidly.
There has even been some discussion of including it in the Python standard library in
some future release. Thus, it’s definitely something to keep an eye on.
15.2. Writing a Simple C Extension Module
Problem
You want to write a simple C extension module directly using Python’s extension API
and no other tools.

Solution
For simple C code, it is straightforward to make a handcrafted extension module. As a
preliminary step, you probably want to make sure your C code has a proper header file.
For example,

/* sample.h */

#include <math.h>

extern int gcd(int, int);
extern int in_mandel(double x0, double y0, int n);
extern int divide(int a, int b, int *remainder);
extern double avg(double *a, int n);

typedef struct Point {
    double x,y;
} Point;

extern double distance(Point *p1, Point *p2);

15.2. Writing a Simple C Extension Module 

Typically, this header would correspond to a library that has been compiled separately.
With that assumption, here is a sample extension module that illustrates the basics of
writing extension functions:

#include "Python.h"
#include "sample.h"

/* int gcd(int, int) */
static PyObject *py_gcd(PyObject *self, PyObject *args) {
  int x, y, result;

  if (!PyArg_ParseTuple(args,"ii", &x, &y)) {
    return NULL;
  }
  result = gcd(x,y);
  return Py_BuildValue("i", result);
}

/* int in_mandel(double, double, int) */
static PyObject *py_in_mandel(PyObject *self, PyObject *args) {
  double x0, y0;
  int n;
  int result;

  if (!PyArg_ParseTuple(args, "ddi", &x0, &y0, &n)) {
    return NULL;
  }
  result = in_mandel(x0,y0,n);
  return Py_BuildValue("i", result);
}

/* int divide(int, int, int *) */
static PyObject *py_divide(PyObject *self, PyObject *args) {
  int a, b, quotient, remainder;
  if (!PyArg_ParseTuple(args, "ii", &a, &b)) {
    return NULL;
  }
  quotient = divide(a,b, &remainder);
  return Py_BuildValue("(ii)", quotient, remainder);
}

/* Module method table */
static PyMethodDef SampleMethods[] = {
  {"gcd",  py_gcd, METH_VARARGS, "Greatest common divisor"},
  {"in_mandel", py_in_mandel, METH_VARARGS, "Mandelbrot test"},
  {"divide", py_divide, METH_VARARGS, "Integer division"},
  { NULL, NULL, 0, NULL}
};

/* Module structure */
static struct PyModuleDef samplemodule = {
  PyModuleDef_HEAD_INIT,

  "sample",           /* name of module */
  "A sample module",  /* Doc string (may be NULL) */
  -1,                 /* Size of per-interpreter state or -1 */
  SampleMethods       /* Method table */
};

/* Module initialization function */
PyMODINIT_FUNC
PyInit_sample(void) {
  return PyModule_Create(&samplemodule);
}

For building the extension module, create a setup.py file that looks like this:

# setup.py
from distutils.core import setup, Extension

setup(name='sample',
      ext_modules=[
        Extension('sample',
                  ['pysample.c'],
                  include_dirs = ['/some/dir'],
                  define_macros = [('FOO','1')],
                  undef_macros = ['BAR'],
                  library_dirs = ['/usr/local/lib'],
                  libraries = ['sample']
                  )
        ]
)

Now, to build the resulting library, simply use python3 buildlib.py build_ext --
inplace. For example:

bash % python3 setup.py build_ext --inplace
running build_ext
building 'sample' extension
gcc -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes
 -I/usr/local/include/python3.3m -c pysample.c
 -o build/temp.macosx-10.6-x86_64-3.3/pysample.o
gcc -bundle -undefined dynamic_lookup
build/temp.macosx-10.6-x86_64-3.3/pysample.o \
 -L/usr/local/lib -lsample -o sample.so
bash %

As shown, this creates a shared library called sample.so. When compiled, you should
be able to start importing it as a module:

>>> import sample
>>> sample.gcd(35, 42)
7
>>> sample.in_mandel(0, 0, 500)
1
>>> sample.in_mandel(2.0, 1.0, 500)

15.2. Writing a Simple C Extension Module 

0
>>> sample.divide(42, 8)
(5, 2)
>>>

If you are attempting these steps on Windows, you may need to spend some time fiddling
with your environment and the build environment to get extension modules to build
correctly.  Binary  distributions  of  Python  are  typically  built  using  Microsoft  Visual
Studio. To get extensions to work, you may have to compile them using the same or
compatible tools. See the Python documentation.

Discussion
Before attempting any kind of handwritten extension, it is absolutely critical that you
consult Python’s documentation on “Extending and Embedding the Python Interpret‐
er”. Python’s C extension API is large, and repeating all of it here is simply not practical.
However, the most important parts can be easily discussed.
First, in extension modules, functions that you write are all typically written with a
common prototype such as this:

static PyObject *py_func(PyObject *self, PyObject *args) {
  ...
}

PyObject is the C data type that represents any Python object. At a very high level, an
extension function is a C function that receives a tuple of Python objects (in PyObject
*args) and returns a new Python object as a result. The self argument to the function
is unused for simple extension functions, but comes into play should you want to define
new classes or object types in C (e.g., if the extension function were a method of a class,
then self would hold the instance).
The PyArg_ParseTuple() function is used to convert values from Python to a C rep‐
resentation. As input, it takes a format string that indicates the required values, such as
“i” for integer and “d” for double, as well as the addresses of C variables in which to place
the converted results. PyArg_ParseTuple() performs a variety of checks on the number
and type of arguments. If there is any mismatch with the format string, an exception is
raised and NULL is returned. By checking for this and simply returning NULL, an ap‐
propriate exception will have been raised in the calling code.
The Py_BuildValue() function is used to create Python objects from C data types. It
also accepts a format code to indicate the desired type. In the extension functions, it is
used to return results back to Python. One feature of Py_BuildValue() is that it can
build more complicated kinds of objects, such as tuples and dictionaries. In the code
for py_divide(), an example showing the return of a tuple is shown. However, here are
a few more examples:

return Py_BuildValue("i", 34);      // Return an integer
return Py_BuildValue("d", 3.4);     // Return a double
return Py_BuildValue("s", "Hello"); // Null-terminated UTF-8 string
return Py_BuildValue("(ii)", 3, 4); // Tuple (3, 4)

Near the bottom of any extension module, you will find a function table such as the
SampleMethods table shown in this recipe. This table lists C functions, the names to use
in Python, as well as doc strings. All modules are required to specify such a table, as it
gets used in the initialization of the module.
The final function PyInit_sample() is the module initialization function that executes
when the module is first imported. The primary job of this function is to register the
module object with the interpreter.
As a final note, it must be stressed that there is considerably more to extending Python
with C functions than what is shown here (in fact, the C API contains well over 500
functions in it). You should view this recipe simply as a stepping stone for getting started.
To do more, start with the documentation on the PyArg_ParseTuple() and Py_Build
Value() functions, and expand from there.
15.3. Writing an Extension Function That Operates on
Arrays
Problem
You want to write a C extension function that operates on contiguous arrays of data, as
might be created by the array module or libraries like NumPy. However, you would like
your function to be general purpose and not specific to any one array library.

Solution
To receive and process arrays in a portable manner, you should write code that uses the
Buffer Protocol. Here is an example of a handwritten C extension function that receives
array data and calls the avg(double *buf, int len) function from this chapter’s in‐
troduction:

/* Call double avg(double *, int) */
static PyObject *py_avg(PyObject *self, PyObject *args) {
  PyObject *bufobj;
  Py_buffer view;
  double result;
  /* Get the passed Python object */
  if (!PyArg_ParseTuple(args, "O", &bufobj)) {
    return NULL;
  }

  /* Attempt to extract buffer information from it */

15.3. Writing an Extension Function That Operates on Arrays 

  if (PyObject_GetBuffer(bufobj, &view,
      PyBUF_ANY_CONTIGUOUS | PyBUF_FORMAT) == -1) {
    return NULL;
  }

  if (view.ndim != 1) {
    PyErr_SetString(PyExc_TypeError, "Expected a 1-dimensional array");
    PyBuffer_Release(&view);
    return NULL;
  }

  /* Check the type of items in the array */
  if (strcmp(view.format,"d") != 0) {
    PyErr_SetString(PyExc_TypeError, "Expected an array of doubles");
    PyBuffer_Release(&view);
    return NULL;
  }

  /* Pass the raw buffer and size to the C function */
  result = avg(view.buf, view.shape[0]);

  /* Indicate we're done working with the buffer */
  PyBuffer_Release(&view);
  return Py_BuildValue("d", result);
}

Here is an example that shows how this extension function works:

>>> import array
>>> avg(array.array('d',[1,2,3]))
2.0
>>> import numpy
>>> avg(numpy.array([1.0,2.0,3.0]))
2.0
>>> avg([1,2,3])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'list' does not support the buffer interface
>>> avg(b'Hello')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: Expected an array of doubles
>>> a = numpy.array([[1.,2.,3.],[4.,5.,6.]])
>>> avg(a[:,2])
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: ndarray is not contiguous
>>> sample.avg(a)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: Expected a 1-dimensional array
>>> sample.avg(a[0])

2.0
>>>

Discussion
Passing array objects to C functions might be one of the most common things you would
want to do with a extension function. A large number of Python applications, ranging
from image processing to scientific computing, are based on high-performance array
processing. By writing code that can accept and operate on arrays, you can write cus‐
tomized code that plays nicely with those applications as opposed to having some sort
of custom solution that only works with your own code.
The key to this code is the PyBuffer_GetBuffer() function. Given an arbitrary Python
object, it tries to obtain information about the underlying memory representation. If
it’s not possible, as is the case with most normal Python objects, it simply raises an
exception  and  returns  -1.  The  special  flags  passed  to  PyBuffer_GetBuffer()  give
additional  hints  about  the  kind  of  memory  buffer  that  is  requested.  For  example,
PyBUF_ANY_CONTIGUOUS specifies that a contiguous region of memory is required.
For arrays, byte strings, and other similar objects, a Py_buffer structure is filled with
information about the underlying memory. This includes a pointer to the memory, size,
itemsize, format, and other details. Here is the definition of this structure:

typedef struct bufferinfo {
    void *buf;              /* Pointer to buffer memory */
    PyObject *obj;          /* Python object that is the owner */
    Py_ssize_t len;         /* Total size in bytes */
    Py_ssize_t itemsize;    /* Size in bytes of a single item */
    int readonly;           /* Read-only access flag */
    int ndim;               /* Number of dimensions */
    char *format;           /* struct code of a single item */
    Py_ssize_t *shape;      /* Array containing dimensions */
    Py_ssize_t *strides;    /* Array containing strides */
    Py_ssize_t *suboffsets; /* Array containing suboffsets */
} Py_buffer;

In this recipe, we are simply concerned with receiving a contiguous array of doubles.
To check if items are a double, the format attribute is checked to see if the string is
"d". This is the same code that the struct module uses when encoding binary values.
As a general rule, format could be any format string that’s compatible with the struct
module and might include multiple items in the case of arrays containing C structures.
Once we have verified the underlying buffer information, we simply pass it to the C
function, which treats it as a normal C array. For all practical purposes, it is not con‐
cerned with what kind of array it is or what library created it. This is how the function
is able to work with arrays created by the array module or by numpy.

15.3. Writing an Extension Function That Operates on Arrays 

Before  returning  a  final  result,  the  underlying  buffer  view  must  be  released  using 
PyBuffer_Release(). This step is required to properly manage reference counts of
objects.
Again, this recipe only shows a tiny fragment of code that receives an array. If working
with arrays, you might run into issues with multidimensional data, strided data, different
data types, and more that will require study. Make sure you consult the official docu‐
mentation to get more details.
If you need to write many extensions involving array handling, you may find it easier
to implement the code in Cython. See Recipe 15.11. 
15.4. Managing Opaque Pointers in C Extension Modules
Problem
You have an extension module that needs to handle a pointer to a C data structure, but
you don’t want to expose any internal details of the structure to Python.

Solution
Opaque data structures are easily handled by wrapping them inside capsule objects.
Consider this fragment of C code from our sample code:

typedef struct Point {
    double x,y;
} Point;

extern double distance(Point *p1, Point *p2);

Here is an example of extension code that wraps the Point structure and distance()
function using capsules:

/* Destructor function for points */
static void del_Point(PyObject *obj) {
  free(PyCapsule_GetPointer(obj,"Point"));
}

/* Utility functions */
static Point *PyPoint_AsPoint(PyObject *obj) {
  return (Point *) PyCapsule_GetPointer(obj, "Point");
}

static PyObject *PyPoint_FromPoint(Point *p, int must_free) {
  return PyCapsule_New(p, "Point", must_free ? del_Point : NULL);
}

/* Create a new Point object */
static PyObject *py_Point(PyObject *self, PyObject *args) {

  Point *p;
  double x,y;
  if (!PyArg_ParseTuple(args,"dd",&x,&y)) {
    return NULL;
  }
  p = (Point *) malloc(sizeof(Point));
  p->x = x;
  p->y = y;
  return PyPoint_FromPoint(p, 1);
}

static PyObject *py_distance(PyObject *self, PyObject *args) {
  Point *p1, *p2;
  PyObject *py_p1, *py_p2;
  double result;

  if (!PyArg_ParseTuple(args,"OO",&py_p1, &py_p2)) {
    return NULL;
  }
  if (!(p1 = PyPoint_AsPoint(py_p1))) {
    return NULL;
  }
  if (!(p2 = PyPoint_AsPoint(py_p2))) {
    return NULL;
  }
  result = distance(p1,p2);
  return Py_BuildValue("d", result);
}

Using these functions from Python looks like this:

>>> import sample
>>> p1 = sample.Point(2,3)
>>> p2 = sample.Point(4,5)
>>> p1
<capsule object "Point" at 0x1004ea330>
>>> p2
<capsule object "Point" at 0x1005d1db0>
>>> sample.distance(p1,p2)
2.8284271247461903
>>>

Discussion
Capsules are similar to a typed C pointer. Internally, they hold a generic pointer along
with an identifying name and can be easily created using the PyCapsule_New() function.
In addition, an optional destructor function can be attached to a capsule to release the
underlying memory when the capsule object is garbage collected.

15.4. Managing Opaque Pointers in C Extension Modules 

To extract the pointer contained inside a capsule, use the  PyCapsule_GetPointer()
function and specify the name. If the supplied name doesn’t match that of the capsule
or some other error occurs, an exception is raised and NULL is returned.
In  this  recipe,  a  pair  of  utility  functions—PyPoint_FromPoint()  and  PyPoint_As
Point()—have been written to deal with the mechanics of creating and unwinding
Point instances from capsule objects. In any extension functions, we’ll use these func‐
tions instead of working with capsules directly. This design choice makes it easier to
deal with possible changes to the wrapping of Point objects in the future. For example,
if you decided to use something other than a capsule later, you would only have to change
these two functions.
One tricky part about capsules concerns garbage collection and memory management.
The  PyPoint_FromPoint()  function  accepts  a  must_free  argument  that  indicates
whether the underlying Point * structure is to be collected when the capsule is de‐
stroyed. When working with certain kinds of C code, ownership issues can be difficult
to handle (e.g., perhaps a Point structure is embedded within a larger data structure
that is managed separately). Rather than making a unilateral decision to garbage collect,
this extra argument gives control back to the programmer. It should be noted that the
destructor associated with an existing capsule can also be changed using the  PyCap
sule_SetDestructor() function.
Capsules are a sensible solution to interfacing with certain kinds of C code involving
structures. For instance, sometimes you just don’t care about exposing the internals of
a structure or turning it into a full-fledged extension type. With a capsule, you can put
a lightweight wrapper around it and easily pass it around to other extension functions.
15.5. Defining and Exporting C APIs from Extension
Modules
Problem
You have a C extension module that internally defines a variety of useful functions that
you would like to export as a public C API for use elsewhere. You would like to use these
functions inside other extension modules, but don’t know how to link them together,
and doing it with the C compiler/linker seems excessively complicated (or impossible).

Solution
This recipe focuses on the code written to handle Point objects, which were presented
in Recipe 15.4. If you recall, that C code included some utility functions like this:

/* Destructor function for points */
static void del_Point(PyObject *obj) {

  free(PyCapsule_GetPointer(obj,"Point"));
}

/* Utility functions */
static Point *PyPoint_AsPoint(PyObject *obj) {
  return (Point *) PyCapsule_GetPointer(obj, "Point");
}

static PyObject *PyPoint_FromPoint(Point *p, int must_free) {
  return PyCapsule_New(p, "Point", must_free ? del_Point : NULL);
}

The  problem  now  addressed  is  how  to  export  the  PyPoint_AsPoint()  and  Py
Point_FromPoint() functions as an API that other extension modules could use and
link to (e.g., if you have other extensions that also want to use the wrapped  Point
objects).
To solve this problem, start by introducing a new header file for the “sample” extension
called pysample.h. Put the following code in it:

/* pysample.h */
#include "Python.h"
#include "sample.h"
#ifdef __cplusplus
extern "C" {
#endif

/* Public API Table */
typedef struct {
  Point *(*aspoint)(PyObject *);
  PyObject *(*frompoint)(Point *, int);
} _PointAPIMethods;

#ifndef PYSAMPLE_MODULE
/* Method table in external module */
static _PointAPIMethods *_point_api = 0;

/* Import the API table from sample */
static int import_sample(void) {
  _point_api = (_PointAPIMethods *) PyCapsule_Import("sample._point_api",0);
  return (_point_api != NULL) ? 1 : 0;
}

/* Macros to implement the programming interface */
#define PyPoint_AsPoint(obj) (_point_api->aspoint)(obj)
#define PyPoint_FromPoint(obj) (_point_api->frompoint)(obj)
#endif

#ifdef __cplusplus
}
#endif

15.5. Defining and Exporting C APIs from Extension Modules 

The most important feature here is the _PointAPIMethods table of function pointers. It
will be initialized in the exporting module and found by importing modules.
Change the original extension module to populate the table and export it as follows:

/* pysample.c */

#include "Python.h"
#define PYSAMPLE_MODULE
#include "pysample.h"

...
/* Destructor function for points */
static void del_Point(PyObject *obj) {
  printf("Deleting point\n");
  free(PyCapsule_GetPointer(obj,"Point"));
}

/* Utility functions */
static Point *PyPoint_AsPoint(PyObject *obj) {
  return (Point *) PyCapsule_GetPointer(obj, "Point");
}

static PyObject *PyPoint_FromPoint(Point *p, int free) {
  return PyCapsule_New(p, "Point", free ? del_Point : NULL);
}

static _PointAPIMethods _point_api = {
  PyPoint_AsPoint,
  PyPoint_FromPoint
};
...

/* Module initialization function */
PyMODINIT_FUNC
PyInit_sample(void) {
  PyObject *m;
  PyObject *py_point_api;

  m = PyModule_Create(&samplemodule);
  if (m == NULL)
    return NULL;

  /* Add the Point C API functions */
  py_point_api = PyCapsule_New((void *) &_point_api, "sample._point_api", NULL);
  if (py_point_api) {
    PyModule_AddObject(m, "_point_api", py_point_api);
  }
  return m;
}

Finally, here is an example of a new extension module that loads and uses these API
functions:

/* ptexample.c */

/* Include the header associated with the other module */
#include "pysample.h"

/* An extension function that uses the exported API */
static PyObject *print_point(PyObject *self, PyObject *args) {
  PyObject *obj;
  Point *p;
  if (!PyArg_ParseTuple(args,"O", &obj)) {
    return NULL;
  }

  /* Note: This is defined in a different module */
  p = PyPoint_AsPoint(obj);
  if (!p) {
    return NULL;
  }
  printf("%f %f\n", p->x, p->y);
  return Py_BuildValue("");
}

static PyMethodDef PtExampleMethods[] = {
  {"print_point", print_point, METH_VARARGS, "output a point"},
  { NULL, NULL, 0, NULL}
};

static struct PyModuleDef ptexamplemodule = {
  PyModuleDef_HEAD_INIT,
  "ptexample",           /* name of module */
  "A module that imports an API",  /* Doc string (may be NULL) */
  -1,                 /* Size of per-interpreter state or -1 */
  PtExampleMethods       /* Method table */
};

/* Module initialization function */
PyMODINIT_FUNC
PyInit_ptexample(void) {
  PyObject *m;

  m = PyModule_Create(&ptexamplemodule);
  if (m == NULL)
    return NULL;

  /* Import sample, loading its API functions */
  if (!import_sample()) {
    return NULL;
  }

15.5. Defining and Exporting C APIs from Extension Modules 

  return m;
}

When compiling this new module, you don’t even need to bother to link against any of
the libraries or code from the other module. For example, you can just make a simple
setup.py file like this:

# setup.py
from distutils.core import setup, Extension

setup(name='ptexample',
      ext_modules=[
        Extension('ptexample',
                  ['ptexample.c'],
                  include_dirs = [],  # May need pysample.h directory
                  )
        ]
)

If it all works, you’ll find that your new extension function works perfectly with the C
API functions defined in the other module:

>>> import sample
>>> p1 = sample.Point(2,3)
>>> p1
<capsule object "Point *" at 0x1004ea330>
>>> import ptexample
>>> ptexample.print_point(p1)
2.000000 3.000000
>>>

Discussion
This recipe relies on the fact that capsule objects can hold a pointer to anything you
wish. In this case, the defining module populates a structure of function pointers, creates
a capsule that points to it, and saves the capsule in a module-level attribute (e.g., sam
ple._point_api).
Other modules can be programmed to pick up this attribute when imported and extract
the underlying pointer. In fact, Python provides the PyCapsule_Import() utility func‐
tion, which takes care of all the steps for you. You simply give it the name of the attribute
(e.g., sample._point_api), and it will find the capsule and extract the pointer all in one
step.
There are some C programming tricks involved in making exported functions look
normal in other modules. In the pysample.h file, a pointer _point_api is used to point
to the method table that was initialized in the exporting module. A related function
import_sample() is used to perform the required capsule import and initialize this
pointer. This function must be called before any functions are used. Normally, it would

be called in during module initialization. Finally, a set of C preprocessor macros have
been defined to transparently dispatch the API functions through the method table.
The user just uses the original function names, but doesn’t know about the extra indi‐
rection through these macros.
Finally, there is another important reason why you might use this technique to link
modules together—it’s actually easier and it keeps modules more cleanly decoupled. If
you didn’t want to use this recipe as shown, you might be able to cross-link modules
using advanced features of shared libraries and the dynamic loader. For example, putting
common API functions into a shared library and making sure that all extension modules
link against that shared library. Yes, this works, but it can be tremendously messy in
large systems. Essentially, this recipe cuts out all of that magic and allows modules to
link to one another through Python’s normal import mechanism and just a tiny number
of capsule calls. For compilation of modules, you only need to worry about header files,
not the hairy details of shared libraries.
Further information about providing C APIs for extension modules can be found in the
Python documentation.
15.6. Calling Python from C
Problem
You want to safely execute a Python callable from C and return a result back to C. For
example, perhaps you are writing C code that wants to use a Python function as a
callback.

Solution
Calling Python from C is mostly straightforward, but involves a number of tricky parts.
The following C code shows an example of how to do it safely:

#include <Python.h>

/* Execute func(x,y) in the Python interpreter.  The
   arguments and return result of the function must
   be Python floats */

double call_func(PyObject *func, double x, double y) {
  PyObject *args;
  PyObject *kwargs;
  PyObject *result = 0;
  double retval;

  /* Make sure we own the GIL */
  PyGILState_STATE state = PyGILState_Ensure();

15.6. Calling Python from C 

  /* Verify that func is a proper callable */
  if (!PyCallable_Check(func)) {
    fprintf(stderr,"call_func: expected a callable\n");
    goto fail;
  }
  /* Build arguments */
  args = Py_BuildValue("(dd)", x, y);
  kwargs = NULL;

  /* Call the function */
  result = PyObject_Call(func, args, kwargs);
  Py_DECREF(args);
  Py_XDECREF(kwargs);

  /* Check for Python exceptions (if any) */
  if (PyErr_Occurred()) {
    PyErr_Print();
    goto fail;
  }

  /* Verify the result is a float object */
  if (!PyFloat_Check(result)) {
    fprintf(stderr,"call_func: callable didn't return a float\n");
    goto fail;
  }

  /* Create the return value */
  retval = PyFloat_AsDouble(result);
  Py_DECREF(result);

  /* Restore previous GIL state and return */
  PyGILState_Release(state);
  return retval;

fail:
  Py_XDECREF(result);
  PyGILState_Release(state);
  abort();   // Change to something more appropriate
}

To use this function, you need to have obtained a reference to an existing Python callable
to pass in. There are many ways that you can go about doing that, such as having a
callable object passed into an extension module or simply writing C code to extract a
symbol from an existing module.
Here is a simple example that shows calling a function from an embedded Python
interpreter:

#include <Python.h>

/* Definition of call_func() same as above */
...

/* Load a symbol from a module */
PyObject *import_name(const char *modname, const char *symbol) {
  PyObject *u_name, *module;
  u_name = PyUnicode_FromString(modname);
  module = PyImport_Import(u_name);
  Py_DECREF(u_name);
  return PyObject_GetAttrString(module, symbol);
}

/* Simple embedding example */
int main() {
  PyObject *pow_func;
  double x;

  Py_Initialize();
  /* Get a reference to the math.pow function */
  pow_func = import_name("math","pow");

  /* Call it using our call_func() code */
  for (x = 0.0; x < 10.0; x += 0.1) {
    printf("%0.2f %0.2f\n", x, call_func(pow_func,x,2.0));
  }
  /* Done */
  Py_DECREF(pow_func);
  Py_Finalize();
  return 0;
}

To build this last example, you’ll need to compile the C and link against the Python
interpreter. Here is a Makefile that shows how you might do it (this is something that
might require some amount of fiddling with on your machine):

all::
        cc -g embed.c -I/usr/local/include/python3.3m \
          -L/usr/local/lib/python3.3/config-3.3m -lpython3.3m

Compiling and running the resulting executable should produce output similar to this:

0.00 0.00
0.10 0.01
0.20 0.04
0.30 0.09
0.40 0.16
...

Here is a slightly different example that shows an extension function that receives a
callable  and  some  arguments  and  passes  them  to  call_func()  for  the  purposes  of
testing:

/* Extension function for testing the C-Python callback */
PyObject *py_call_func(PyObject *self, PyObject *args) {
  PyObject *func;

15.6. Calling Python from C 

  double x, y, result;
  if (!PyArg_ParseTuple(args,"Odd", &func,&x,&y)) {
    return NULL;
  }
  result = call_func(func, x, y);
  return Py_BuildValue("d", result);
}

Using this extension function, you could test it as follows:

>>> import sample
>>> def add(x,y):
...     return x+y
...
>>> sample.call_func(add,3,4)
7.0
>>>

Discussion
If you are calling Python from C, the most important thing to keep in mind is that C is
generally going to be in charge. That is, C has the responsibility of creating the argu‐
ments, calling the Python function, checking for exceptions, checking types, extracting
return values, and more.
As a first step, it is critical that you have a Python object representing the callable that
you’re going to invoke. This could be a function, class, method, built-in method, or
anything that implements the __call__() operation. To verify that it’s callable, use 
PyCallable_Check() as shown in this code fragment:

double call_func(PyObject *func, double x, double y) {
  ...
  /* Verify that func is a proper callable */
  if (!PyCallable_Check(func)) {
    fprintf(stderr,"call_func: expected a callable\n");
    goto fail;
  }
  ...

As an aside, handling errors in the C code is something that you will need to carefully
study. As a general rule, you can’t just raise a Python exception. Instead, errors will have
to be handled in some other manner that makes sense to your C code. In the solution,
we’re using goto to transfer control to an error handling block that calls abort(). This
causes the whole program to die, but in real code you would probably want to do some‐
thing more graceful (e.g., return a status code). Keep in mind that C is in charge here,
so there isn’t anything comparable to just raising an exception. Error handling is some‐
thing you’ll have to engineer into the program somehow.
Calling a function is relatively straightforward—simply use PyObject_Call(), supply‐
ing  it  with  the  callable  object,  a  tuple  of  arguments,  and  an  optional  dictionary  of

keyword arguments. To build the argument tuple or dictionary, you can use Py_Build
Value(), as shown.

double call_func(PyObject *func, double x, double y) {
  PyObject *args;
  PyObject *kwargs;

  ...
  /* Build arguments */
  args = Py_BuildValue("(dd)", x, y);
  kwargs = NULL;

  /* Call the function */
  result = PyObject_Call(func, args, kwargs);
  Py_DECREF(args);
  Py_XDECREF(kwargs);
  ...

If there are no keyword arguments, you can pass NULL, as shown. After making the
function call, you need to make sure that you clean up the arguments using  Py_DE
CREF() or  Py_XDECREF(). The latter function safely allows the NULL pointer to be
passed (which is ignored), which is why we’re using it for cleaning up the optional
keyword arguments.
After calling the Python function, you must check for the presence of exceptions. The 
PyErr_Occurred() function can be used to do this. Knowing what to do in response to
an exception is tricky. Since you’re working from C, you really don’t have the exception
machinery that Python has. Thus, you would have to set an error status code, log the
error, or do some kind of sensible processing. In the solution, abort() is called for lack
of a simpler alternative (besides, hardened C programmers will appreciate the abrupt
crash):

  ...
  /* Check for Python exceptions (if any) */
  if (PyErr_Occurred()) {
    PyErr_Print();
    goto fail;
  }
  ...
fail:
  PyGILState_Release(state);
  abort();

Extracting information from the return value of calling a Python function is typically
going to involve some kind of type checking and value extraction. To do this, you may
have to use functions in the Python concrete objects layer. In the solution, the code
checks for and extracts the value of a Python float using  PyFloat_Check() and  Py
Float_AsDouble().

15.6. Calling Python from C 

A final tricky part of calling into Python from C concerns the management of Python’s
global interpreter lock (GIL). Whenever Python is accessed from C, you need to make
sure that the GIL is properly acquired and released. Otherwise, you run the risk of having
the interpreter corrupt data or crash. The calls to  PyGILState_Ensure() and  PyGIL
State_Release() make sure that it’s done correctly:

double call_func(PyObject *func, double x, double y) {
  ...
  double retval;

  /* Make sure we own the GIL */
  PyGILState_STATE state = PyGILState_Ensure();
  ...
  /* Code that uses Python C API functions */
  ...
  /* Restore previous GIL state and return */
  PyGILState_Release(state);
  return retval;

fail:
  PyGILState_Release(state);
  abort();
}

Upon return, PyGILState_Ensure() always guarantees that the calling thread has ex‐
clusive access to the Python interpreter. This is true even if the calling C code is running
a different thread that is unknown to the interpreter. At this point, the C code is free to
use any Python C-API functions that it wants. Upon successful completion,  PyGIL
State_Release() is used to restore the interpreter back to its original state.
It is critical to note that every PyGILState_Ensure() call must be followed by a matching
PyGILState_Release() call—even in cases where errors have occurred. In the solution,
the use of a goto statement might look like a horrible design, but we’re actually using it
to transfer control to a common exit block that performs this required step. Think of
the code after the fail: lable as serving the same purpose as code in a Python final
ly: block.
If you write your C code using all of these conventions including management of the
GIL, checking for exceptions, and thorough error checking, you’ll find that you can
reliably call into the Python interpreter from C—even in very complicated programs
that utilize advanced programming techniques such as multithreading.

15.7. Releasing the GIL in C Extensions
Problem
You have C extension code in that you want to execute concurrently with other threads
in the Python interpreter. To do this, you need to release and reacquire the global in‐
terpreter lock (GIL).

Solution
In C extension code, the GIL can be released and reacquired by inserting the following
macros in the code:

#include "Python.h"
...

PyObject *pyfunc(PyObject *self, PyObject *args) {
   ...
   Py_BEGIN_ALLOW_THREADS
   // Threaded C code.  Must not use Python API functions
   ...
   Py_END_ALLOW_THREADS
   ...
   return result;
}

Discussion
The GIL can only safely be released if you can guarantee that no Python C API functions
will be executed in the C code. Typical examples where the GIL might be released are
in computationally intensive code that performs calculations on C arrays (e.g., in ex‐
tensions such as numpy) or in code where blocking I/O operations are going to be per‐
formed (e.g., reading or writing on a file descriptor).
While the GIL is released, other Python threads are allowed to execute in the interpreter.
The Py_END_ALLOW_THREADS macro blocks execution until the calling threads reacquires
the GIL in the interpreter.
15.8. Mixing Threads from C and Python
Problem
You have a program that involves a mix of C, Python, and threads, but some of the
threads are created from C outside the control of the Python interpreter. Moreover,
certain threads utilize functions in the Python C API.

15.7. Releasing the GIL in C Extensions 

Solution
If you’re going to mix C, Python, and threads together, you need to make sure you
properly initialize and manage Python’s global interpreter lock (GIL). To do this, include
the following code somewhere in your C code and make sure it’s called prior to creation
of any threads:

#include <Python.h>

  ...
  if (!PyEval_ThreadsInitialized()) {
    PyEval_InitThreads();
  }
  ...

For any C code that involves Python objects or the Python C API, make sure you prop‐
erly acquire and release the GIL first. This is done using PyGILState_Ensure() and
PyGILState_Release(), as shown in the following:

  ...
  /* Make sure we own the GIL */
  PyGILState_STATE state = PyGILState_Ensure();

  /* Use functions in the interpreter */
  ...
  /* Restore previous GIL state and return */
  PyGILState_Release(state);
  ...

Every  call  to  PyGILState_Ensure()  must  have  a  matching  call  to  PyGILState_Re
lease().

Discussion
In advanced applications involving C and Python, it is not uncommon to have many
things going on at once—possibly involving a mix of a C code, Python code, C threads,
and Python threads. As long as you diligently make sure the interpreter is properly
initialized and that C code involving the interpreter has the proper GIL management
calls, it all should work.
Be aware that the PyGILState_Ensure() call does not immediately preempt or interrupt
the interpreter. If other code is currently executing, this function will block until that
code decides to release the GIL. Internally, the interpreter performs periodic thread
switching, so even if another thread is executing, the caller will eventually get to run
(although it may have to wait for a while first).

15.9. Wrapping C Code with Swig
Problem
You have existing C code that you would like to access as a C extension module. You
would like to do this using the Swig wrapper generator.

Solution
Swig operates by parsing C header files and automatically creating extension code. To
use it, you first need to have a C header file. For example, this header file for our sample
code:

/* sample.h */

#include <math.h>
extern int gcd(int, int);
extern int in_mandel(double x0, double y0, int n);
extern int divide(int a, int b, int *remainder);
extern double avg(double *a, int n);

typedef struct Point {
    double x,y;
} Point;

extern double distance(Point *p1, Point *p2);

Once you have the header files, the next step is to write a Swig “interface” file. By con‐
vention, these files have a .i suffix and might look similar to the following:

// sample.i - Swig interface
%module sample
%{
#include "sample.h"
%}

/* Customizations */
%extend Point {
    /* Constructor for Point objects */
    Point(double x, double y) {
        Point *p = (Point *) malloc(sizeof(Point));
        p->x = x;
        p->y = y;
        return p;
   };
};

/* Map int *remainder as an output argument */
%include typemaps.i
%apply int *OUTPUT { int * remainder };

15.9. Wrapping C Code with Swig 

/* Map the argument pattern (double *a, int n) to arrays */
%typemap(in) (double *a, int n)(Py_buffer view) {
  view.obj = NULL;
  if (PyObject_GetBuffer($input, &view, PyBUF_ANY_CONTIGUOUS | PyBUF_FORMAT) == -1) {
    SWIG_fail;
  }
  if (strcmp(view.format,"d") != 0) {
    PyErr_SetString(PyExc_TypeError, "Expected an array of doubles");
    SWIG_fail;
  }
  $1 = (double *) view.buf;
  $2 = view.len / sizeof(double);
}

%typemap(freearg) (double *a, int n) {
  if (view$argnum.obj) {
    PyBuffer_Release(&view$argnum);
  }
}

/* C declarations to be included in the extension module */

extern int gcd(int, int);
extern int in_mandel(double x0, double y0, int n);
extern int divide(int a, int b, int *remainder);
extern double avg(double *a, int n);

typedef struct Point {
    double x,y;
} Point;

extern double distance(Point *p1, Point *p2);

Once you have written the interface file, Swig is invoked as a command-line tool:

bash % swig -python -py3 sample.i
bash %

The output of swig is two files, sample_wrap.c and sample.py. The latter file is what
users import. The sample_wrap.c file is C code that needs to be compiled into a sup‐
porting module called _sample. This is done using the same techniques as for normal
extension modules. For example, you create a setup.py file like this:

# setup.py
from distutils.core import setup, Extension

setup(name='sample',
      py_modules=['sample.py'],
      ext_modules=[
        Extension('_sample',
                  ['sample_wrap.c'],
                  include_dirs = [],
                  define_macros = [],

                  undef_macros = [],
                  library_dirs = [],
                  libraries = ['sample']
                  )
        ]
)

To compile and test, run python3 on the setup.py file like this:

bash % python3 setup.py build_ext --inplace
running build_ext
building '_sample' extension
gcc -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes
-I/usr/local/include/python3.3m -c sample_wrap.c
 -o build/temp.macosx-10.6-x86_64-3.3/sample_wrap.o
sample_wrap.c: In function ‘SWIG_InitializeModule’:
sample_wrap.c:3589: warning: statement with no effect
gcc -bundle -undefined dynamic_lookup build/temp.macosx-10.6-x86_64-3.3/sample.o
 build/temp.macosx-10.6-x86_64-3.3/sample_wrap.o -o _sample.so -lsample
bash %

If all of this works, you’ll find that you can use the resulting C extension module in a
straightforward way. For example:

>>> import sample
>>> sample.gcd(42,8)
2
>>> sample.divide(42,8)
[5, 2]
>>> p1 = sample.Point(2,3)
>>> p2 = sample.Point(4,5)
>>> sample.distance(p1,p2)
2.8284271247461903
>>> p1.x
2.0
>>> p1.y
3.0
>>> import array
>>> a = array.array('d',[1,2,3])
>>> sample.avg(a)
2.0
>>>

Discussion
Swig is one of the oldest tools for building extension modules, dating back to Python
1.4. However, recent versions currently support Python 3. The primary users of Swig
tend to have large existing bases of C that they are trying to access using Python as a
high-level control language. For instance, a user might have C code containing thou‐
sands  of  functions  and  various  data  structures  that  they  would  like  to  access  from
Python. Swig can automate much of the wrapper generation process.

15.9. Wrapping C Code with Swig 

All Swig interfaces tend to start with a short preamble like this:

%module sample
%{
#include "sample.h"
%}

This merely declares the name of the extension module and specifies C header files that
must be included to make everything compile (the code enclosed in %{ and %} is pasted
directly into the output code so this is where you put all included files and other defi‐
nitions needed for compilation).
The bottom part of a Swig interface is a listing of C declarations that you want to be
included in the extension. This is often just copied from the header files. In our example,
we just pasted in the header file directly like this:

%module sample
%{
#include "sample.h"
%}
...
extern int gcd(int, int);
extern int in_mandel(double x0, double y0, int n);
extern int divide(int a, int b, int *remainder);
extern double avg(double *a, int n);

typedef struct Point {
    double x,y;
} Point;

extern double distance(Point *p1, Point *p2);

It is important to stress that these declarations are telling Swig what you want to include
in the Python module. It is quite common to edit the list of declarations or to make
modifications as appropriate. For example, if you didn’t want certain declarations to be
included, you would remove them from the declaration list.
The most complicated part of using Swig is the various customizations that it can apply
to the C code. This is a huge topic that can’t be covered in great detail here, but a number
of such customizations are shown in this recipe.
The first customization involving the %extend directive allows methods to be attached
to existing structure and class definitions. In the example, this is used to add a con‐
structor method to the Point structure. This customization makes it possible to use the
structure like this:

>>> p1 = sample.Point(2,3)
>>>

If omitted, then Point objects would have to be created in a much more clumsy manner
like this:

>>> # Usage if %extend Point is omitted
>>> p1 = sample.Point()
>>> p1.x = 2.0
>>> p1.y = 3

The second customization involving the inclusion of the typemaps.i library and the 
%apply directive is instructing Swig that the argument signature int *remainder is to
be treated as an output value. This is actually a pattern matching rule. In all declarations
that follow, any time  int  *remainder is encountered, it is handled as output. This
customization is what makes the divide() function return two values:

>>> sample.divide(42,8)
[5, 2]
>>>

The last customization involving the %typemap directive is probably the most advanced
feature shown here. A typemap is a rule that gets applied to specific argument patterns
in the input. In this recipe, a typemap has been written to match the argument pattern
(double *a, int n). Inside the typemap is a fragment of C code that tells Swig how
to convert a Python object into the associated C arguments. The code in this recipe has
been written using Python’s buffer protocol in an attempt to match any input argument
that looks like an array of doubles (e.g., NumPy arrays, arrays created by the  array
module, etc.). See Recipe 15.3.
Within the typemap code, substitutions such as $1 and $2 refer to variables that hold
the converted values of the C arguments in the typemap pattern (e.g., $1 maps to double
*a and $2 maps to int n). $input refers to a PyObject * argument that was supplied
as an input argument. $argnum is the argument number.
Writing and understanding typemaps is often the bane of programmers using Swig. Not
only is the code rather cryptic, but you need to understand the intricate details of both
the Python C API and the way in which Swig interacts with it. The Swig documentation
has many more examples and detailed information.
Nevertheless, if you have a lot of a C code to expose as an extension module, Swig can
be a very powerful tool for doing it. The key thing to keep in mind is that Swig is basically
a compiler that processes C declarations, but with a powerful pattern matching and
customization component that lets you change the way in which specific declarations
and types get processed. More information can be found at Swig’s website, including
Python-specific documentation. 

15.9. Wrapping C Code with Swig 

15.10. Wrapping Existing C Code with Cython
Problem
You want to use Cython to make a Python extension module that wraps around an
existing C library.

Solution
Making an extension module with Cython looks somewhat similar to writing a hand‐
written extension, in that you will be creating a collection of wrapper functions. How‐
ever, unlike previous recipes, you won’t be doing this in C—the code will look a lot more
like Python.
As preliminaries, assume that the sample code shown in the introduction to this chapter
has been compiled into a C library called  libsample. Start by creating a file named
csample.pxd that looks like this:

# csample.pxd
#
# Declarations of "external" C functions and structures

cdef extern from "sample.h":
    int gcd(int, int)
    bint in_mandel(double, double, int)
    int divide(int, int, int *)
    double avg(double *, int) nogil

    ctypedef struct Point:
         double x
         double y

    double distance(Point *, Point *)

This file serves the same purpose in Cython as a C header file. The initial declaration
cdef  extern  from  "sample.h"  declares  the  required  C  header  file.  Declarations
that follow are taken from that header. The name of this file is csample.pxd, not sam‐
ple.pxd—this is important.
Next, create a file named sample.pyx. This file will define wrappers that bridge the
Python interpreter to the underlying C code declared in the csample.pxd file:

# sample.pyx

# Import the low-level C declarations
cimport csample

# Import some functionality from Python and the C stdlib
from cpython.pycapsule cimport *

from libc.stdlib cimport malloc, free

# Wrappers
def gcd(unsigned int x, unsigned int y):
    return csample.gcd(x, y)

def in_mandel(x, y, unsigned int n):
    return csample.in_mandel(x, y, n)

def divide(x, y):
    cdef int rem
    quot = csample.divide(x, y, &rem)
    return quot, rem

def avg(double[:] a):
    cdef:
        int sz
        double result

    sz = a.size
    with nogil:
        result = csample.avg(<double *> &a[0], sz)
    return result

# Destructor for cleaning up Point objects
cdef del_Point(object obj):
    pt = <csample.Point *> PyCapsule_GetPointer(obj,"Point")
    free(<void *> pt)

# Create a Point object and return as a capsule
def Point(double x,double y):
    cdef csample.Point *p
    p = <csample.Point *> malloc(sizeof(csample.Point))
    if p == NULL:
        raise MemoryError("No memory to make a Point")
    p.x = x
    p.y = y
    return PyCapsule_New(<void *>p,"Point",<PyCapsule_Destructor>del_Point)

def distance(p1, p2):
    pt1 = <csample.Point *> PyCapsule_GetPointer(p1,"Point")
    pt2 = <csample.Point *> PyCapsule_GetPointer(p2,"Point")
    return csample.distance(pt1,pt2)

Various details of this file will be covered further in the discussion section. Finally, to
build the extension module, create a setup.py file that looks like this:

from distutils.core import setup
from distutils.extension import Extension
from Cython.Distutils import build_ext

ext_modules = [
    Extension('sample',

15.10. Wrapping Existing C Code with Cython 

              ['sample.pyx'],
              libraries=['sample'],
              library_dirs=['.'])]
setup(
  name = 'Sample extension module',
  cmdclass = {'build_ext': build_ext},
  ext_modules = ext_modules
)

To build the resulting module for experimentation, type this:

bash % python3 setup.py build_ext --inplace
running build_ext
cythoning sample.pyx to sample.c
building 'sample' extension
gcc -fno-strict-aliasing -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes
 -I/usr/local/include/python3.3m -c sample.c
 -o build/temp.macosx-10.6-x86_64-3.3/sample.o
gcc -bundle -undefined dynamic_lookup build/temp.macosx-10.6-x86_64-3.3/sample.o
  -L. -lsample -o sample.so
bash %

If it works, you should have an extension module sample.so that can be used as shown
in the following example:

>>> import sample
>>> sample.gcd(42,10)
2
>>> sample.in_mandel(1,1,400)
False
>>> sample.in_mandel(0,0,400)
True
>>> sample.divide(42,10)
(4, 2)
>>> import array
>>> a = array.array('d',[1,2,3])
>>> sample.avg(a)
2.0
>>> p1 = sample.Point(2,3)
>>> p2 = sample.Point(4,5)
>>> p1
<capsule object "Point" at 0x1005d1e70>
>>> p2
<capsule object "Point" at 0x1005d1ea0>
>>> sample.distance(p1,p2)
2.8284271247461903
>>>

Discussion
This recipe incorporates a number of advanced features discussed in prior recipes, in‐
cluding manipulation of arrays, wrapping opaque pointers, and releasing the GIL. Each
of these parts will be discussed in turn, but it may help to review earlier recipes first.
At a high level, using Cython is modeled after C. The .pxd files merely contain C defi‐
nitions (similar to .h files) and the .pyx files contain implementation (similar to a .c file).
The cimport statement is used by Cython to import definitions from a .pxd file. This is
different than using a normal Python import statement, which would load a regular
Python module.
Although .pxd files contain definitions, they are not used for the purpose of automati‐
cally creating extension code. Thus, you still have to write simple wrapper functions.
For example, even though the csample.pxd file declares int gcd(int, int) as a func‐
tion, you still have to write a small wrapper for it in sample.pyx. For instance:

cimport csample

def gcd(unsigned int x, unsigned int y):
    return csample.gcd(x,y)

For simple functions, you don’t have to do too much. Cython will generate wrapper code
that properly converts the arguments and return value. The C data types attached to the
arguments are optional. However, if you include them, you get additional error checking
for free. For example, if someone calls this function with negative values, an exception
is generated:

>>> sample.gcd(-10,2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "sample.pyx", line 7, in sample.gcd (sample.c:1284)
    def gcd(unsigned int x,unsigned int y):
OverflowError: can't convert negative value to unsigned int
>>>

If you want to add additional checking to the wrapper, just use additional wrapper code.
For example:

def gcd(unsigned int x, unsigned int y):
    if x <= 0:
        raise ValueError("x must be > 0")
    if y <= 0:
        raise ValueError("y must be > 0")
    return csample.gcd(x,y)

The declaration of in_mandel() in the csample.pxd file has an interesting, but subtle
definition. In that file, the function is declared as returning a bint instead of an int.
This causes the function to create a proper Boolean value from the result instead of a
simple integer. So, a return value of 0 gets mapped to False and 1 to True.

15.10. Wrapping Existing C Code with Cython 

Within the Cython wrappers, you have the option of declaring C data types in addition
to using all of the usual Python objects. The wrapper for divide() shows an example
of this as well as how to handle a pointer argument.

def divide(x,y):
    cdef int rem
    quot = csample.divide(x,y,&rem)
    return quot, rem

Here, the rem variable is explicitly declared as a C int variable. When passed to the
underlying divide() function, &rem makes a pointer to it just as in C.
The code for the avg() function illustrates some more advanced features of Cython.
First the declaration def avg(double[:] a) declares avg() as taking a one-dimensional
memoryview of double values. The amazing part about this is that the resulting function
will accept any compatible array object, including those created by libraries such as
numpy. For example:
>>> import array
>>> a = array.array('d',[1,2,3])
>>> import numpy
>>> b = numpy.array([1., 2., 3.])
>>> import sample
>>> sample.avg(a)
2.0
>>> sample.avg(b)
2.0
>>>

In the wrapper, a.size and &a[0] refer to the number of array items and underlying
pointer, respectively. The syntax <double *> &a[0] is how you type cast pointers to a
different type if necessary. This is needed to make sure the C avg() receives a pointer
of the correct type. Refer to the next recipe for some more advanced usage of Cython
memoryviews.
In addition to working with general arrays, the avg() example also shows how to work
with the global interpreter lock. The statement with nogil: declares a block of code as
executing without the GIL. Inside this block, it is illegal to work with any kind of normal
Python object—only objects and functions declared as cdef can be used. In addition to
that, external functions must explicitly declare that they can execute without the GIL.
Thus, in the csample.pxd file, the avg() is declared as double avg(double *, int)
nogil.
The handling of the Point structure presents a special challenge. As shown, this recipe
treats  Point  objects  as  opaque  pointers  using  capsule  objects,  as  described  in
Recipe 15.4. However, to do this, the underlying Cython code is a bit more complicated.
First, the following imports are being used to bring in definitions of functions from the
C library and Python C API:

from cpython.pycapsule cimport *
from libc.stdlib cimport malloc, free

The function del_Point() and Point() use this functionality to create a capsule object
that  wraps  around  a  Point  *  pointer.  The  declaration  cdef  del_Point()  declares
del_Point() as a function that is only accessible from Cython and not Python. Thus,
this function will not be visible to the outside—instead, it’s used as a callback function
to  clean  up  memory  allocated  by  the  capsule.  Calls  to  functions  such  as  PyCap
sule_New(), PyCapsule_GetPointer() are directly from the Python C API and are used
in the same way.
The distance() function has been written to extract pointers from the capsule objects
created by Point(). One notable thing here is that you simply don’t have to worry about
exception handling. If a bad object is passed, PyCapsule_GetPointer() raises an ex‐
ception,  but  Cython  already  knows  to  look  for  it  and  propagate  it  out  of  the  dis
tance() function if it occurs.
A downside to the handling of Point structures is that they will be completely opaque
in this implementation. You won’t be able to peek inside or access any of their attributes.
There is an alternative approach to wrapping, which is to define an extension type, as
shown in this code:

# sample.pyx

cimport csample
from libc.stdlib cimport malloc, free
...

cdef class Point:
    cdef csample.Point *_c_point
    def __cinit__(self, double x, double y):
        self._c_point = <csample.Point *> malloc(sizeof(csample.Point))
        self._c_point.x = x
        self._c_point.y = y

    def __dealloc__(self):
        free(self._c_point)

    property x:
        def __get__(self):
            return self._c_point.x
        def __set__(self, value):
            self._c_point.x = value

    property y:
        def __get__(self):
            return self._c_point.y
        def __set__(self, value):
            self._c_point.y = value

15.10. Wrapping Existing C Code with Cython 

def distance(Point p1, Point p2):
    return csample.distance(p1._c_point, p2._c_point)

Here, the cdef class Point is declaring Point as an extension type. The class variable
cdef csample.Point *_c_point is declaring an instance variable that holds a pointer
to an underlying Point structure in C. The __cinit__() and __dealloc__() methods
create and destroy the underlying C structure using malloc() and free() calls. The
property x and property y declarations give code that gets and sets the underlying
structure attributes. The wrapper for distance() has also been suitably modified to
accept instances of the  Point extension type as arguments, but pass the underlying
pointer to the C function.
Making this change, you will find that the code for manipulating Point objects is more
natural:

>>> import sample
>>> p1 = sample.Point(2,3)
>>> p2 = sample.Point(4,5)
>>> p1
<sample.Point object at 0x100447288>
>>> p2
<sample.Point object at 0x1004472a0>
>>> p1.x
2.0
>>> p1.y
3.0
>>> sample.distance(p1,p2)
2.8284271247461903
>>>

This recipe has illustrated many of Cython’s core features that you might be able to
extrapolate to more complicated kinds of wrapping. However, you will definitely want
to read more of the official documentation to do more.
The next few recipes also illustrate a few additional Cython features.
15.11. Using Cython to Write High-Performance Array
Operations
Problem
You would like to write some high-performance array processing functions to operate
on arrays from libraries such as NumPy. You’ve heard that tools such as Cython can
make this easier, but aren’t sure how to do it.

Solution
As an example, consider the following code which shows a Cython function for clipping
the values in a simple one-dimensional array of doubles:

# sample.pyx (Cython)

cimport cython

@cython.boundscheck(False)
@cython.wraparound(False)
cpdef clip(double[:] a, double min, double max, double[:] out):
    '''
    Clip the values in a to be between min and max. Result in out
    '''
    if min > max:
        raise ValueError("min must be <= max")
    if a.shape[0] != out.shape[0]:
        raise ValueError("input and output arrays must be the same size")
    for i in range(a.shape[0]):
        if a[i] < min:
            out[i] = min
        elif a[i] > max:
            out[i] = max
        else:
            out[i] = a[i]

To compile and build the extension, you’ll need a setup.py file such as the following (use
python3 setup.py build_ext --inplace to build it):

from distutils.core import setup
from distutils.extension import Extension
from Cython.Distutils import build_ext

ext_modules = [
    Extension('sample',
              ['sample.pyx'])
]

setup(
  name = 'Sample app',
  cmdclass = {'build_ext': build_ext},
  ext_modules = ext_modules
)

You will find that the resulting function clips arrays, and that it works with many dif‐
ferent kinds of array objects. For example:

>>> # array module example
>>> import sample
>>> import array
>>> a = array.array('d',[1,-3,4,7,2,0])
>>> a

15.11. Using Cython to Write High-Performance Array Operations 

array('d', [1.0, -3.0, 4.0, 7.0, 2.0, 0.0])
>>> sample.clip(a,1,4,a)
>>> a
array('d', [1.0, 1.0, 4.0, 4.0, 2.0, 1.0])

>>> # numpy example
>>> import numpy
>>> b = numpy.random.uniform(-10,10,size=1000000)
>>> b
array([-9.55546017,  7.45599334,  0.69248932, ...,  0.69583148,
       -3.86290931,  2.37266888])
>>> c = numpy.zeros_like(b)
>>> c
array([ 0.,  0.,  0., ...,  0.,  0.,  0.])
>>> sample.clip(b,-5,5,c)
>>> c
array([-5.        ,  5.        ,  0.69248932, ...,  0.69583148,
       -3.86290931,  2.37266888])
>>> min(c)
-5.0
>>> max(c)
5.0
>>>

You will also find that the resulting code is fast. The following session puts our imple‐
mentation in a head-to-head battle with the clip() function already present in numpy:

>>> timeit('numpy.clip(b,-5,5,c)','from __main__ import b,c,numpy',number=1000)
8.093049556000551
>>> timeit('sample.clip(b,-5,5,c)','from __main__ import b,c,sample',
...         number=1000)
3.760528204000366
>>>

As you can see, it’s quite a bit faster—an interesting result considering the core of the
NumPy version is written in C.

Discussion
This recipe utilizes Cython typed memoryviews, which greatly simplify code that op‐
erates on arrays. The declaration cpdef clip() declares clip() as both a C-level and
Python-level function. In Cython, this is useful, because it means that the function call
is more efficently called by other Cython functions (e.g., if you want to invoke clip()
from a different Cython function).
The typed parameters double[:] a and double[:] out declare those parameters as
one-dimensional  arrays  of  doubles.  As  input,  they  will  access  any  array  object  that
properly implements the memoryview interface, as described in PEP 3118. This includes
arrays from NumPy and from the built-in array library.

When writing code that produces a result that is also an array, you should follow the
convention shown of having an output parameter as shown. This places the responsi‐
bility of creating the output array on the caller and frees the code from having to know
too much about the specific details of what kinds of arrays are being manipulated (it
just assumes the arrays are already in-place and only needs to perform a few basic sanity
checks such as making sure their sizes are compatible). In libraries such as NumPy, it
is relatively easy to create output arrays using functions such as  numpy.zeros() or
numpy.zeros_like().  Alternatively,  to  create  uninitialized  arrays,  you  can  use  num
py.empty() or numpy.empty_like(). This will be slightly faster if you’re about to over‐
write the array contents with a result.
In the implementation of your function, you simply write straightforward looking array
processing code using indexing and array lookups (e.g., a[i], out[i], and so forth).
Cython will take steps to make sure these produce efficient code.
The two decorators that precede the definition of clip() are a few optional performance
optimizations. @cython.boundscheck(False) eliminates all array bounds checking and
can  be  used  if  you  know  the  indexing  won’t  go  out  of  range.  @cython.wrap
around(False) eliminates the handling of negative array indices as wrapping around
to the end of the array (like with Python lists). The inclusion of these decorators can
make the code run substantially faster (almost 2.5 times faster on this example when
tested).
Whenever working with arrays, careful study and experimentation with the underlying
algorithm can also yield large speedups. For example, consider this variant of the clip()
function that uses conditional expressions:

@cython.boundscheck(False)
@cython.wraparound(False)
cpdef clip(double[:] a, double min, double max, double[:] out):
    if min > max:
        raise ValueError("min must be <= max")
    if a.shape[0] != out.shape[0]:
        raise ValueError("input and output arrays must be the same size")
    for i in range(a.shape[0]):
        out[i] = (a[i] if a[i] < max else max) if a[i] > min else min

When tested, this version of the code runs over 50% faster (2.44s versus 3.76s on the
timeit() test shown earlier).
At this point, you might be wondering how this code would stack up against a hand‐
written C version. For example, perhaps you write the following C function and craft a
handwritten extension to using techniques shown in earlier recipes:

void clip(double *a, int n, double min, double max, double *out) {
  double x;
  for (; n >= 0; n--, a++, out++) {
    x = *a;

15.11. Using Cython to Write High-Performance Array Operations 

    *out = x > max ? max : (x < min ? min : x);
  }
}

The extension code for this isn’t shown, but after experimenting, we found that a hand‐
crafted C extension ran more than 10% slower than the version created by Cython. The
bottom line is that the code runs a lot faster than you might think.
There are several extensions that can be made to the solution code. For certain kinds of
array operations, it might make sense to release the GIL so that multiple threads can
run in parallel. To do that, modify the code to include the with nogil: statement:

@cython.boundscheck(False)
@cython.wraparound(False)
cpdef clip(double[:] a, double min, double max, double[:] out):
    if min > max:
        raise ValueError("min must be <= max")
    if a.shape[0] != out.shape[0]:
        raise ValueError("input and output arrays must be the same size")
    with nogil:
        for i in range(a.shape[0]):
            out[i] = (a[i] if a[i] < max else max) if a[i] > min else min

If you want to write a version of the code that operates on two-dimensional arrays, here
is what it might look like:

@cython.boundscheck(False)
@cython.wraparound(False)
cpdef clip2d(double[:,:] a, double min, double max, double[:,:] out):
    if min > max:
        raise ValueError("min must be <= max")
    for n in range(a.ndim):
        if a.shape[n] != out.shape[n]:
            raise TypeError("a and out have different shapes")
    for i in range(a.shape[0]):
        for j in range(a.shape[1]):
            if a[i,j] < min:
                out[i,j] = min
            elif a[i,j] > max:
                out[i,j] = max
            else:
                out[i,j] = a[i,j]

Hopefully it’s not lost on the reader that all of the code in this recipe is not tied to any
specific array library (e.g., NumPy). That gives the code a great deal of flexibility. How‐
ever, it’s also worth noting that dealing with arrays can be significantly more complicated
once multiple dimensions, strides, offsets, and other factors are introduced. Those top‐
ics are beyond the scope of this recipe, but more information can be found in PEP
3118. The Cython documentation on “typed memoryviews” is also essential reading.

15.12. Turning a Function Pointer into a Callable
Problem
You have (somehow) obtained the memory address of a compiled function, but want
to turn it into a Python callable that you can use as an extension function.

Solution
The ctypes module can be used to create Python callables that wrap around arbitrary
memory addresses. The following example shows how to obtain the raw, low-level ad‐
dress of a C function and how to turn it back into a callable object:

>>> import ctypes
>>> lib = ctypes.cdll.LoadLibrary(None)
>>> # Get the address of sin() from the C math library
>>> addr = ctypes.cast(lib.sin, ctypes.c_void_p).value
>>> addr
140735505915760

>>> # Turn the address into a callable function
>>> functype = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double)
>>> func = functype(addr)
>>> func
<CFunctionType object at 0x1006816d0>

>>> # Call the resulting function
>>> func(2)
0.9092974268256817
>>> func(0)
0.0
>>>

Discussion
To make a callable, you must first create a CFUNCTYPE instance. The first argument to
CFUNCTYPE() is the return type. Subsequent arguments are the types of the arguments.
Once you have defined the function type, you wrap it around an integer memory address
to create a callable object. The resulting object is used like any normal function accessed
through ctypes.
This recipe might look rather cryptic and low level. However, it is becoming increasingly
common for programs and libraries to utilize advanced code generation techniques like
just in-time compilation, as found in libraries such as LLVM.
For example, here is a simple example that uses the llvmpy extension to make a small
assembly function, obtain a function pointer to it, and turn it into a Python callable:

15.12. Turning a Function Pointer into a Callable 

>>> from llvm.core import Module, Function, Type, Builder
>>> mod = Module.new('example')
>>> f = Function.new(mod,Type.function(Type.double(), \
                     [Type.double(), Type.double()], False), 'foo')
>>> block = f.append_basic_block('entry')
>>> builder = Builder.new(block)
>>> x2 = builder.fmul(f.args[0],f.args[0])
>>> y2 = builder.fmul(f.args[1],f.args[1])
>>> r = builder.fadd(x2,y2)
>>> builder.ret(r)
<llvm.core.Instruction object at 0x10078e990>
>>> from llvm.ee import ExecutionEngine
>>> engine = ExecutionEngine.new(mod)
>>> ptr = engine.get_pointer_to_function(f)
>>> ptr
4325863440
>>> foo = ctypes.CFUNCTYPE(ctypes.c_double, ctypes.c_double, ctypes.c_double)(ptr)

>>> # Call the resulting function
>>> foo(2,3)
13.0
>>> foo(4,5)
41.0
>>> foo(1,2)
5.0
>>>

It goes without saying that doing anything wrong at this level will probably cause the
Python interpreter to die a horrible death. Keep in mind that you’re directly working
with  machine-level  memory  addresses  and  native  machine  code—not  Python
functions.
15.13. Passing NULL-Terminated Strings to C Libraries
Problem
You are writing an extension module that needs to pass a NULL-terminated string to a
C library. However, you’re not entirely sure how to do it with Python’s Unicode string
implementation.

Solution
Many C libraries include functions that operate on NULL-terminated strings declared
as type char *. Consider the following C function that we will use for the purposes of
illustration and testing:

void print_chars(char *s) {
    while (*s) {
        printf("%2x ", (unsigned char) *s);

        s++;
    }
    printf("\n");
}

This function simply prints out the hex representation of individual characters so that
the passed strings can be easily debugged. For example:
print_chars("Hello");   // Outputs: 48 65 6c 6c 6f

For calling such a C function from Python, you have a few choices. First, you could
restrict it to only operate on bytes using "y" conversion code to PyArg_ParseTuple()
like this:

static PyObject *py_print_chars(PyObject *self, PyObject *args) {
  char *s;

  if (!PyArg_ParseTuple(args, "y", &s)) {
    return NULL;
  }
  print_chars(s);
  Py_RETURN_NONE;
}

The resulting function operates as follows. Carefully observe how bytes with embedded
NULL bytes and Unicode strings are rejected:

>>> print_chars(b'Hello World')
48 65 6c 6c 6f 20 57 6f 72 6c 64
>>> print_chars(b'Hello\x00World')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: must be bytes without null bytes, not bytes
>>> print_chars('Hello World')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'str' does not support the buffer interface
>>>

If you want to pass Unicode strings instead, use the "s" format code to PyArg_Parse
Tuple() such as this:

static PyObject *py_print_chars(PyObject *self, PyObject *args) {
  char *s;

  if (!PyArg_ParseTuple(args, "s", &s)) {
    return NULL;
  }
  print_chars(s);
  Py_RETURN_NONE;
}

15.13. Passing NULL-Terminated Strings to C Libraries 

When used, this will automatically convert all strings to a NULL-terminated UTF-8
encoding. For example:

>>> print_chars('Hello World')
48 65 6c 6c 6f 20 57 6f 72 6c 64
>>> print_chars('Spicy Jalape\u00f1o')  # Note: UTF-8 encoding
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f
>>> print_chars('Hello\x00World')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: must be str without null characters, not str
>>> print_chars(b'Hello World')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: must be str, not bytes
>>>

If for some reason, you are working directly with a PyObject * and can’t use PyArg_Par
seTuple(), the following code samples show how you can check and extract a suitable
char * reference, from both a bytes and string object:

/* Some Python Object (obtained somehow) */
PyObject *obj;

/* Conversion from bytes */
{
   char *s;
   s = PyBytes_AsString(o);
   if (!s) {
      return NULL;   /* TypeError already raised */
   }
   print_chars(s);
}

/* Conversion to UTF-8 bytes from a string */
{
   PyObject *bytes;
   char *s;
   if (!PyUnicode_Check(obj)) {
       PyErr_SetString(PyExc_TypeError, "Expected string");
       return NULL;
   }
   bytes = PyUnicode_AsUTF8String(obj);
   s = PyBytes_AsString(bytes);
   print_chars(s);
   Py_DECREF(bytes);
}

Both of the preceding conversions guarantee NULL-terminated data, but they do not
check for embedded NULL bytes elsewhere inside the string. Thus, that’s something
that you would need to check yourself if it’s important.

Discussion
If it all possible, you should try to avoid writing code that relies on NULL-terminated
strings since Python has no such requirement. It is almost always better to handle strings
using the combination of a pointer and a size if possible. Nevertheless, sometimes you
have to work with legacy C code that presents no other option.
Although it is easy to use, there is a hidden memory overhead associated with using the
"s" format code to PyArg_ParseTuple() that is easy to overlook. When you write code
that uses this conversion, a UTF-8 string is created and permanently attached to the
original string object. If the original string contains non-ASCII characters, this makes
the size of the string increase until it is garbage collected. For example:

>>> import sys
>>> s = 'Spicy Jalape\u00f1o'
>>> sys.getsizeof(s)
87
>>> print_chars(s)     # Passing string
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f
>>> sys.getsizeof(s)   # Notice increased size
103
>>>

If this growth in memory use is a concern, you should rewrite your C extension code
to use the PyUnicode_AsUTF8String() function like this:

static PyObject *py_print_chars(PyObject *self, PyObject *args) {
  PyObject *o, *bytes;
  char *s;

  if (!PyArg_ParseTuple(args, "U", &o)) {
    return NULL;
  }
  bytes = PyUnicode_AsUTF8String(o);
  s = PyBytes_AsString(bytes);
  print_chars(s);
  Py_DECREF(bytes);
  Py_RETURN_NONE;
}

With this modification, a UTF-8 encoded string is created if needed, but then discarded
after use. Here is the modified behavior:

>>> import sys
>>> s = 'Spicy Jalape\u00f1o'
>>> sys.getsizeof(s)
87
>>> print_chars(s)
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f
>>> sys.getsizeof(s)
87
>>>

15.13. Passing NULL-Terminated Strings to C Libraries 

If you are trying to pass NULL-terminated strings to functions wrapped via ctypes, be
aware that ctypes only allows bytes to be passed and that it does not check for embedded
NULL bytes. For example:

>>> import ctypes
>>> lib = ctypes.cdll.LoadLibrary("./libsample.so")
>>> print_chars = lib.print_chars
>>> print_chars.argtypes = (ctypes.c_char_p,)
>>> print_chars(b'Hello World')
48 65 6c 6c 6f 20 57 6f 72 6c 64
>>> print_chars(b'Hello\x00World')
48 65 6c 6c 6f
>>> print_chars('Hello World')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ctypes.ArgumentError: argument 1: <class 'TypeError'>: wrong type
>>>

If you want to pass a string instead of bytes, you need to perform a manual UTF-8
encoding first. For example:

>>> print_chars('Hello World'.encode('utf-8'))
48 65 6c 6c 6f 20 57 6f 72 6c 64
>>>

For other extension tools (e.g., Swig, Cython), careful study is probably in order should
you decide to use them to pass strings to C code.
15.14. Passing Unicode Strings to C Libraries
Problem
You are writing an extension module that needs to pass a Python string to a C library
function that may or may not know how to properly handle Unicode.

Solution
There are many issues to be concerned with here, but the main one is that existing C
libraries won’t understand Python’s native representation of Unicode. Therefore, your
challenge is to convert the Python string into a form that can be more easily understood
by C libraries.
For the purposes of illustration, here are two C functions that operate on string data
and output it for the purposes of debugging and experimentation. One uses bytes pro‐
vided in the form char *, int, whereas the other uses wide characters in the form
wchar_t *, int:

void print_chars(char *s, int len) {
  int n = 0;

  while (n < len) {
    printf("%2x ", (unsigned char) s[n]);
    n++;
  }
  printf("\n");
}

void print_wchars(wchar_t *s, int len) {
  int n = 0;
  while (n < len) {
    printf("%x ", s[n]);
    n++;
  }
  printf("\n");
}

For the byte-oriented function print_chars(), you need to convert Python strings into
a suitable byte encoding such as UTF-8. Here is a sample extension function that does
this:

static PyObject *py_print_chars(PyObject *self, PyObject *args) {
  char *s;
  Py_ssize_t  len;

  if (!PyArg_ParseTuple(args, "s#", &s, &len)) {
    return NULL;
  }
  print_chars(s, len);
  Py_RETURN_NONE;
}

For library functions that work with the machine native wchar_t type, you can write
extension code such as this:

static PyObject *py_print_wchars(PyObject *self, PyObject *args) {
  wchar_t *s;
  Py_ssize_t  len;

  if (!PyArg_ParseTuple(args, "u#", &s, &len)) {
    return NULL;
  }
  print_wchars(s,len);
  Py_RETURN_NONE;
}

Here is an interactive session that illustrates how these functions work:

>>> s = 'Spicy Jalape\u00f1o'
>>> print_chars(s)
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f
>>> print_wchars(s)
53 70 69 63 79 20 4a 61 6c 61 70 65 f1 6f
>>>

15.14. Passing Unicode Strings to C Libraries 

Carefully observe how the byte-oriented function print_chars() is receiving UTF-8
encoded data, whereas print_wchars() is receiving the Unicode code point values.

Discussion
Before considering this recipe, you should first study the nature of the C library that
you’re accessing. For many C libraries, it might make more sense to pass bytes instead
of a string. To do that, use this conversion code instead:

static PyObject *py_print_chars(PyObject *self, PyObject *args) {
  char *s;
  Py_ssize_t  len;

  /* accepts bytes, bytearray, or other byte-like object */
  if (!PyArg_ParseTuple(args, "y#", &s, &len)) {
    return NULL;
  }
  print_chars(s, len);
  Py_RETURN_NONE;
}

If you decide that you still want to pass strings, you need to know that Python 3 uses an
adaptable string representation that is not entirely straightforward to map directly to C
libraries using the standard types char * or wchar_t * See PEP 393 for details. Thus,
to present string data to C, some kind of conversion is almost always necessary. The s#
and u# format codes to PyArg_ParseTuple() safely perform such conversions.
One potential downside is that such conversions cause the size of the original string
object to permanently increase. Whenever a conversion is made, a copy of the converted
data is kept and attached to the original string object so that it can be reused later. You
can observe this effect:

>>> import sys
>>> s = 'Spicy Jalape\u00f1o'
>>> sys.getsizeof(s)
87
>>> print_chars(s)
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f
>>> sys.getsizeof(s)
103
>>> print_wchars(s)
53 70 69 63 79 20 4a 61 6c 61 70 65 f1 6f
>>> sys.getsizeof(s)
163
>>>

For small amounts of string data, this might not matter, but if you’re doing large amounts
of  text  processing  in  extensions,  you  may  want  to  avoid  the  overhead.  Here  is  an
alternative implementation of the first extension function that avoids these memory
inefficiencies:

static PyObject *py_print_chars(PyObject *self, PyObject *args) {
  PyObject *obj, *bytes;
  char *s;
  Py_ssize_t   len;

  if (!PyArg_ParseTuple(args, "U", &obj)) {
    return NULL;
  }
  bytes = PyUnicode_AsUTF8String(obj);
  PyBytes_AsStringAndSize(bytes, &s, &len);
  print_chars(s, len);
  Py_DECREF(bytes);
  Py_RETURN_NONE;
}

Avoiding  memory  overhead  for  wchar_t  handling  is  much  more  tricky.  Internally,
Python stores strings using the most efficient representation possible. For example,
strings containing nothing but ASCII are stored as arrays of bytes, whereas strings con‐
taining characters in the range U+0000 to U+FFFF use a two-byte representation. Since
there isn’t a single representation of the data, you can’t just cast the internal array to
wchar_t * and hope that it works. Instead, a wchar_t array has to be created and text
copied into it. The "u#" format code to PyArg_ParseTuple() does this for you at the
cost of efficiency (it attaches the resulting copy to the string object).
If you want to avoid this long-term memory overhead, your only real choice is to copy
the Unicode data into a temporary array, pass it to the C library function, and then
deallocate the array. Here is one possible implementation:

static PyObject *py_print_wchars(PyObject *self, PyObject *args) {
  PyObject *obj;
  wchar_t *s;
  Py_ssize_t len;

  if (!PyArg_ParseTuple(args, "U", &obj)) {
    return NULL;
  }
  if ((s = PyUnicode_AsWideCharString(obj, &len)) == NULL) {
    return NULL;
  }
  print_wchars(s, len);
  PyMem_Free(s);
  Py_RETURN_NONE;
}

In this implementation, PyUnicode_AsWideCharString() creates a temporary buffer of
wchar_t characters and copies data into it. That buffer is passed to C and then released
afterward. As of this writing, there seems to be a possible bug related to this behavior,
as described at the Python issues page.

15.14. Passing Unicode Strings to C Libraries 

If, for some reason you know that the C library takes the data in a different byte encoding
than UTF-8, you can force Python to perform an appropriate conversion using exten‐
sion code such as the following:

static PyObject *py_print_chars(PyObject *self, PyObject *args) {
  char *s = 0;
  int   len;
  if (!PyArg_ParseTuple(args, "es#", "encoding-name", &s, &len)) {
    return NULL;
  }
  print_chars(s, len);
  PyMem_Free(s);
  Py_RETURN_NONE;
}

Last, but not least, if you want to work directly with the characters in a Unicode string,
here is an example that illustrates low-level access:

static PyObject *py_print_wchars(PyObject *self, PyObject *args) {
  PyObject *obj;
  int n, len;
  int kind;
  void *data;

  if (!PyArg_ParseTuple(args, "U", &obj)) {
    return NULL;
  }
  if (PyUnicode_READY(obj) < 0) {
    return NULL;
  }

  len = PyUnicode_GET_LENGTH(obj);
  kind = PyUnicode_KIND(obj);
  data = PyUnicode_DATA(obj);

  for (n = 0; n < len; n++) {
    Py_UCS4 ch = PyUnicode_READ(kind, data, n);
    printf("%x ", ch);
  }
  printf("\n");
  Py_RETURN_NONE;
}

In this code, the PyUnicode_KIND() and PyUnicode_DATA() macros are related to the
variable-width storage of Unicode, as described in PEP 393. The kind variable encodes
information about the underlying storage (8-bit, 16-bit, or 32-bit) and data points the
buffer. In reality, you don’t need to do anything with these values as long as you pass
them to the PyUnicode_READ() macro when extracting characters.
A few final words: when passing Unicode strings from Python to C, you should probably
try to make it as simple as possible. If given the choice between an encoding such as

UTF-8 or wide characters, choose UTF-8. Support for UTF-8 seems to be much more
common, less trouble-prone, and better supported by the interpreter. Finally, make sure
your review the documentation on Unicode handling. 
15.15. Converting C Strings to Python
Problem
You want to convert strings from C to Python bytes or a string object.

Solution
For C strings represented as a pair char *, int, you must decide whether or not you
want the string presented as a raw byte string or as a Unicode string. Byte objects can
be built using Py_BuildValue() as follows:

char *s;     /* Pointer to C string data */
int   len;   /* Length of data */

/* Make a bytes object */
PyObject *obj = Py_BuildValue("y#", s, len);

If you want to create a Unicode string and you know that s points to data encoded as
UTF-8, you can use the following:

PyObject *obj = Py_BuildValue("s#", s, len);

If s is encoded in some other known encoding, you can make a string using PyUni
code_Decode() as follows:

PyObject *obj = PyUnicode_Decode(s, len, "encoding", "errors");

/* Examples /*
obj = PyUnicode_Decode(s, len, "latin-1", "strict");
obj = PyUnicode_Decode(s, len, "ascii", "ignore");

If you happen to have a wide string represented as a wchar_t *, len pair, there are a
few options. First, you could use Py_BuildValue() as follows:

wchar_t *w;    /* Wide character string */
int len;       /* Length */

PyObject *obj = Py_BuildValue("u#", w, len);

Alternatively, you can use PyUnicode_FromWideChar():

PyObject *obj = PyUnicode_FromWideChar(w, len);

For wide character strings, no interpretation is made of the character data—it is assumed
to be raw Unicode code points which are directly converted to Python.

15.15. Converting C Strings to Python 

Discussion
Conversion of strings from C to Python follow the same principles as I/O. Namely, the
data from C must be explicitly decoded into a string according to some codec. Common
encodings include ASCII, Latin-1, and UTF-8. If you’re not entirely sure of the encoding
or the data is binary, you’re probably best off encoding the string as bytes instead.
When making an object, Python always copies the string data you provide. If necessary,
it’s up to you to release the C string afterward (if required). Also, for better reliability,
you should try to create strings using both a pointer and a size rather than relying on
NULL-terminated data.
15.16. Working with C Strings of Dubious Encoding
Problem
You are converting strings back and forth between C and Python, but the C encoding
is of a dubious or unknown nature. For example, perhaps the C data is supposed to be
UTF-8, but it’s not being strictly enforced. You would like to write code that can handle
malformed data in a graceful way that doesn’t crash Python or destroy the string data
in the process.

Solution
Here is some C data and a function that illustrates the nature of this problem:

/* Some dubious string data (malformed UTF-8) */
const char *sdata = "Spicy Jalape\xc3\xb1o\xae";
int slen = 16;

/* Output character data */
void print_chars(char *s, int len) {
  int n = 0;
  while (n < len) {
    printf("%2x ", (unsigned char) s[n]);
    n++;
  }
  printf("\n");
}

In this code, the string sdata contains a mix of UTF-8 and malformed data. Neverthe‐
less, if a user calls print_chars(sdata, slen) in C, it works fine.
Now suppose you want to convert the contents of sdata into a Python string. Further
suppose you want to later pass that string to the print_chars() function through an
extension. Here’s how to do it in a way that exactly preserves the original data even
though there are encoding problems:

/* Return the C string back to Python */
static PyObject *py_retstr(PyObject *self, PyObject *args) {
  if (!PyArg_ParseTuple(args, "")) {
    return NULL;
  }
  return PyUnicode_Decode(sdata, slen, "utf-8", "surrogateescape");
}

/* Wrapper for the print_chars() function */
static PyObject *py_print_chars(PyObject *self, PyObject *args) {
  PyObject *obj, *bytes;
  char *s = 0;
  Py_ssize_t   len;

  if (!PyArg_ParseTuple(args, "U", &obj)) {
    return NULL;
  }

  if ((bytes = PyUnicode_AsEncodedString(obj,"utf-8","surrogateescape"))
        == NULL) {
    return NULL;
  }
  PyBytes_AsStringAndSize(bytes, &s, &len);
  print_chars(s, len);
  Py_DECREF(bytes);
  Py_RETURN_NONE;
}

If you try these functions from Python, here’s what happens:

>>> s = retstr()
>>> s
'Spicy Jalapeño\udcae'
>>> print_chars(s)
53 70 69 63 79 20 4a 61 6c 61 70 65 c3 b1 6f ae
>>>

Careful observation will reveal that the malformed string got encoded into a Python
string without errors, and that when passed back into C, it turned back into a byte string
that exactly encoded the same bytes as the original C string.

Discussion
This recipe addresses a subtle, but potentially annoying problem with string handling
in extension modules. Namely, the fact that C strings in extensions might not follow the
strict Unicode encoding/decoding rules that Python normally expects. Thus, it’s possible
that some malformed C data would pass to Python. A good example might be C strings
associated with low-level system calls such as filenames. For instance, what happens if
a  system  call  returns  a  broken  string  back  to  the  interpreter  that  can’t  be  properly
decoded.

15.16. Working with C Strings of Dubious Encoding 

Normally, Unicode errors are often handled by specifying some sort of error policy, such
as strict, ignore, replace, or something similar. However, a downside of these policies
is that they irreparably destroy the original string content. For example, if the malformed
data in the example was decoded using one of these polices, you would get results such
as this:

>>> raw = b'Spicy Jalape\xc3\xb1o\xae'
>>> raw.decode('utf-8','ignore')
'Spicy Jalapeño'
>>> raw.decode('utf-8','replace')
'Spicy Jalapeño?'
>>>

The surrogateescape error handling policies takes all nondecodable bytes and turns
them into the low-half of a surrogate pair (\udcXX where XX is the raw byte value). For
example:

>>> raw.decode('utf-8','surrogateescape')
'Spicy Jalapeño\udcae'
>>>

Isolated low surrogate characters such as \udcae never appear in valid Unicode. Thus,
this string is technically an illegal representation. In fact, if you ever try to pass it to
functions that perform output, you’ll get encoding errors:

>>> s = raw.decode('utf-8', 'surrogateescape')
>>> print(s)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
UnicodeEncodeError: 'utf-8' codec can't encode character '\udcae'
in position 14: surrogates not allowed
>>>

However, the main point of allowing the surrogate escapes is to allow malformed strings
to pass from C to Python and back into C without any data loss. When the string is
encoded using surrogateescape again, the surrogate characters are turned back into
their original bytes. For example:

>>> s
'Spicy Jalapeño\udcae'
>>> s.encode('utf-8','surrogateescape')
b'Spicy Jalape\xc3\xb1o\xae'
>>>

As a general rule, it’s probably best to avoid surrogate encoding whenever possible—
your code will be much more reliable if it uses proper encodings. However, sometimes
there are situations where you simply don’t have control over the data encoding and
you aren’t free to ignore or replace the bad data because other functions may need to
use it. This recipe shows how to do it.

As a final note, many of Python’s system-oriented functions, especially those related to
filenames, environment variables, and command-line options, use surrogate encoding.
For example, if you use a function such as os.listdir() on a directory containing a
undecodable  filename,  it  will  be  returned  as  a  string  with  surrogate  escapes.  See
Recipe 5.15 for a related recipe.
PEP 383 has more information about the problem addressed by this recipe and surro
gateescape error handling.
15.17. Passing Filenames to C Extensions
Problem
You need to pass filenames to C library functions, but need to make sure the filename
has been encoded according to the system’s expected filename encoding.

Solution
To write an extension function that receives a filename, use code such as this:

static PyObject *py_get_filename(PyObject *self, PyObject *args) {
  PyObject *bytes;
  char *filename;
  Py_ssize_t len;
  if (!PyArg_ParseTuple(args,"O&", PyUnicode_FSConverter, &bytes)) {
    return NULL;
  }
  PyBytes_AsStringAndSize(bytes, &filename, &len);
  /* Use filename */
  ...

  /* Cleanup and return */
  Py_DECREF(bytes)
  Py_RETURN_NONE;
}

If you already have a PyObject * that you want to convert as a filename, use code such
as the following:

PyObject *obj;    /* Object with the filename */
PyObject *bytes;
char *filename;
Py_ssize_t len;

bytes = PyUnicode_EncodeFSDefault(obj);
PyBytes_AsStringAndSize(bytes, &filename, &len);
/* Use filename */
...

15.17. Passing Filenames to C Extensions 

/* Cleanup */
Py_DECREF(bytes);

If you need to return a filename back to Python, use the following code:

/* Turn a filename into a Python object */

char *filename;       /* Already set */
int   filename_len;   /* Already set */

PyObject *obj = PyUnicode_DecodeFSDefaultAndSize(filename, filename_len);

Discussion
Dealing with filenames in a portable way is a tricky problem that is best left to Python.
If you use this recipe in your extension code, filenames will be handled in a manner that
is consistent with filename handling in the rest of Python. This includes encoding/
decoding of bytes, dealing with bad characters, surrogate escapes, and other complica‐
tions.
15.18. Passing Open Files to C Extensions
Problem
You have an open file object in Python, but need to pass it to C extension code that will
use the file.

Solution
To convert a file to an integer file descriptor, use PyFile_FromFd(), as shown:

PyObject *fobj;     /* File object (already obtained somehow) */
int fd = PyObject_AsFileDescriptor(fobj);
if (fd < 0) {
   return NULL;
}

The resulting file descriptor is obtained by calling the fileno() method on fobj. Thus,
any object that exposes a descriptor in this manner should work (e.g., file, socket, etc.).
Once you have the descriptor, it can be passed to various low-level C functions that
expect to work with files.
If  you  need  to  convert  an  integer  file  descriptor  back  into  a  Python  object,  use
PyFile_FromFd() as follows:

int fd;     /* Existing file descriptor (already open) */
PyObject *fobj = PyFile_FromFd(fd, "filename","r",-1,NULL,NULL,NULL,1);

The arguments to PyFile_FromFd() mirror those of the built-in open() function. NULL
values simply indicate that the default settings for the encoding, errors, and newline
arguments are being used.

Discussion
If you are passing file objects from Python to C, there are a few tricky issues to be
concerned about. First, Python performs its own I/O buffering through the io module.
Prior to passing any kind of file descriptor to C, you should first flush the I/O buffers
on the associated file objects. Otherwise, you could get data appearing out of order on
the file stream.
Second, you need to pay careful attention to file ownership and the responsibility of
closing the file in particular. If a file descriptor is passed to C, but still used in Python,
you need to make sure C doesn’t accidentally close the file. Likewise, if a file descriptor
is being turned into a Python file object, you need to be clear about who is responsible
for closing it. The last argument to PyFile_FromFd() is set to 1 to indicate that Python
should close the file.
If you need to make a different kind of file object such as a FILE * object from the C
standard I/O library using a function such as fdopen(), you’ll need to be especially
careful. Doing so would introduce two completely different I/O buffering layers into
the I/O stack (one from Python’s io module and one from C stdio). Operations such
as fclose() in C could also inadvertently close the file for further use in Python. If given
a choice, you should probably make extension code work with the low-level integer file
descriptors as opposed to using a higher-level abstraction such as that provided by
<stdio.h>.
15.19. Reading File-Like Objects from C
Problem
You want to write C extension code that consumes data from any Python file-like object
(e.g., normal files, StringIO objects, etc.).

Solution
To consume data on a file-like object, you need to repeatedly invoke its read() method
and take steps to properly decode the resulting data.
Here is a sample C extension function that merely consumes all of the data on a file-like
object and dumps it to standard output so you can see it:

#define CHUNK_SIZE 8192

15.19. Reading File-Like Objects from C 

/* Consume a "file-like" object and write bytes to stdout */
static PyObject *py_consume_file(PyObject *self, PyObject *args) {
  PyObject *obj;
  PyObject *read_meth;
  PyObject *result = NULL;
  PyObject *read_args;

  if (!PyArg_ParseTuple(args,"O", &obj)) {
    return NULL;
  }

  /* Get the read method of the passed object */
  if ((read_meth = PyObject_GetAttrString(obj, "read")) == NULL) {
    return NULL;
  }

  /* Build the argument list to read() */
  read_args = Py_BuildValue("(i)", CHUNK_SIZE);
  while (1) {
    PyObject *data;
    PyObject *enc_data;
    char *buf;
    Py_ssize_t len;

    /* Call read() */
    if ((data = PyObject_Call(read_meth, read_args, NULL)) == NULL) {
      goto final;
    }

    /* Check for EOF */
    if (PySequence_Length(data) == 0) {
      Py_DECREF(data);
      break;
    }

    /* Encode Unicode as Bytes for C */
    if ((enc_data=PyUnicode_AsEncodedString(data,"utf-8","strict"))==NULL) {
      Py_DECREF(data);
      goto final;
    }

    /* Extract underlying buffer data */
    PyBytes_AsStringAndSize(enc_data, &buf, &len);

    /* Write to stdout (replace with something more useful) */
    write(1, buf, len);

    /* Cleanup */
    Py_DECREF(enc_data);
    Py_DECREF(data);
  }
  result = Py_BuildValue("");

 final:
  /* Cleanup */
  Py_DECREF(read_meth);
  Py_DECREF(read_args);
  return result;
}

To test the code, try making a file-like object such as a StringIO instance and pass it in:

>>> import io
>>> f = io.StringIO('Hello\nWorld\n')
>>> import sample
>>> sample.consume_file(f)
Hello
World
>>>

Discussion
Unlike a normal system file, a file-like object is not necessarily built around a low-level
file descriptor. Thus, you can’t use normal C library functions to access it. Instead, you
need to use Python’s C API to manipulate the file-like object much like you would in
Python.
In the solution, the read() method is extracted from the passed object. An argument
list is built and then repeatedly passed to PyObject_Call() to invoke the method. To
detect end-of-file (EOF), PySequence_Length() is used to see if the returned result has
zero length.
For all I/O operations, you’ll need to concern yourself with the underlying encoding
and distinction between bytes and Unicode. This recipe shows how to read a file in text
mode and decode the resulting text into a bytes encoding that can be used by C. If you
want to read the file in binary mode, only minor changes will be made. For example:

...
    /* Call read() */
    if ((data = PyObject_Call(read_meth, read_args, NULL)) == NULL) {
      goto final;
    }

    /* Check for EOF */
    if (PySequence_Length(data) == 0) {
      Py_DECREF(data);
      break;
    }
    if (!PyBytes_Check(data)) {
      Py_DECREF(data);
      PyErr_SetString(PyExc_IOError, "File must be in binary mode");
      goto final;
    }

15.19. Reading File-Like Objects from C 

    /* Extract underlying buffer data */
    PyBytes_AsStringAndSize(data, &buf, &len);
    ...

The trickiest part of this recipe concerns proper memory management. When working
with PyObject * variables, careful attention needs to be given to managing reference
counts and cleaning up values when no longer needed. The various Py_DECREF() calls
are doing this.
The recipe is written in a general-purpose manner so that it can be adapted to other file
operations, such as writing. For example, to write data, merely obtain the  write()
method of the file-like object, convert data into an appropriate Python object (bytes or
Unicode), and invoke the method to have it written to the file.
Finally,  although  file-like  objects  often  provide  other  methods  (e.g.,  readline(),
read_into()), it is probably best to just stick with the basic read() and write() meth‐
ods for maximal portability. Keeping things as simple as possible is often a good policy
for C extensions.
15.20. Consuming an Iterable from C
Problem
You want to write C extension code that consumes items from any iterable object such
as a list, tuple, file, or generator.

Solution
Here is a sample C extension function that shows how to consume the items on an
iterable:

static PyObject *py_consume_iterable(PyObject *self, PyObject *args) {
  PyObject *obj;
  PyObject *iter;
  PyObject *item;

  if (!PyArg_ParseTuple(args, "O", &obj)) {
    return NULL;
  }
  if ((iter = PyObject_GetIter(obj)) == NULL) {
    return NULL;
  }
  while ((item = PyIter_Next(iter)) != NULL) {
    /* Use item */
    ...
    Py_DECREF(item);
  }

  Py_DECREF(iter);
  return Py_BuildValue("");
}

Discussion
The code in this recipe mirrors similar code in Python. The PyObject_GetIter() call
is the same as calling iter() to get an iterator. The PyIter_Next() function invokes
the next method on the iterator returning the next item or NULL if there are no more
items. Make sure you’re careful with memory management—Py_DECREF() needs to be
called on both the produced items and the iterator object itself to avoid leaking memory.
15.21. Diagnosing Segmentation Faults
Problem
The interpreter violently crashes with a segmentation fault, bus error, access violation,
or other fatal error. You would like to get a Python traceback that shows you where your
program was running at the point of failure.

Solution
The  faulthandler module can be used to help you solve this problem. Include the
following code in your program:

import faulthandler
faulthandler.enable()

Alternatively, run Python with the -Xfaulthandler option such as this:

bash % python3 -Xfaulthandler program.py

Last, but not least, you can set the PYTHONFAULTHANDLER environment variable.
With faulthandler enabled, fatal errors in C extensions will result in a Python trace‐
back being printed on failures. For example:

    Fatal Python error: Segmentation fault

    Current thread 0x00007fff71106cc0:
      File "example.py", line 6 in foo
      File "example.py", line 10 in bar
      File "example.py", line 14 in spam
      File "example.py", line 19 in <module>
    Segmentation fault

Although this won’t tell you where in the C code things went awry, at least it can tell you
how it got there from Python.

15.21. Diagnosing Segmentation Faults 

Discussion
The faulthandler will show you the stack traceback of the Python code executing at
the time of failure. At the very least, this will show you the top-level extension function
that was invoked. With the aid of pdb or other Python debugger, you can investigate the
flow of the Python code leading to the error.
faulthandler will not tell you anything about the failure from C. For that, you will
need to use a traditional C debugger, such as gdb. However, the information from the
faulthandler traceback may give you a better idea of where to direct your attention.
It should be noted that certain kinds of errors in C may not be easily recoverable. For
example, if a C extension trashes the stack or program heap, it may render faulthan
dler inoperable and you’ll simply get no output at all (other than a crash). Obviously,
your mileage may vary.

APPENDIX A
Further Reading

There are a large number of books and online resources available for learning and
programming Python. However, if like this book, your focus is on the use of Python 3,
finding reliable information is made a bit more difficult simply due to the sheer volume
of existing material written for earlier Python versions.
In this appendix, we provide a few selected links to material that may be particularly
useful in the context of Python 3 programming and the recipes contained in this book.
This is by no means an exhaustive list of resources, so you should definitely check to
see if new titles or more up-to-date editions of these books have been published.
Online Resources
http://docs.python.org

It goes without saying that Python’s own online documentation is an excellent re‐
source if you need to delve into the finer details of the language and modules. Just
make sure you’re looking at the documentation for Python 3 and not earlier ver‐
sions.

http://www.python.org/dev/peps

Python Enhancement Proposals (PEPs) are invaluable if you want to understand
the motivation for adding new features to the Python language as well as subtle
implementation details. This is especially true for some of the more advanced lan‐
guage features. In writing this book, the PEPs were often more useful than the
official documentation.

http://pyvideo.org

This is a large collection of video presentations and tutorials from past PyCon con‐
ferences, user group meetings, and more. It can be an invaluable resource for learn‐
ing about modern Python development. Many of the videos feature Python core
developers talking about the new features being added in Python 3.

http://code.activestate.com/recipes/langs/python

The ActiveState Python recipes site has long been a resource for finding the solution
to thousands of specific programming problems. As of this writing, it contains
approximately 300 recipes specific to Python 3. You’ll find that many of its recipes
either expand upon topics covered in this book or focus on more narrowly defined
tasks. As such, it’s a good companion.

http://stackoverflow.com/questions/tagged/python

Stack Overflow currently has more than 175,000 questions tagged as Python-related
(and almost 5000 questions specific to Python 3). Although the quality of the ques‐
tions and answers varies, there is a lot of good material to be found.

Books for Learning Python
The following books provide an introduction to Python with a focus on Python 3:

• Learning Python, 4th Edition, by Mark Lutz, O’Reilly & Associates (2009).
• The Quick Python Book, 2nd Edition, by Vernon Ceder, Manning (2010).
• Python Programming for the Absolute Beginner, 3rd Edition, by Michael Dawson,

Course Technology PTR (2010).

• Beginning Python: From Novice to Professional, 2nd Edition, by Magnus Lie Het‐

land, Apress (2008).

• Programming in Python 3, 2nd Edition, by Mark Summerfield, Addison-Wesley

(2010).

Advanced Books
The following books provide more advanced coverage and include Python 3 topics:
• Programming Python, 4th Edition, by Mark Lutz, O’Reilly & Associates (2010).
• Python Essential Reference, 4th Edition, by David Beazley, Addison-Wesley (2009).
• Core Python Applications Programming, 3rd Edition, by Wesley Chun, Prentice Hall

(2012).

• The Python Standard Library by Example, by Doug Hellmann, Addison-Wesley

(2011).

• Python 3 Object Oriented Programming, by Dusty Phillips, Packt Publishing (2010).
• Porting to Python 3, by Lennart Regebro, CreateSpace (2011), http://python3port

ing.com.

666 

|  Appendix A: Further Reading


Index

_ (single underscore)

avoiding clashes with reserved words, 251
naming convention and, 250

__ (double underscore), naming conventions

and, 250

A
abc module, 274
abspath() (os.path module), 551
abstract base classes, 274–276

collections module and, 283–286
predefined, 276

abstract syntax tree, 388–392
abstraction, gratuitous, 593
@abstractmethod decorator (abc module), 275
accepting script via input files, 539
accessor functions, 238–241

adjusting decorators with, 336–339
ACCESS_COPY (mmap module), 155
ACLs, 548
actor model, 516–520
addHandler() operation (logging module), 559
add_argument() method (ArgumentParser

module), 543, 544

Advanced Programming in the Unix Environ‐

ment, 2e (Stevens, Rago), 538

algorithms, 1–35

filtering sequence elements, 26–28

\$ (end-marker) in regular expressions, 44

We’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.


667

Symbols
!r formatting code, 244
% (percent) operator, 58, 556
format() function vs., 88

* (star) operator

EBNFs and, 70

**kwargs, 218

decorators and, 331
enforcing signature on, 364–367
help function and, 219
wrapped functions and, 353

decorators and, 331
enforcing signature on, 364–367
wrapped functions and, 353

*args, 217

+ (plus) operator, 48, 59
-O option (interpreter), 343
-OO option (interpreter), 343
-W (warnings), 584

all option, 583
error option, 584
ignore option, 584
. (dot) operator, 49, 591
< (less than) operator, date comparisons with,

== operator, 462
? (question mark) modifier in regular expres‐

108

sions, 48

finding most frequent item (sequences), 20–

21

limited history, keeping, 5–7
sorting for largest/smallest N items, 7–8
unpacking, 1–5

anonymous functions
defining, 224–225
variables, capturing, 225–227

Apache web server, 451
append() method (ElementTree module), 193
%apply directive (Swig), 631
archives

creating, 549
formats of, 550
unpacking, 549

argparse module, 541, 543, 544
.args attribute (Exceptions), 579
ArgumentParser instance, 543
arguments

annotations, 220
cleaning up in C extensions, 623
multiple-dispatch, implementing with, 376–

382

arrays

calculating with large numerical, 97–100
high-performance with Cython, 638–642
in C extensions, 603
large, sending/receiving, 481–483
NumPy and, 97–100
operating on, with extension functions, 609–

612

readinto() method and, 152
writing to files from, 147

assertRaises() method (unittest module), 570,

assertRaisesRegex() method (unittest module),

571

571

ast module, 78, 311, 388–392
asynchronous processing, 232–235
attrgetter() function (operator module), 24
authentication

encryption vs., 463
hmac module, 461–463
passwords, prompting at runtime, 544
requests package and, 439
simple, 461–463
testing with httpbin.org, 441

B
b16decode() function (base64 module), 199

binascii functions vs., 198

b16encode() function (base64 module), 199

binascii functions vs., 198

base64 module, 199

binascii module vs., 198

BaseException class, 577–579
except statements and, 576

443

557

203

BaseRequestHandler (socketserver module),

basicConfig() function (logging module), 556–

bin() function, 89–90
binary data

arrays of structures, reading/writing, 199–

encoding with base64 module, 199
memory mapping, 153–156
nested records, reading, 203–213
reading into mutable buffers, 152–153
reading/writing, 145–147
unpack_from() method and, 202
variable-sized records, reading, 203–213

binary integers, 89–90
binascii module, 197–198
bind() method (Signature objects), 344, 364
bind_partial() method (Signature objects), 343
bit_length() method (int type), 92
BNF, 69
boundscheck() decorator (Cython), 641
buffer attribute (files), 165
Buffer Protocol, 147, 609
BufferedWriter object (io module), 164
built-ins

containers, 593
exception class, 578
methods, overriding, 256

byte strings

as return value of read(), 146
encoding with base64, 199
indexing of, 79
performing text operations on, 78–81
printing, 80
writing to text files, 165

bytes, interpreting as text, 546
BytesIO() class (io module), 148–149, 570
bz2 compression format, 550

668 

bz2 module, 149–151

compresslevel keyword, 150

C
C APIs, 612–614
C extension programming, 566, 597

APIs, defining/exporting, 612–614
aquiring GIL in, 624
capsule objects in, 612–614
ctypes module and, 599–605
Cython, wrapping code in, 632–638
file-like objects, reading, 659–662
filenames, passing to, 657–658
function pointers, converting to callables,

643–644

GIL and, 515–516
GIL, releasing in, 625
iterables from C, consuming, 662–663
loading modules, 601
modules, building, 607
null-terminated strings, passing, 644–648
opaque pointers in, 612–614
open files, passing to, 658–659
operating on arrays with, 609–612
Python callables in, 619–624
Python/C threads, mixing, 625–626
segmentation faults and, 663–664
strings, converting to Python objects, 653–

654

Swig, wrapping with, 627–631
Unicode strings, passing, 648–653
unknown string type and, 654–657
writing, modules, 605–609

C structures, 147
calendar module, 108
__call__() method (metaclasses), 347–349, 356
callback functions

carrying extra state with, 232–235
inlining, 235–238

capsule objects, 612–614

C APIs, defining/exporting with, 612–614

Celery, 457
center() method (str type), 57
certificate authorities, 468
CFFI, 605
CFUNCTYPE instances (ctypes module), 643
chain() method (itertools module), 131–132
chained exceptions, 581, 582

ChainMap class (collections module), 33–35

update() method vs., 35

chdir() function (os module), 537
check_output() function (subprocess class), 546
Chicago, Illinois, 186, 214
choice() function (random module), 102
__class__ attribute, 302–304
class decorators, 279, 355–356

data models and, 281
mixin classes vs., 281
mixins and, 298

classes, 243–327

abstract base classes, 274–276
attribute definition order, 359–362
caching instances of, 323
closures vs., 238–241
coding conventions in, 367–370
constructors, multiple, 291–292
containers, custom, 283–286
data models, implementing, 277–283
decorators, 279
decorators, defining in, 345–347
defining decorators as, 347–349
delegating attributes, 287–291
descriptors and, 264–267
extending properties in subclasses, 260–264
extending with mixins, 294–299
implementing state for objects/machines,

299–305

375

inheritance, implementation of, 258
initializing members at definition time, 374–

initializing of data structures in, 270–274
lazy attributes and, 267
mixins, 260, 294–299
private data in, 250–251
replacing with functions, 231–232
statements, metaclass keyword in, 362–364
super() function and, 256–260
supporting comparison operations, 321–323
type system, implementing, 277–283
__init__() method, bypassing, 293–294

@classmethod decorator, 330

decorating class methods with, 350–352
__func__ attribute and, 334

client module (http package), 440
clock() function (time class), 561
close() method (generators), 531
closefd argument (open() function), 166

Index 

closures

accessing variables inside, 238–241
capturing state with, 233–234
classes vs., 238–241
converting classes to functions with, 231
nonlocal declarations and, 239

cmath module, 93
code readability and string templates, 453
coding conventions, 367–370
collections module

ChainMap class, 33–35
Counter class, 20–21
defaultdict class, 11
deque class, 5–7
implementing custom containers with, 283–

286

namedtuple() function, 30
OrderedDict class, 12–13

combinations() function (itertools module), 126
combinations_with_replacement() function

(itertools module), 126

combining() function (unicodedata module), 51
command-line options, parsing, 539, 541, 542
compare_digest() function (hmac module), 462
comparison operations, 321–323
compile() function (ast module), 390
compile() function (re module), 49
complex() function, 92
complex-valued math, 92–94
compress() function (itertools module), 28
compressed files, reading/writing, 149–151

specifying level of, 150

concurrency, 485–538

actor model and, 516–520
coroutines, 524–531
event-driven I/O and, 479
GIL, 513–516
parallel programming, 509–513
polling thread queues, 531–534
threads, 485–488
with generators, 524–531

Condition object (threading module), 489
conditionals and NaN values, 95
config file

Python code vs., 554
Python source file vs., 553
ConfigParser module, 552–555
configuration files, 552
connect() function (sqlite3 module), 196

connection (multiprocessing module), 456–458
constructors, defining multiple, 291–292
containers, custom, 283–286

iterating over items in separate, 131–132
__iter__() method and, 114–115

context managers

defining, 384–385
use instance as, 540

context-management protocols, 246–248
contextlib module, 238
@contextmanager decorator (contextlib mod‐

ule), 238, 385

contextmanager module, 248
cookies, 439
copy2() function (shutil module), 548
copying directories/files, 547
copytree() function (shutil class), 548–549
coroutines, 524–531

capturing state with, 233
inlining callback functions with, 235

Counter objects (collections module), 20–21
country_timezones dictionary (pytz module),

112

cProfile module, 587
CPU-bound programs, 514
CPUs, restricting use of, 561
critical() function (logging module), 556
cryptography, 103
CSV files, reading/writing, 175–179
csv module, 175–179
ctypes module, 599–605

alternatives to, 604
memory addresses, wrapping, 643–644

custom exceptions, 578
Cython, 604

array handling with, 612
high-performance arrays with, 638–642
memoryviews in, 640
releasing the GIL, 636
setup.py file, 633, 639
wrapping existing code in, 632–638

D
daemon processes (Unix), 534–538
daemonic threads, 486
data

pickle module and, 171–174
serializing, 171–174

670 

transforming/reducing simultaneously, 32–

33

data analysis, 178
data encapsulation, 250–251
data encoding

CSV files, reading/writing, 175–179
for RPCs, 460
hexadecimal digits, 197–198
JSON, reading/writing, 179–183
with base64 module, 199
XML, extracting data from, 183–186

data models, 277–283
data processing

dictionaries, converting to XML, 189–191
nested binary records, reading, 203–213
parsing large XML files, 186–189
relational databases and, 195–197
statistics, 214–216
summarizing, 214–216
variable-sized binary records, reading, 203–

213

XML namespaces, 193–195
XML, parsing/modifying/rewriting, 191–193

data structures, 1–35, 593

cyclic, managing memory in, 317–320
dictionaries

grouping records by field, 24–26
sorting by common keys, 21–23

heaps, 7–11
initializations of, simplifying, 270–274
multidicts, 11–12

database API (Python), 195–197

strings and, 197

datagrams, 445
date calculations, 106–107
datetime module, 104–112

databases and, 197
date calculations with, 106–107
replace() method, 108
strptime() method, 109
time conversions, 104–105
time zones and, 110
timedelta object, 104

dateutil module, 105
deadlock avoidance, 503
deadlocks, 500–503

dining philosophers problem, 503
watchdog timers and, 503

__debug__ variable, 343

debug() function (logging module), 556
debugger, launching, 585
debugging, 584
decimal module, 84–86
databases and, 197
formatting output of, 88

decode() method (str type), 56
decorators, 329–356

adding function arguments with, 352–354
adjustable attributes for, 336–339
class methods, applying to, 350–352
defining as class, 347–349
defining in classes, 345–347
forcing type checking with, 341–345
multiple-dispatch, implementing with, 381
optional arguments for, 339–341
patching classes with, 355–356
preserving function metadata in, 331–333
static methods, applying to, 350–352
unwrapping, 333–334
with arguments, 334–336
wrappers as, 329–331
__wrapped__ attribute, 332

deepcopy() function (copy module), 594
defaultdict object (collections module), 11

groupby() function vs., 26
ordinary dictionary vs., 12

__delattr__() method (delegation), 290
delegation

inheritance vs., 289
of attributes, 287–291

__delete__() method (descriptors), 265
DeprecationWarning argument (warn() func‐

deque class (collections module), 5–7

tion), 583

appending, 6
popping, 6

descriptors

creating, 264–267
data models, implementing, 277–283
extending, 263
lazy attributes and, 267–270
type system, implementing, 277–283

deserializing data, 293–294
Design Patterns: Elements of Reusable Object-
Oriented Software (Gamma, Helm, Johnson,
and Vlissides), 305

dest argument (add_argument() method), 544
detach() method, 164

Index 

dict() function, 29, 594
dictionaries

comparing, 15–16
converting to XML, 189–191
Counter objects and, 21
defaultdict vs., 12
grouping records based on field, 24–26
JSON support for, 179
keeping in order, 12–13
multiple values for a single key in, 11–12
removing duplicates from, 17
sorting, 13–15
sorting list of, by common key, 21–23
subsets, extracting, 28–29
dictionary comprehension, 29
dining philosophers problem (deadlocks), 503
directories

as runnable scripts, 407–408
copying, 547
moving, 547
temporary, 167–170

directory listings, 158–159
dis module, 392–395
discarding vs. filtering iterables, 124
distributed systems and property calls, 255
domain sockets (Unix), 457, 470

connecting interpreters with, 472

DOTALL flag (re module), 49
dropwhile() function (itertools module), 123
dump() function (pickle module), 171
dumps() function (json module), 179

indent argument, 181
sort_keys argument, 182

dumps() function (pickle module), 171
dup2() function (os module), 537

E
EBNF, 69
ElementTree module (xml.etree), 66, 183–186

creating XML documents with, 189–191
iterparse() function, 188
parse() function, 185
parse_and_remove() function, 189
parsing namespaces and, 194
parsing/modifying/rewriting files with, 191–

193

empty() method (queue module), 496
encode() method (str type), 56

encoding text data, 141–144

changing in open files, 163–165
passing to C libraries, 648–653
pickling and, 174

encryption vs. authentication, 463
end argument (print() function), 144
endswith() method (str type), 38

pattern matching with, 42

__enter__() method (with statements), 246–248
enumerate() function, 127–128
environ argument (WSGI), 451
error messages, terminating program with, 540
error() function (logging module), 556
escape() function (html module), 65
escape() function (xml.sax.saxutils module), 191
Event object (threading module), 488–491
event-driven I/O, 475–481
except statement, 576, 580

chained exceptions and, 581
reraising exceptions caught by, 582

Exception class, 579
handler for, 576

exceptions

catching all, 576
creating custom, 578
handling multiple, 574
raising in response to another exception, 580
reraising, 582
SystemExit, 540
testing for, 570
with statements and, 247

exec() function, 386–388
execve() function (os module), 546
__exit__() method (with statements), 246–248
@expectedFailure decorator, 574
exponential notation, 87
%extend directive (Swig), 630
external command
executing, 545
getting output, 545

F
factory functions, 323
faulthandler module, 663–664
fdel attribute (properties), 253
fget attribute (properties), 253
FieldStorage() class (cgi module), 452
file descriptor

passing between processes, 470–475

672 

wrapping in object, 166

file-like objects, reading from C, 659–662
FileInput class, 540

helper methods, 540

fileinput module (built-in), 539
FileIO class (io module), 164
FileNotFoundError exception, 575
files, 141–174

accepting scripts via, 539
binary data, reading/writing, 145–147
bypassing name encoding, 160–161
bytes, writing to text, 165
changing encoding in open, 163–165
compressed, reading/writing, 149–151
copying, 547
creating new, 147–148
defining modules in multiple, 401–404
detach() method, 164
directory listings, getting, 158–159
finding by name, 550
getfilesystemencoding() function (sys mod‐

ule), 160–161

headers, unpacking, 205
iterating over fixed-sized records in, 151
line ending, changing, 144–145
manipulating pathnames of, 156–157
memory mapping, 153–156
moving, 547
mutable buffers, reading into, 152–153
names, passing to C extensions, 657–658
newlines, 142
open() function, 141–144
open, passing to C extensions, 658–659
path module (os), 156–157
printing bad filenames for, 161–163
printing to, 144
reading, 408–409
readinto() method of, 147
saving objects to, 171–174
separator character, changing, 144–145
temporary, 167–170
testing for existence of, 157–158
text data from, 141–144
unpacking, 2
wrapping descriptors in objects, 166
x mode of open() function, 147–148

files list, 551
filesystem, byte strings and, 80
fill() function (textwrap module), 65

filter() function, 27
filtering

discarding iterables vs., 124
normalization of Unicode text and, 51
sequence elements, 26–28
token streams, 68
find Unix utility, 551
find() method (str type)

pattern matching with, 42
search/replace text and, 46

findall() method (re module), 43
finditer() method (re module), 44
find_library() function (ctypes.util module),

601
float()

42

for loops

decimal arithmetic and, 84–86
format() function (built-in), 87–88
j suffix for, 92

fnmatch module, 40–42
fnmatch() function (fnmatch module), 40–42
fnmatchcase() function (fnmatch module), 40–

consuming iterators manually vs., 113–114
generators and, 115–117

fork() function (os module), 537
ForkingTCPServer objects (socketserver mod‐

ForkingUDPServer objects (socketserver mod‐

ule), 442

ule), 446

format() function (built-in )

and non-decimal integers, 89
floats, specifying precision with, 84
formatting output with, 244

format() function (built-in), 87–88
format() method (str type), 57
customizing string, 245–246
interpolating variables, 61–64
__format__() method, 245–246
format_map() method (str type), 62
fpectl module, 95
fractions module, 96–96
frame hacking, 373
from module import * statement (modules), 398

reload() function and, 406

from module import name, 591
fromkeys() method (dict type), 55
from_bytes() method (int type), 91
fset attribute (properties), 253

Index 

fsum() function (math module), 86
full() method (queue module), 496
functions, 217–241

accepting arbitrary arguments for, 217–218
accessor functions as attributes of, 238–241
anonymous/inline, defining, 224–225
argument annotations, 220
calling with fewer arguments, 227–230
closures, accessing variables inside, 238–241
default arguments for, 222–224
disassembling, 392–395
inlining callbacks, 235–238
keyword arguments only, 219–220
multiple return values for, 221
partial() function and, 227–230
pointers, converting to callables, 643–644
replacing classes with, 231–232
state, carrying, 232–235
wrapper layers for, 329–331
wrapping with Cython, 632–638

futures module (concurrent module), 479, 505–

509
ProcessPoolExecutor class, 509–513

FutureWarning argument (warn() function),

583

G
garbage collection

caching instances and, 325
cyclic data structures and, 318

gauss() function (random module), 103
gdb debugger, 663–664
General Decimal Arithmetic Specification

(IBM), 86

generator expressions

concatenating/combining strings and, 60
filtering sequence elements with, 26
flattening nested sequences with, 135–136
recursive algorithms vs., 311–317
transforming/reducing data with, 32–33
Generator Tricks for Systems Programmers

(Beazley), 135

generator-expression argument, 32
GeneratorExit exception, 577
generators, 113–139

concurrency with, 524–531
creating iterator patterns with, 115–117
defining with extra states, 120
inlining callback functions with, 235

islice() function and, 122–123
iterator protocol, implementing, 117–119
pipelines, creating with, 132–135
search functions and, 6
slicing, 122–123
unpacking, 2

GET requests (HTTP), 437
get() function (webbrowser class), 563
get() method (ElementTree module), 185
get() method (queue module), 491, 495
__get__() method, 347–349

descriptors, 265–266

getattr() function, 305

visitor patterns and, 310

__getattr__() method (delegation), 287, 290
getboolean() method (ConfigParser module),

554

160–161

getdefaultencoding() function (sys module), 142
getfilesystemencoding() function (sys module),

getframe() (sys module), 63
frame hacking with, 373
__getitem__() method, 23
getLogger() function (logging module), 558
getpass module, 544
getpass() method (getpass module), 545
getrandbits() function (random module), 103
getrecursionlimit() function (sys module), 310
gettempdir() function (tempfile module), 169
getuser() (getpass module), 544
get_archive_formats() function (shutil module),

550

545

get_data() function (pkgutil module), 409
get_terminal_size() function (os module), 65,

gevent module, 531
glob module, 42
Global Interpreter Lock (GIL), 487, 513–516

C, calling Python from, 624
C/Python threads, mixing, 625–626
releasing in C extensions, 625
releasing in Cython, 636
thread pools and, 508

grammar rules, for parsers, 69–78
greenlets, 531
.group() method (re module), 46
groupby() function (itertools module), 24–26
grouping records, in dictionaries, 12
gzip compression format, 550

674 

gzip module, 149–151

compresslevel keyword, 150

H
hard limit on resources, 562
hashes, 17
heappop() method (heapq module), 8

priority queues and, 9

heapq module, 7–8

merge() function, 136–137

heaps, 7–11

nlargest()/nsmallest() functions and, 8
reversing order of, 10

help function

keyword arguments and, 219
metadata for arguments and, 220

hex() function, 89–90
hexadecimal digits, encoding, 197–198
hexadecimal integers, 89–90
hmac module, 461–463
HTML entities

handling in text, 65–66
replacing, 66
html module, 65
HTTP services

clients, 437–441
headers, custom, 438
testing client code, 441

httpbin.org service, 441

I
I/O, 141–174

decoding/encoding functions for, 56
event-driven, 475–481
iter() function and, 139
objects, serializing, 171–174
operations on strings, 148–149
passing open files and, 659
serial ports, communicating with, 170–171
thread termination and, 487

IDEs for Python development, 587

__init__() method, data structures and, 273

if-elif-else blocks, 304
IGNORECASE flag (re module), 47
ignore_patterns() function (shutil module), 548
ignore_types argument (isinstance() function),

135

import hooks, 418–420

patching modules on import, 428–431
sys.path_hooks variable and, 420

import statement, 412–428
ImportError exceptions, 425
importlib module, 421
import_module() function (importlib module),

411, 430

index-value pairs, iterating over, 127–128
IndexError exception, 19
indexing in CSV files, 175–179
indices() method (slice), 19
infinite values, 94–95
info() function (logging module), 556
inheritance

class decorators and, 356
class defined decorators and, 347
delegation vs., 289
implementation of, 258

.ini file features, 555
__init__() method (classes), 579

bypassing, 293–294
coding conventions and, 367–370
data structure initialization and, 270–274
mixins and, 297
multiple constructors and, 291–292
optional arguments and, 363
super() function and, 256–260

inline functions

callbacks, 235–238
defining, 224–225

input files, 539
input() function (fileinput), 540
insert() method (ElementTree module), 193
inspect() module, 364–367
instances

cached, metaclasses and, 358
creation, controlling with metaclasses, 356–

359

descriptors and, 264–267
saving memory when creating, 248–249
serializing with JSON, 182
WSGI applications as, 452

int type, 90–92
int() function, 90
interface files (Swig), 627
interpreters, communication between, 456–458
invalidate_caches() function (importlib mod‐

ule), 427

Index 

io module, 148–149, 163–165
ioctl(), 545
IPv4Address objects (ipaddress module), 448
is operator, 223
isinf() function (math module), 94
isinstance() function, 135
islice() function (itertools module), 122–123,

124

isnan() function (math module), 94
issuing warning messages, 583
is_alive() method (Thread class), 486
itemgetter function (operator module), 22
items() method (dictionaries), 16
iter() function

fixed size records and, 151
while loops vs., 138–139

__iter__() method (iterators), 114–115
generators with extra states and, 121

iterables

custom containers for, 283–286
discarding parts of, 123–125
from C, consuming, 662–663
sorted/merged, iterating over, 136–137
unpacking, 1–2

chain() method (itertools module), 131–132
iter() function vs. while loops, 138–139
merge() function (heapq module), 136–137
over fixed-sized records, 151
over index-value pairs, 127–128
over separate containers, 131–132
over sorted/merged iterables, 136–137
reverse, 119
zip() function and, 129–130

iterators, 113–139

consuming manually, 113–114
creating patterns with generators, 115–117
delegating, 114–115
islice() function (itertools module), 122–123
protocol, implementing, 117–119
slicing, 122–123

iterparse() function (ElementTree module), 188
iterparse() method (ElementTree module), 194
itertools module, 123–125, 125–127

compress() function, 28
groupby() function, 24–26

iter_as() function, 211

iterating

125–127

all possible combinations/permutations,

launching

J
join() method (str type), 58–61

changing single characters with, 145

join() method (Thread class), 486
JSON (JavaScript Object Notation)

reading/writing, 179–183
types supported by, 179

json module, 179–183
just-in-time (JIT) compiling, 590, 594

K
key argument (sorted() function), 23
KeyboardInterrupt exception, 577, 578
keys() method (dictionaries), 16
keyword arguments, 218

annotations, 220
for metaclasses, 362–364
functions that only accept, 219–220
help function and, 219

L
lambda expressions, 24, 225

variables, capturing, 225–227

Python debugger, 585
web browsers, 562

lazy imports, 403
lazy unpacking, 212
leap years, datetime module and, 104
libraries, adding logging to, 558
limited history, 5–7
linalg subpackage (NumPy module), 101
linear algebra, 100–101
list comprehension, 26
listdir() function (os module), 158–159

bad filenames, printing, 161–163

lists

NumPy vs., 99
processing and star unpacking, 5
unpacking, 2

ljust() method (str type), 57
llvmpy extension module, 643
load() function (pickle module), 171

untrusted data and, 172

LoadLibrary() function (ctypes.cdll module),

601

loads() function (json module), 179

676 

loads() function (pickle module), 171
loadTestsFromModule() method (TestLoader

loadTestsFromTestCase() method (TestCase

module), 573

module), 573

local() method (threading module), 504–505
locale module, 88
locals() function and exec() function, 386–388
Lock objects (threading module), 497–500
logging

adding to libraries, 558
adding to simple scripts, 555
output of, 557
test output to file, 572
logging module, 557, 559

critical(), 556
debug(), 556
error(), 556
info(), 556
warning(), 556

lower() method (str type), 54
lstrip() method (str type), 53
lxml module, 195

M
MagicMock instances, 568
makefile() method (socket module), 167
make_archive() function (shutil module), 549
MANIFEST.in file, 434
manipulating pathnames, 156–157
map() operation (ProcessPoolExecutor class),

511

Mapping class (collections module), 283
mappings

class definition order and, 362
consolidating multiple, 33–35
match() method (re module), 43

search/replace text and, 46

math() module, 94
matrix calculations, 100–101
matrix object (NumPy module), 100
max() function, 8, 15

attrgetter() function and, 24
generator expressions and, 33
itemgetter() function, 23

memory

management in cyclic data structures, 317–

320

mapping, 153–156

OrderedDict objects and, 13
parsing XML and, 186–189
restricting use of, 561
wchar_t strings and, 651
__slots__ attribute and, 248–249

memory mapping, 153–156
MemoryError exceptions, 562
memoryview() object (struct module), 153, 206,

213
Cython-typed, 640
large arrays and, 481–483

memory_map() function (mmap module), 154
merge() function (heapq module), 136–137
Mersenne Twister algorithm, 103
meta path importer, 414–418
sys.metapath object, 418

metaclass keyword (class statement), 362–364
metaclasses, 279

capturing attribute definition order, 359–362
controlling instance creation, 356–359
enforcing coding conventions, 367–370
initializing members at definition time, 374–

375

non-standardized binary files and, 207
optional arguments for, 362–364
__prepare__() method and, 361

metaprogramming, 329–395

adjustable attributes, 336–339
decorators, 329–356
decorators with arguments, 334–336
dis module, 392–395
exec() function vs., 386–388
function metadata in decorators, 331–333
function wrappers, 329–331
repetitive property methods and, 382–384
unwrapping decorators, 333–334

method overloading, 376–382
methodcaller() function (operator module), 305
Microsoft Excel, CSV encoding rules of, 177
Microsoft Visual Studio, 608
min() function, 8, 15

attrgetter() function and, 24
generator expressions and, 33
itemgetter() function, 23

__missing__() method (dict module), 62
mixin classes, 260

extending unrelated classes with, 294–299
in descriptors of data models, 280

mkdtemp() function (tempfile module), 169

Index 

mkstemp() function (tempfile module), 169
mmap module, 153–156
mock module (unittest module), 565, 570
module import * statement, 398–399
modules, 397–435

controlling import of, 398–399
from module import * statement, 398
hierarchical packages of, 397–398
import hooks, using, 412–428
importing with relative names, 399–401
importing, using string as name, 411
importlib module, 421
import_module() function (importlib mod‐

invalidate_caches() function (importlib

ule), 411

module), 427

meta path importer, 414–418
new_module() function (imp module), 421
objects, creating, 421
patching on import, 428–431
relative imports vs. absolute names, 400
reloading, 406–407
remote machines, loading from, 412–428
splitting into multiple files, 401–404
sys.path, adding directories to, 409–411
sys.path_importer_cache object, 424
virtual environments and, 432–433
__all__ variable in, 398–399
__init__.py file, 397
__main__.py files, 407–408

monthrange() function (calendar module), 108
months, finding date ranges of, 107–109
moving directories/files, 547
__mro__ attribute, 576
multidicts, 11–12
multiple-dispatch, 376–382
multiprocessing module, 488

GIL and, 514
passing file descriptors with, 470–475
reduction module, 470–475
RPCs and, 459

mutable buffers, reading into, 152–153
MutableMapping class (collections module),

283

283

MutableSequence class (collections module),

MutableSet class (collections module), 283

N
named pipes (Windows), 457, 470, 472
NamedTemporaryFile() function (tempfile

module), 169

namedtuple() function (collections module), 30
namedtuple() method (collections module)

new_class() function and, 372
unpacking binary data and, 203

NameError exception, 581
namespace package, checking for, 426
namespaces (XML), parsing, 193–195
namespaces, multiple directories in, 404–406
naming conventions

for private data, 250–251
__ (double underscore) and, 250
NaN (not a number) values, 94–95

in JSON, 183
isnan() function and, 95

nested sequences, flattening, 135–136
network programming, 437–483

connection (multiprocessing module), 456–

event-driven I/O, 475–481
hmac module, 461–463
interpreters, communication between, 456–

458

458

large arrays, sending/receiving, 481–483
passing file descriptors, 470–475
remote procedure calls, 454–456, 458–461
simple authentication, 461–463
socketserver module, 441–444
SSL, implementing, 464–470
TCP servers, 441–444
UDP server, implementing, 445–446
UDPServer class, 446
XML-RPC, 454–456

__new__() method (classes)

coding conventions and, 367–370
optional arguments and, 363

newlines, 142
new_class() function (types module), 370–373
new_module() function (imp module), 421
next() function (iterators), 113
nlargest() function (heapq module), 7–8
nonblocking, supporting with queues, 495
noncapture groups (regular expressions), 49
nonlocal declarations, 239

adjusting decorators with, 336–339

678 

normalize() function (unicodedata module), 51,

55

normalize() method (pytz module), 111
normalizing Unicode, 50–51

supported forms in Python, 51
normpath() (os.path module), 551
nsmallest() function (heapq module), 7–8
null-terminated strings, passing to C, 644–648
NullHandler() class (logging module), 559
numerical operations, 83–112

complex-valued math, 92–94
decimal calculations, 84–86
formating for output, 87–88
infinity and, 94–95
linear algebra calculations, 100–101
matrix calculations, 100–101
NaNs and, 94–95
NumPy and, 97–100
on fractions, 96–96
packing/unpacking integers from byte

string, 90–92

random number generators, 102–103
rounding, 83–84
time, 104–112
time conversions, 104–105
with binary integers, 89–90
with hexadecimal integers, 89–90
with octal integers, 89–90
NumPy module, 97–100, 640

complex math and, 93
CPU-bound programs and, 514
linear algebra and, 100–101
matrix calculations and, 100–101
unpacking binary data with, 203

O
objects, 243–327

248

305–306

and context-management protocols, 246–

calling methods on, when named in strings,

creating large numbers of, 248–249
defining default arguments and, 224
format() function and, 245–246
formatting output of, 243–244
implementing states for, 299–305
iterator protocol, implementing, 117–119
JSON dictionary, decoding to, 181
memory management, 317–320

representing in C, 608
serializing, 171–174
visitor pattern, implementing, 306–311
visitor patterns without recursion, 311–317
with statement and, 246–248

object_hook (json.loads() function), 181
object_pairs_hook (json.loads() function), 181
oct() function, 89–90
octal integers, 89–90
Olson time zone database, 110
opaque pointers, 612–614
open() function, 141–144

binary data, reading/writing, 145–147
closefd argument, 166
file descriptors and, 166
on non-Unix systems, 167
rb/wb modes of, 145–147
rt/wt mode, 141–144
x mode of, 147–148

optimizating your programs, 590
optparse vs. argparse, 544
OrderedDict object (collections module), 12–13

decoding JSON data into, 181

os module

listdir() function, 158–159
path module in, 156–157
OSError exception, 575–576
ouput sent to stdout, 565

P
pack() function (structs), 201
packages, 397–435

datafiles, reading, 408–409
directories/zip files, running, 407–408
distributing, 433–435
hierarchical, of modules, 397–398
MANIFEST.in file, 434
namespaces, multiple directories in, 404–406
per-user directory, installing, 431–432
submodules, importing with relative names,

399–401

sys.path, adding directories to, 409–411
third-party options for, 435
virtual environments and, 432–433
__init__.py file, 397
__path__ variable and, 405

pack_into() function (struct module), 153
Pandas package, 178, 214–216
parallel programming, 509–513

Index 

parse tree, 73
parse() function (ElementTree module), 185
parser (html), 66
parser, recursive descent, 69–78
parse_and_remove() function (ElementTree

module), 189

parsing command-line options, 539, 541
partial() function (functools module), 227–230

defining property methods and, 383
fixed size records and, 151
optional arguments for decorators and, 341
state, carrying with, 234, 235

Paste, 453
patch() function (unittest.mock class)

testing output with, 565
unit tests with, 567
patching objects, 567
path module (os), 156–157

testing for existing files with, 157–158

pathnames, manipulating, 156–157
pattern matching, 42–45
pattern object (regular expressions), 43
pdb debugger, 663–664
per-user directory, 431–432
performance

ast manipulation and, 392
attrgetter() function and, 24
Cython and, 638–642
dictionary comprehension vs. dict() type, 29
join() method vs. + operator, 60
keeping limited history and, 6
lazy attributes and, 267–270
nlargest()/nsmallest() functions and, 7
NumPy module and, 97–100
of byte vs. text strings, 81
pattern objects and, 45
profiling/timing, 587
sanitizing strings and, 56
strptime() method and, 110

perf_counter() function (time class), 561, 589
PermissionError exception, 575
permutations() function (itertools module), 125
pexpect, 547
pickle module, 171–174

limits on, 172
multiprocessing.connection functions and,

457

RPCs and, 458–461

Pipe object (multiprocessing module), 471

pipelines, creating, 132–135
pipes

accepting script via, 539
mimicking, 132–135

PLY, 69, 76
polling thread queues, 531–534
Popen class (subprocess module), 547
POST method (HTTP), 438
pprint() function (pprint module), 180
predefined abstract base classes, 276
prefix variable, 554, 555
__prepare__() method (classes), 361

new_class() function and, 372
optional arguments and, 363

prepare_class() function (types module), 373
print() function, 243, 542, 586

end argument, 144
line ending, changing, 144–145
redirecting to files, 144
separator character, changing, 144–145

private data/methods

caching instances within, 326
naming conventions for, 250–251

probability distributions (random module), 103
process pools, GIL and, 514, 516
ProcessPoolExecutor class (futures module),

509–513

process_time() function (time class), 561, 589
program crashes, debugging, 584
program profiling, 587
property attributes (classes)

delegating, 287–291
extending in subclasses, 260–264
repetitive definitions for, 382–384

@property decorator, 330, 346
proxies, delegating attributes with, 288, 290
public-facing services, 457
put() method (queue module), 491, 495
pwd module, 544
PyArg_ParseTuple() function (C extensions),

609
converting string encoding with, 650

PyBuffer_GetBuffer() method (Py_buffer ob‐

PyBuffer_Release() method (Py_buffer object),

PyCallable_Check() function (C extensions),

ject), 611

612

622

680 

sions), 614

618

sions), 614

PyCapsule_GetPointer() function (C exten‐

PyCapsule_Import() function (C extensions),

PyCapsule_New() function (C extensions), 613
PyCapsule_SetDestructor() function (C exten‐

PyErr_Occurred() function (C extensions), 623
PyFile_FromFd() function (C extensions), 658–

PyFloat_AsDouble() function (C extensions),

PyFloat_Check() function (C extensions), 623
PyGILState_Ensure() function (C extensions),

PyGILState_Release() function (C extensions),

PyIter_Next() function (C extensions), 662–663
PyObject data type (C extension)

filenames, passing with, 657–658

PyObject_BuildValue function (C extensions),

PyObject_Call() function (C extensions), 622

file-like objects and, 661

PyObject_GetIter() function (C extensions),

PyParsing, 69, 76
PyPy, 514
PySequence_Length() function (C extensions),

pySerial package, 170–171
Python for Data Analysis (McKinney), 216
Python Package Index, 434
Python syntax vs. JSON, 180
PYTHONPATH environment variable, 409
pytz module, 110–112
PyUnicode_FromWideChar() function (C ex‐

tensions)
C strings, converting to Python objects, 653–

659

623

624, 626

624, 626

623

661

662–663

625

654

654

Py_BuildValue() function (C extensions), 609

C strings, converting to Python objects, 653–

Py_DECREF() function (C extensions), 623
Py_END_ALLOW_THREADS (Py objects), 625
Py_XDECREF() function (C extensions), 623

pyvenv command, 432–433
Py_BEGIN_ALLOW_THREADS (Py objects),

470–475

Q
qsize() method (queue module), 496
queue module, 491–496
queues

bounding, 495
structures, deque() and, 6

quote() function (shlex class), 546

R
race conditions, avoiding, 497–500
raise from statement, 580, 582

None modifier, 581

raise statement, 580, 582
randint() function (random module), 102
random module, 102–103
random() function (random module), 103
re module

compile() function, 49
DOTALL flag, 49
IGNORECASE flag, 47
pattern matching and, 42–45
patterns, naming, 67
scanner() method, 67
search/replace text and, 45–46
split() method, 37–38
sub() function, 54
Unicode text and, 52–53

read() function, 139

file-like C objects and, 659–662
readability, naming slices for, 18–19
readinto() method (files), 147

reading into mutable buffers with, 152–153

getrecursionlimit() function (sys module),

recursion, 5

310

recursive descent parsers, 69–78

limitations on, 76

recvfrom() method (socket module), 445
recv_handle() function (reduction module),

recv_into() function (socket module), 153, 483
redirection, 539
regex module, 53
register() function (atexit module), 538
register_function() method (XML-RPC), 455
regular expressions

* (star) operator and, 48
? (question mark) modifier and, 48

Index 

greedy vs. nongreedy, 47–48
matching multiple lines, 48–49
newline support in, 48–49
on byte strings, 79
order of tokens in, 68
pattern matching and, 42–45
re.split() method and, 38
regex module for, 53
shortest match with, 47–48
stripping characters with, 54
Unicode and, 52–53

relational databases, 195–197

connecting to, 196
cursors, creating, 196

400

relative imports vs. absolute names (modules),

relativedelta() function (dateutil module), 105
reload() function (imp module), 406–407, 431
remote machines

loading modules from, 412–428
XML-RPC, 454–456

remote procedure calls (RPC), 458–461

exception handling with, 461

remove() method (ElementTree module), 193
replace() method (date module), 108
replace() method (datetime module), 108
_replace() method (namedtuple object), 31
replace() method (str type), 54, 54

performance and, 56
search/replace text and, 45
repr() function (built-in), 243
request module (urllib module), 438–441

client package vs., 440

requests package (urllib module)

return values of, 439
reraising exceptions, 582
reserved words, clashing with, 251
resource forks, 548
resource management, 248
resource module, 561
ResourceWarning argument (warn() function),

REST-base interface, 449–453

testing, 450

restricting CPU time, 561
result() method (ProcessPoolExecutor class),

583

512

rjust() method (str type), 57
RLock objects (threading module), 498
round() function, 83–84
rounding numbers, 83–84
RSS feeds, parsing, 183
rstrip() method (str type), 53
RTS-CTS handshaking, 170
RuntimeWarning argument (warn() function),

583

S
sample() function (random module), 102
sanitizing text, 54–57
scanner() method (re module), 67
search

for shortest match with regular expressions,

47–48

matching multiple lines, 48–49
nlargest()/nsmallest() functions (heapq

module), 7–8

noncapture groups, 49
normalization of Unicode text and, 51
visitor pattern and, 311
search/replace text, 45–46
case insensitive, 46–47

security

import statement and, 412
pickle and, 172
RPCs and, 460
SSL certificates and, 466

seed() function (random module), 103
segmentation faults, 663–664
select() function (event driven I/O), 476
self-signed certificates (SSL), 468
Semaphore objects (threading module), 490,

498

send() function (socket module), 483
send() method (generators), 316
sendmsg() method (socket module), 473
sendto() method (socket module), 445
send_handle() function (reduction module),

470–475

Sequence class (collections module), 283
sequences

filtering elements of, 26–28
flattening nested, 135–136
iterating over multiple, simultaneously, 129–

130

mapping names to elements of, 29–32

reverse iteration, 119–120
reversed() function, 119–120

682 

most frequently occurring item in, 20–21
removing duplicates from, 17–18
unpacking, 1–2

serial ports, communicating with, 170–171
serve_forever() method (XML-RPC), 455
Set class (collections module), 283
__set__() method (descriptors), 265

in data models, 280

setattr() method, 294
__setattr__() method (delegation), 290
setrlimit() function, 562
setsid() function (os module), 537
setup.py file, 633, 639

distributing packages and, 434

set_trace() function (pdb module), 586–587
shell, 546
shell scripts, 539
shelling out, 550
shuffle() function (random module), 102
shutil module

archives and, 549
copying files/directories with, 547

sig module, 343–345
signature objects, 364–367
signature() function (inspect), 343
SIGXCPU signal, 562
simple scripts, 555
simplefilter() function (warnings class), 584
SimpleXMLRPCServer (XML-RPC), 456
site-packages directories, 410

virtual environments vs., 432–433

skip() decorator, 574
skipIf() function (unittest module), 574
skipping test failures, 573
skipUnless() function (unittest module), 574
slice() object, 19
slices, naming, 18–19
__slots__ attribute
classes with, 32
memory management and, 249

socket module, 444

ipaddress module and, 448
sending datagrams with, 445
socketpair() function (Unix), 532
sockets

large arrays, sending/receiving, 483
setting options on, 443
threads and, 532

socketserver module

implementing TCP servers with, 441–444
UDP server, implementing, 445–446

sorted() function

and objects without comparison support,

23–24

itemgetter() function, 22

sorting

dictionaries, 13–15
dictionaries by common key, 21–23
finding largest/smallest N items, 7–8
groupby() function and, 25
itemgetter function (operator module), 22
objects without comparison support, 23–24

sort_keys argument (json.dumps() function),

182

source file vs. config file, 553
special characters, escaping, 66
split() method (re object), 37–38
split() method (str type), 37–38
SQLAlchemy, 197
sqlite3 module, 195
sqrt() function (math module), 592
ssh session, 547
SSL, 464–470

certificate authorities, 468
self-signed certificates, 468

ssl module, 464–470

random module vs., 103

Stackless Python, 531
stack_size() function (threading module), 509
star expressions

discarding values from, 4
unpacking iterables and, 3

start attribute (slice), 19
start() method (Thread class), 486
startswith() method (str type), 38

pattern matching with, 42

start_response argument (WSGI), 452
state

capturing with closures, 233–234
carrying with callback functions, 232–235
implementing for objects/machines, 299–

305

of mixins, 297
thread-specific, storing, 504–505
states, generators with extra, 120–121
static methods, applying decorators to, 350–352

Index 

@staticmethod decorator, 330

decorating class methods with, 350–352
__func__ attribute and, 334

stdout object (sys), changing encoding on, 163
step attribute (slice), 19
stop attribute (slice), 19
StopIteration exception, 113
str type

strip() method (str type), 53–54
strptime() method (datetime module), 109
struct module, 199–203

nested binary records, reading, 203–213
packing/unpacking integers from byte

strings and, 91

structure codes for, 201
variable-sized binary records, reading, 203–

submit() operation (ProcessPoolExecutor class),

213

structures (data type), 612–614
sub() function (re module), 54, 63

search/replace text and, 45

512

subn() method (re module), 46
subprocess module, 547
subsitution callback functions, 46
super() function, 256–260

converting to datetime objects, 109–110
decode() method, 56
encode() method, 56
endswith() method, 38
format() function, 57
join() method, 58–61
lower() method, 54
pattern matching with, 42
replace() method, 54
sanitizing text with, 54–57
split() method, 37–38
startswith() method, 38
stripping unwanted characters from, 53–54
translate() method, 55
upper() method, 54

str() function, 243
StreamRequestHandler (socketserver module),

442–444

string module, 246
string templates and code readability, 453
StringIO object (io module), 566
StringIO() object (io module), 148–149
strings, 37–81

aligning, 57–58
bad encoding in C/Python extensions, 654–

657

306

C, converting to Python objects, 653–654
calling object methods when named in, 305–

combining, 58–61
concatenating, 58–61
converting to dates/times, 109–110
database API and, 197
I/O operations, performing on, 148–149
interpolating variables in, 61–64
matching start/end text of, 38–40
null-terminated, passing to C, 644–648
splitting on delimiters, 37–38
stripping unwanted characters from, 53–54
Unicode, passing to C modules, 648–653
unpacking, 2

class decorators and, 356
coding conventions and, 370
extending properties in subclasses and, 262
mixin classes and, 298

surrogate encoding, 654–657
Swig, 604, 627–631
headers and, 630
symbolic links, 548

broken, 549

SyntaxWarning argument (warn() function),

583

sys.arg value, 543
sys.argv (commaand line arguments), 544
sys.metapath object, 418, 430

extending import operations and, 422

sys.modules dictionary, 421
sys.path

adding directories to, 409–411
site-packages directories, 410

sys.path_hooks variable (importers), 420
sys.path_importer_cache object, 424
sys.stderr, 540
sys.stdout, 565–566
system-exiting exceptions, 578
SystemExit exception, 540, 577–580

T
tag attribute (ElementTree module), 185
tarfile compression format, 550
TCP servers, 441–444

event-driven I/O implementation, 477

684 

tempfile module, 167–170
temporary files, 167–170
TemporaryDirectory() method (tempfile mod‐

ule), 169

TemporaryFile (tempfile module), 168
terminal, finding size of, 65, 545
test failures, 573
TestCase classes (TestLoader module), 573
testing

ouput sent to stdout, 565
output, logging to file, 572
unit tests for exceptions, 570

TestLoader class (unitest module), 573
text attribute (ElementTree module), 185
text data

encoding, 141–144
reading/writing, 141–144

text manipulation, 37–81
aligning strings, 57–58
case insensitive search/replace, 46–47
combining/concatenating, 58–61
HTML entities, handling in text, 65–66
interpolating variables, 61–64
matching start/end of strings, 38–40
of Unicode with regular expressions, 52–53
on byte strings, 78–81
parsers, implementing, 69–78
pattern matching, 42–45
reformatting into columns, 64–65
sanitizing, 54–57
search/replace, 45–46
stripping unwanted characters, 53–54
tokenizing, 66–69
wildcard matcing, 40–42
XML entities, handling in text, 65–66

TextIOWrapper object (io module), 163–165
textwrap module, 64–65
Thread class (threading module), 485–488
thread pools

GIL and, 508
queues and, 506

threading module, 488

Condition object, 489
Event object, 488–491
local() method, 504–505
Lock objects, 497–500
Semaphore objects, 490
stack_size() function, 509

ThreadingMixIn class (socketserver module),

ThreadingTCPServer objects (socketserver

ThreadingUDPServer objects (socketserver

297

module), 442

module), 446

ThreadPoolExecutor class (futures module),

505–509

threads

actor model and, 516–520
C/Python, mixing, 625–626
communication between, 491–496
creating/destroying, 485–488
daemonic, 486
deadlocks between, 500–503
generators as alternative to, 524–531
locking critical sections, 497–500
nonblocking, supporting with queues, 495
polling multiple queues, 531–534
pools of, 505–509
priority queues and, 11
queue module and, 491–496
race conditions, avoiding, 497–500
status of, finding, 488–491
storing state of, 504–505
timeouts, supporting with queues, 495

throw() method (generators), 531
time command, 587
time module, 590
time zones, 110–112

ule), 112

UTC time, 111

time() function (time class), 561
time, operations on, 104–112

country_timezones dictionary (pytz mod‐

converting strings for, 109–110
date calculations, 106–107
date ranges, finding, 107–109
pytz module, 110–112
time conversions, 104–105
time zones, manipulating, 110–112
UTC time, 111

timedelta object (datetime module), 104, 107–

109

timeit module, 589, 590
timeouts, supporting with queues, 495
Timer class, 561
tokenizing and DOTALL flag (re module), 49
tokens streams, filtering, 68

Index 

tostring() function (ElementTree module), 190
total_ordering decorator (functools module),

unpacking

321–323

to_bytes() method (int module), 91
traceback, segmentation faults and, 663–664
traceback.print_stack() function, 586
translate() method (str type), 54, 55

numerical output and, 88
performance and, 56

transmitting data, 155
tree structures, memory management of, 317–

320

tree traversal, 311, 314
TTYs, 545, 547
tuples

and endswith()/startswith() methods, 39
as return values, 221
relational databases and, 195–197
unpacking, 2

Twisted package, 238
type checking (data)

abstract base classes and, 274–276
forcing with decorator functions, 341–345

type systems, 277–283
%typemap directive (Swig), 631
types module, 370–373

U
UDP server, 445–446

event-driven I/O implementation, 476

UDPServer class, 446
umask() function (os module), 537
unescape() function (xml.sax.saxutils module),

191

Unicode, 50–53

657

bad encoding in C/Python extensions, 654–

archives, 549
discarding values while, 2
enumerate() function and, 128
integers from byte string, 90–92
iterables into separate variables, 1–2
iterables of arbitrary length, 3–5
sequences into separate variables, 1–2
star expressions and, 3

unpack_archive() function (shutil module), 549
unpack_from() method (struct module), 202
uploading using requests module, 440
upper() method (str type), 54
urllib module, 413, 437–441
urlopen() function (urllib module), 569–570

import statements vs., 413
sending query parameters with, 438

UserWarning argument (warn() function), 583
UTC time, 111

V
validation of data, abstract base classes and,

274–276

ValueError, 570

unpacking and, 2

values() method (dictionaries), 14, 16
variables

interpolating, in strings, 61–64
understand locality of, 592

vars() method, 62
virtual environments, 432–433
visitor pattern

implementing with recursion, 306–311
implementing without recursion, 311–317

W
walk() function (os module), 550
warn() function (warning class), 583
warning messages, issuing, 583
warning module, 584
warning() function (logging module), 556
watchdog timers, 503
wchar_t * declarations (C), 648–653
converting to Python objects, 653

weak references and caching instances, 325
weakref module, 317, 320
web browsers, launching, 562

IGNORECASE flag and matching, 47
regular expressions and, 52–53
strings, passing to C modules, 648–653

unicodedata module, 55
uniform() function (random module), 103
unittest module, 565, 572–573
Unix, 548

commands, 548
find utility, 551

unpack() function (struct module), 201

binary data and, 205

686 

web programming, 437–483

HTTP service clients, 437–441
REST-base interface, 449–453
urllib module and, 437–441

webbrowser module, 563
WebOb, 453
while loops vs. iter() function, 138–139
wildcard matcing, strings, 40–42
Windows, 548

C extension modules and, 608

with nogil: statement (Cython), 636
with statement, 246–248, 561, 566

contextmanager decorator and, 385
Lock objects and, 497–500
Semaphore objects and, 499

wraparound() decorator (Cython), 641
__wrapped__ attribute, 332
@wraps decorator (functools)

function metadata and, 331–333
unwrapping, 333–334
WSGI standard, 449–453

return value for apps based on, 452

X
XML entities

handling in text, 65–66
replacing, 66

XML files

dictionaries, converting to, 189–191

extracting data from, 183–186
modifying, 191–193
parsing, 191–193

incrementally, 186–189
with namespaces, 193–195

rewriting, 191–193
tags, specifying, 185

XML-RPC, 454–456
adding SSL to, 466
data types handled by, 455

Tree module)

xml.sax.saxutils module, 191
XMLNamespaces class, 194

xml.etree.ElementTree module (see Element‐

Y
yield from statement, 135–136
yield statement, 60, 134

behavior in generators, 315
concurrency implementations and, 524–531
generators and, 116
search functions and, 6

Z
ZeroMQ, 457
zip files as runnable scripts, 407–408
zip() method (dictionaries), 13–15, 129–130
zipfile compression format, 550

Index 

About the Authors
David Beazley is an independent software developer and book author living in the city
of Chicago. He primarily works on programming tools, providing custom software
development, and teaching practical programming courses for software developers,
scientists, and engineers. He is best known for his work with the Python programming
language, for which he has created several open source packages (e.g., Swig and PLY)
and authored the acclaimed Python Essential Reference. He also has significant experi‐
ence with systems programming in C, C++, and assembly language.
Brian K. Jones is a system administrator in the department of computer science at
Princeton University.
Colophon
The animal on the cover of Python Cookbook, Third Edition is a springhaas (Pedetes
capensis), also known as a spring hare. Springhaas are not hares at all, but rather the
only member of the family Pedetidae in the order Rodentia. They are not marsupials,
but they are vaguely kangaroo-like, with small front legs, powerful hind legs designed
for hopping, jumping, and leaping, and long, strong, bushy (but not prehensile) tails
used for balance and as a brace when sitting. They grow to be about 14 to 18 inches
long, with tails as long as their bodies, and can weigh approximately eight pounds.
Springhaas have rich, glossy, tawny, or golden-reddish coats with long, soft fur and white
underbellies. Their heads are disproportionately large, and they have long ears (with a
flap of skin at the base they can close to prevent sand from getting inside while they are
digging) and large, dark brown eyes.
Springhaas mate throughout the year and have a gestation period of about 78 to 82 days.
Females generally give birth to only one baby (which stays with its mother until it is
approximately seven weeks old) per litter but have three or four litters each year. Babies
are born with teeth and are fully furred, with their eyes closed and ears open.
Springhaas are terrestrial and well-adapted for digging, and they tend to spend their
days in the small networks of their burrows and tunnels. They are nocturnal and pri‐
marily herbivorous, feeding on bulbs, roots, grains, and occasionally insects. While they
are foraging, they move about on all fours, but they are able to move 10 to 25 feet in a
single horizontal leap and are capable of quick getaways when frightened. Although they
are often seen foraging in groups in the wild, they do not form an organized social unit
and usually nest alone or in breeding pairs. Springhaas can live up to 15 years in captivity.
They are found in Zaire, Kenya, and South Africa, in dry, desert, or semiarid areas, and
they are a favorite and important food source in South Africa.
The cover image is from Animal Creation: Mammalia. The cover font is Adobe ITC
Garamond. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Con‐
densed; and the code font is Dalton Maag’s Ubuntu Mono.


